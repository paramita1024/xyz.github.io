<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-08-04T12:24:48+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Sankarshan Mridha</title><subtitle>On machine learning, deep learning, nlp, computer vision, statistics and other areas of data science</subtitle><author><name>Sankarshan Mridha</name></author><entry><title type="html">Blog 703: Build website using Jekyll and MathJax</title><link href="http://localhost:4000/blog/2019/07/31/blog_703_Build_Website" rel="alternate" type="text/html" title="Blog 703: Build website using Jekyll and MathJax" /><published>2019-07-31T00:11:31+05:30</published><updated>2019-07-31T00:11:31+05:30</updated><id>http://localhost:4000/blog/2019/07/31/blog_703_Build_Website</id><content type="html" xml:base="http://localhost:4000/blog/2019/07/31/blog_703_Build_Website">&lt;p&gt;I write lot of content related to machine learning, deep learning. So obviously those contents have lots of math equation written in &lt;a href=&quot;https://www.latex-project.org/&quot;&gt;latex&lt;/a&gt;. But all the popular blog hosting websties like &lt;a href=&quot;https://medium.com/&quot;&gt;medium.com&lt;/a&gt;, &lt;a href=&quot;https://towardsdatascience.com/&quot;&gt;towardsdatascience.com&lt;/a&gt; etc don’t support latex equation and I don’t like them. Also I am a very lazy person. So my requirement is to write all the content, equation in &lt;a href=&quot;https://en.wikipedia.org/wiki/Markdown&quot;&gt;markdown&lt;/a&gt; file and push them to &lt;a href=&quot;https://github.com/&quot;&gt;Github&lt;/a&gt; and voila !! Content published in blog. Simple !!!&lt;/p&gt;

&lt;p&gt;But it’s easier said than done. First of all Github doesn’t render latex math equation by default in the markdown file. So check some package where you can build website using Markdown file. There are couple of such package but I prefer this particular one. Enter &lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt;. And good part is Jekyll websites are easy to host in Github.&lt;/p&gt;

&lt;p&gt;So set up Jekyll, write content in markdown, push to Github and done. But only flaw is latex math equation rendering. So we need to configure the Jekyll with &lt;a href=&quot;https://www.mathjax.org/&quot;&gt;MathJax&lt;/a&gt;. So below are the resources that will help you to set up a simple system like this.&lt;/p&gt;

&lt;h3 id=&quot;create-sample-jekyll-blog-and-host-on-github&quot;&gt;&lt;strong&gt;Create Sample Jekyll Blog and Host on Github&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=fqFjuX4VZmU&quot;&gt;Youtube: Hosting Jekyll website on Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;configuring-jekyll&quot;&gt;&lt;strong&gt;Configuring Jekyll&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Heavily borrowing from this &lt;a href=&quot;https://github.crookster.org/Adding-MathJAX-LaTeX-MathML-to-Jekyll/&quot;&gt;Blog&lt;/a&gt;, it was a quick task to add. Add the following to Jekyll website repo:&lt;/p&gt;

&lt;div class=&quot;language-css highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;_includes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;mathjax&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;.html&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;assets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;js&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;MathJaxLocal&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;.js&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;_layouts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;posts&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;.html&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;mathjax.html&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Copy the below snippet and put it in the above mentioned location.&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;script &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text/x-mathjax-config&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;MathJax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Hub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;tex2jax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;inlineMath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;processEscapes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;script
  &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text/javascript&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;charset=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;utf-8&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;script
  &lt;/span&gt;&lt;span class=&quot;na&quot;&gt;type=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text/javascript&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;charset=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;utf-8&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://vincenttam.github.io/javascripts/MathJaxLocal.js&quot;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/script&amp;gt;&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;MathJaxLocal.js&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Copy the below file and put it in the above mentioned repo.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/idcrook/idcrook.github.io/blob/master/assets/js/MathJaxLocal.js&quot;&gt;MathJaxLocal.js&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;posts.html&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Put the below content in the above mentioned repo.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/post_html.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;final-repo-structure&quot;&gt;Final Repo Structure&lt;/h3&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;├── 404.html
├── about.md
├── assets
│   └── js
│       └── MathJaxLocal.js
├── _config.yml
├── Gemfile
├── Gemfile.lock
├── _includes
│   └── mathjax.html
├── index.md
├── _layouts
│   └── post.html
├── _posts
│   ├── 2019-07-28-blog_000.md
│   ├── 2019-07-28-blog_001.md
├── pub.md
└── _site
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://images.squarespace-cdn.com/content/550c6978e4b0c1da40fd1208/1541615714652-2BCR3Y3BU5X4U7KYMT5R/that%27s+it+done+logo-01.png?format=1500w&amp;amp;content-type=image%2Fpng&quot; alt=&quot;image&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;</content><author><name>Sankarshan Mridha</name></author><summary type="html">I write lot of content related to machine learning, deep learning. So obviously those contents have lots of math equation written in latex. But all the popular blog hosting websties like medium.com, towardsdatascience.com etc don’t support latex equation and I don’t like them. Also I am a very lazy person. So my requirement is to write all the content, equation in markdown file and push them to Github and voila !! Content published in blog. Simple !!!</summary></entry><entry><title type="html">Blog 000: Probability and Statistics</title><link href="http://localhost:4000/blog/2019/07/28/blog_000_Prob_Stat" rel="alternate" type="text/html" title="Blog 000: Probability and Statistics" /><published>2019-07-28T00:11:31+05:30</published><updated>2019-07-28T00:11:31+05:30</updated><id>http://localhost:4000/blog/2019/07/28/blog_000_Prob_Stat</id><content type="html" xml:base="http://localhost:4000/blog/2019/07/28/blog_000_Prob_Stat">&lt;h2 id=&quot;what-is-p-value&quot;&gt;What is P Value?&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;In statistical hypothesis testing, the p-value or probability value is, for a given statistical model, the probability that, when the null hypothesis is true, the statistical summary (such as the absolute value of the sample mean difference between two compared groups) would be greater than or equal to the actual observed results.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Hypothesis Testing&lt;/li&gt;
  &lt;li&gt;Normal Distribution&lt;/li&gt;
  &lt;li&gt;What is P-value?&lt;/li&gt;
  &lt;li&gt;Statistical Significance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/0*XqFCVZreewh3lATA.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Before we talk about what p-value means, let’s begin by understanding hypothesis testing where p-value is used to determine the statistical significance of our results.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Hypothesis testing is used to test the validity of a claim (null hypothesis) that is made about a population using sample data. The alternative hypothesis is the one you would believe if the null hypothesis is concluded to be untrue.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;The lower the p-value, the more surprising the evidence is, the more ridiculous our null hypothesis looks.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;If the p-value is lower than a predetermined significance level (people call it alpha, I call it the threshold of being ridiculous — don’t ask my why, I just find it easier for me to understand), then we reject the null hypothesis.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In my opinion, p-values are used as a tool to challenge our initial belief (null hypothesis) when the result is statistically significant. The moment we feel ridiculous with our own belief (provided the p-value shows the result is statistically significant), we discard our initial belief (reject the null hypothesis) and make a reasonable decision.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;statistical-significance&quot;&gt;Statistical Significance&lt;/h3&gt;

&lt;p&gt;Finally, this is the final stage where we put everything together and test if the result is statistically significant.&lt;/p&gt;

&lt;p&gt;Having just the p-value is not enough, we need to set a threshold (aka significance level — alpha). The alpha should always be set before an experiment to avoid bias. If the observed p-value is lower than alpha, then we conclude that the result is statistically significant.&lt;/p&gt;

&lt;p&gt;The rule of thumb is to set alpha to be either 0.05 or 0.01 (again, the value depends on your problems at hand).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/p-values-explained-by-data-scientist-f40a746cfc8&quot;&gt;Very Very IMP TDS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-p-values&quot;&gt;What is p-values?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;How it is decided for rejecting null hypothesis? Why it’s called null hypothesis?&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Null hypothesis&lt;/code&gt; means the hypothesis which you want to nullify. So there is an &lt;code class=&quot;highlighter-rouge&quot;&gt;alternate hypothesis&lt;/code&gt;, which will be accepted if the null hypothesis is rejected.&lt;/li&gt;
  &lt;li&gt;A &lt;code class=&quot;highlighter-rouge&quot;&gt;p-value&lt;/code&gt; is the probability of finding some sample outcome or a more extreme one if the null hypothesis is true.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Example:&lt;/strong&gt; I want to know if happiness is related to wealth among Dutch people. One approach to find this out is to formulate a null hypothesis. Since “related to” is not precise, we choose the opposite statement as our null hypothesis:
    &lt;blockquote&gt;
      &lt;p&gt;The correlation between wealth and happiness is zero among all Dutch people.&lt;/p&gt;
    &lt;/blockquote&gt;
    &lt;ul&gt;
      &lt;li&gt;We’ll now try to refute this hypothesis in order to demonstrate that happiness and wealth are related all right.&lt;/li&gt;
      &lt;li&gt;Now, we can’t reasonably ask all 17,142,066 Dutch people how happy they generally feel. So we’ll ask a sample (say, 100 people) about their wealth and their happiness. The correlation between happiness and wealth turns out to be 0.25 in our sample. Now we’ve one problem: sample outcomes tend to differ somewhat from population outcomes.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;How we can ever say anything about our population if we only have a tiny sample from it.&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;So how does that work? Well, basically, some sample outcomes are highly unlikely given our null hypothesis.&lt;/li&gt;
      &lt;li&gt;If our population correlation really is zero, then we can find a sample correlation of 0.25 in a sample of N = 100. The probability of this happening is only 0.012. So it’s very unlikely. A reasonable conclusion is that our population correlation wasn’t zero after all.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; we reject the null hypothesis. Given our sample outcome, we no longer believe that happiness and wealth are unrelated. However, we still can’t state this with certainty.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.spss-tutorials.com/null-hypothesis/&quot;&gt;source&lt;/a&gt;, &lt;a href=&quot;https://onlinecourses.science.psu.edu/statprogram/node/138/&quot;&gt;link2&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;why-bayesian-inference-can-be-difficult&quot;&gt;Why bayesian inference can be difficult?&lt;/h2&gt;

&lt;p&gt;Determining the &lt;code class=&quot;highlighter-rouge&quot;&gt;posterior distribution&lt;/code&gt; directly from the Byes’ rule involes coputing the evidence i.e &lt;code class=&quot;highlighter-rouge&quot;&gt;marginal likelihood&lt;/code&gt;. For continutios parameters, the integral can be impossible to solve analytically.&lt;/p&gt;

&lt;p&gt;Historically the difficulty of the integration was bypassed by restricting the models to relitevely simple likelihood functions with corresponding formulas for prior distribution - &lt;code class=&quot;highlighter-rouge&quot;&gt;conjugate prior&lt;/code&gt;, that played nice with the likelihood functions to give us a tractable solution.&lt;/p&gt;

&lt;p&gt;When the conjugate prior approach doesn’t work, another approach is the&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;approximate the original functions with other functions, which are easier to work with and then show that the approximate is reasonably good under typical conditions. This is known as &lt;code class=&quot;highlighter-rouge&quot;&gt;Variational Approximation&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another kind of approximation involves,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;randomly sampling large number of representative combinations of parameter values from the posterior distribution. These types of algorithms are known as MCMC algorithm. These methods help to calculate the posterior distribution without calculating the integral.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-does-gibbs-sampling-work&quot;&gt;How does Gibbs Sampling work?&lt;/h3&gt;

&lt;p&gt;Gibbs Sampling is a MCMC method to draw samples from a potentially really really complicated, high dimensional distribution, where analytically, it’s hard to draw samples from it. The usual suspect would be those nasty integrals when computing the normalizing constant of the distribution, especially in Bayesian inference. Now Gibbs Sampler can draw samples from any distribution, provided&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;you can provide all of the &lt;strong&gt;conditional distributions of the joint distribution analitically&lt;/strong&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wiseodd.github.io/techblog/2015/10/09/gibbs-sampling/&quot;&gt;Github&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://videolectures.net/mlss09uk_murray_mcmc/&quot;&gt;Video Lecture&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;importance-sampling-and-monte-carlo&quot;&gt;Importance Sampling and Monte Carlo&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://shakirm.com/?section=2&quot;&gt;Shakir Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.facebook.com/uclcsml/videos/3027632833943878/&quot;&gt;Shakir Md - MLSS 2019 London&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.math.arizona.edu/~tgk/mc/book_chap6.pdf&quot;&gt;Note&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;distribution&quot;&gt;Distribution&lt;/h2&gt;

&lt;p&gt;While the concept of probability gives us the mathematical calculations, distributions help us actually visualize what’s happening underneath.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2017/09/6-probability-distributions-data-science/&quot;&gt;(AVB)&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;uniform-distribution&quot;&gt;Uniform Distribution&lt;/h3&gt;
&lt;p&gt;A variable X is said to be uniformly distributed if the density function is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) = \frac{1}{b-a}&lt;/script&gt;

&lt;p&gt;where $-\infty&amp;lt;a&amp;lt;=x&amp;lt;=b&amp;lt;\infty$&lt;/p&gt;

&lt;h3 id=&quot;bernoulli-distribution&quot;&gt;Bernoulli Distribution&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt; All you cricket junkies out there! At the beginning of any cricket match, how do you decide who is going to bat or ball? A toss! It all depends on whether you win or lose the toss, right? Let’s say if the toss results in a head, you win. Else, you lose. There’s no midway.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Formulation:&lt;/strong&gt; A Bernoulli distribution has only two possible outcomes, namely &lt;code class=&quot;highlighter-rouge&quot;&gt;1 (success)&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;0 (failure)&lt;/code&gt;, and a &lt;code class=&quot;highlighter-rouge&quot;&gt;single trial&lt;/code&gt;. So the random variable $X$ which has a Bernoulli distribution can take value 1 with the probability of &lt;em&gt;success&lt;/em&gt;, say &lt;code class=&quot;highlighter-rouge&quot;&gt;p&lt;/code&gt;, and the value 0 with the probability of &lt;em&gt;failure&lt;/em&gt;, say &lt;code class=&quot;highlighter-rouge&quot;&gt;q&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;1-p&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The probability mass function is given by:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;p^x(1-p)^{1-x}&lt;/script&gt;&lt;br /&gt;
where $x \in (0, 1)$.&lt;/p&gt;

&lt;p&gt;The expected value of a random variable X from a Bernoulli distribution is found as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(X) = 1*p + 0*(1-p) = p&lt;/script&gt;

&lt;p&gt;The variance of a random variable from a bernoulli distribution is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V(X) = E(X^2)- [E(X)]^2 = p - p^2 = p(1-p)&lt;/script&gt;

&lt;h3 id=&quot;binomial-distribution&quot;&gt;Binomial Distribution&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt; Let’s get back to cricket.  Suppose that you won the toss today and this indicates a successful event. You toss again but you lost this time. If you win a toss today, this does not necessitate that you will win the toss tomorrow. Let’s assign a random variable, say X, to the number of times you won the toss. What can be the possible value of X? It can be any number depending on the number of times you tossed a coin.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Formulation:&lt;/strong&gt; An experiment with only two possible outcomes repeated n number of times is called binomial. The parameters of a binomial distribution are n and p where n is the total number of trials and p is the probability of success in each trial. If &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; is the total number fof success, we can write:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x) = \binom{n}{x} p^x (1-p)^{n-x}&lt;/script&gt;

&lt;p&gt;On the basis of the above explanation, the properties of a Binomial Distribution are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Each trial is independent.&lt;/li&gt;
  &lt;li&gt;There are only two possible outcomes in a trial- either a success or a failure.&lt;/li&gt;
  &lt;li&gt;A total number of n identical trials are conducted.&lt;/li&gt;
  &lt;li&gt;The probability of success and failure is same for all trials. (Trials are identical.)&lt;/li&gt;
  &lt;li&gt;Mean:  $\mu = n*p$&lt;/li&gt;
  &lt;li&gt;Variance: 
&lt;script type=&quot;math/tex&quot;&gt;Var(X) = n*p*(1-p)&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;normal-distribution&quot;&gt;Normal Distribution:&lt;/h3&gt;
&lt;p&gt;Normal distribution represents the behavior of most of the situations in the universe (That is why it’s called a “normal” distribution. I guess!).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;The large sum of (small) random variables often turns out to be normally distributed, contributing to its widespread application ~ Central Limit Theoram&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;Any distribution is known as Normal distribution if it has the following characteristics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;The mean, median and mode of the distribution coincide.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;The curve of the distribution is bell-shaped and symmetrical about the line $x=\mu$ .&lt;/li&gt;
  &lt;li&gt;The total area under the curve is &lt;code class=&quot;highlighter-rouge&quot;&gt;1&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Exactly half of the values are to the left of the center and the other half to the right.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A normal distribution is highly different from Binomial Distribution. However, if the number of trials approaches infinity then the shapes will be quite similar.&lt;/p&gt;

&lt;p&gt;The PDF of a random variable X following a normal distribution is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x) = \frac{1}{\sqrt{2\pi\sigma}}\exp({-\frac{(x-\mu)^2}{2\sigma^2}})&lt;/script&gt;

&lt;p&gt;where $-\infty&amp;lt;x&amp;lt;\infty$&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;standard normal distribution looks as follows:&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x) = \frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}&lt;/script&gt;

&lt;h3 id=&quot;poisson-distribution&quot;&gt;Poisson Distribution&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt; Suppose you work at a call center, approximately how many calls do you get in a day? It can be any number. Now, the entire number of calls at a call center in a day is modeled by Poisson distribution. Some more examples are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The number of emergency calls recorded at a hospital in a day.&lt;/li&gt;
  &lt;li&gt;The number of thefts reported in an area on a day.&lt;/li&gt;
  &lt;li&gt;The number of customers arriving at a salon in an hour.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Poisson Distribution is applicable in situations where events occur at random points of time and space wherein our interest lies only in the number of occurrences of the event.&lt;/p&gt;

&lt;p&gt;A distribution is called Poisson distribution when the following assumptions are valid:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Any successful event should not influence the outcome of another successful event.&lt;/li&gt;
  &lt;li&gt;The probability of success over a short interval must equal the probability of success over a longer interval.&lt;/li&gt;
  &lt;li&gt;The probability of success in an interval approaches zero as the interval becomes smaller.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Formulation:&lt;/strong&gt; Now, if any distribution validates the above assumptions then it is a Poisson distribution. Some notations used in Poisson distribution are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\lambda$ is the rate at which an event occurs,&lt;/li&gt;
  &lt;li&gt;$t$ is the length of a time interval,&lt;/li&gt;
  &lt;li&gt;$X$ is the number of events in that time interval.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here, $X$ is called a Poisson Random Variable and the probability distribution of $X$ is called Poisson distribution.&lt;/p&gt;

&lt;p&gt;Let $\mu$ denote the mean number of events in an interval of length t. Then, $\mu = \lambda*t$.&lt;/p&gt;

&lt;p&gt;The PMF of X following a Poisson distribution is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X=x)=e^{-\mu} \frac{\mu^x}{x!}&lt;/script&gt;

&lt;p&gt;where $x=0,1,2,3,…$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mean: $E(X) = \mu$&lt;/li&gt;
  &lt;li&gt;Variance: $Var(X) = \mu$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;exponential-distribution&quot;&gt;Exponential Distribution&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt; Let’s consider the call center example one more time. &lt;strong&gt;What about the interval of time between the calls?&lt;/strong&gt; Here, exponential distribution comes to our rescue. Exponential distribution models the interval of time between the calls.&lt;/p&gt;

&lt;p&gt;Other examples are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Length of time beteeen metro arrivals,&lt;/li&gt;
  &lt;li&gt;Length of time between arrivals at a gas station&lt;/li&gt;
  &lt;li&gt;The life of an Air Conditioner&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Exponential distribution is widely used for survival analysis. From the expected life of a machine to the expected life of a human, exponential distribution successfully delivers the result.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Formulation:&lt;/strong&gt; A random variable $X$ is said to have an exponential distribution with PDF:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) = \lambda e^{-\lambda x}&lt;/script&gt;

&lt;p&gt;where $x ≥ 0$ and parameter $\lambda&amp;gt;0$ which is also called the rate.&lt;/p&gt;

&lt;p&gt;For survival analysis, $\gamma$ is called the failure rate of a device at any time &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt;, given that it has survived up to &lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Mean and Variance of a random variable X following an exponential distribution:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mean: $E(X) = \frac{1}{\lambda}$&lt;/li&gt;
  &lt;li&gt;Variance: $Var(X) = \frac{1}{\lambda^2}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gamma-distribution&quot;&gt;Gamma Distribution:&lt;/h3&gt;
&lt;p&gt;We now define the gamma distribution by providing its PDF: 
A continuous random variable $X$ is said to have a gamma distribution with parameters $\alpha&amp;gt;0$ and $\lambda&amp;gt;0$, shown as $X \sim Gamma(\alpha,\lambda)$, if its PDF is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_X(x) = \left\{
						\begin{array}{l l}
						\frac{\lambda^{\alpha} x^{\alpha-1} e^{-\lambda x}}{\Gamma(\alpha)} \hspace {5pt} x &gt; 0\\
						0 \hspace{56pt} \textrm{otherwise}
						\end{array}\right.&lt;/script&gt;

&lt;p&gt;If we let $\alpha=1$, we obtain:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_X(x) = \left\{
					\begin{array}{l l}
					\lambda e^{-\lambda x} \hspace{20pt} x &gt; 0\\
					0 \hspace{41pt} \textrm{otherwise}
					\end{array}\right.&lt;/script&gt;

&lt;p&gt;Thus, we conclude $Gamma(1,\lambda)=Exponential(\lambda)$. More generally, if you sum n independent $Exponential(\lambda)$ random variables, then you will get a $Gamma(n,\lambda)$ random variable.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.probabilitycourse.com/chapter4/4_2_4_Gamma_distribution.php&quot;&gt;(gamma)&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;how-all-the-distributions-are-related&quot;&gt;How all the distributions are related?&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/distribution.png&quot; alt=&quot;distribution&quot; /&gt;
&lt;a href=&quot;https://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/&quot;&gt;(blog)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relation between Exponential and Poisson Distribution:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;If the times between random events follow exponential distribution with rate $\lambda$, then the total number of events in a time period of length t follows the Poisson distribution with parameter $\lambda t$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;moments---shape-of-you-&quot;&gt;Moments - Shape of You !!&lt;/h2&gt;

&lt;p&gt;Moments try to measure the &lt;strong&gt;shape of the probability distribution function&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The zeroth moment is the total probability of the distribution which is &lt;strong&gt;1&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;The first moment is the &lt;strong&gt;mean&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;The second moment is the &lt;strong&gt;variance&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;The third moment is the &lt;strong&gt;skew&lt;/strong&gt; which measures how lopsided the distribution is.&lt;/li&gt;
  &lt;li&gt;The fourth moment is &lt;strong&gt;kurtosis&lt;/strong&gt; which is the measure of &lt;strong&gt;how sharp is the peak of the graph&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/technology-nineleaps/basics-of-statistics-for-machine-learning-engineers-ii-d25c5a5dac67&quot;&gt;(medium)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Moments are important because, under some assumptions, moments are a good estimate of how the population probability distribution is based on the sample distribution. We can even have a good feel of how far off the population moments are from our sample moments under some realistic assumptions. And once the population moments are known that means the shape of the population probability distribution is known as well.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;central-limit-theoram&quot;&gt;Central Limit Theoram&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;In probability theory, the central limit theorem (CLT) establishes that, in some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a “bell curve”) even if the original variables themselves are not normally distributed.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions. &lt;a href=&quot;https://en.wikipedia.org/wiki/Central_limit_theorem&quot;&gt;(wiki)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Formulation:&lt;/strong&gt;
Suppose that $[X_1, X_2 ,…,X_n]$ are i.i.d. random variables with expected values $E(X_{\large i})=\mu &amp;lt; \infty$ and variance $\mathrm{Var}(X_{\large i})=\sigma^2 &amp;lt; \infty$. Then as we saw above, the sample mean $\overline{X}={\large\frac{X_1+X_2+…+X_n}{n}}$ has mean $E(\overline{X})=\mu$ and variance $\mathrm{Var}(\overline{X})={\large \frac{\sigma^2}{n}}$. Thus the normalized random variable is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Z_{\large n}=\frac{\overline{X}-\mu}{ \sigma / \sqrt{n}}=\frac{X_1+X_2+...+X_{\large n}-n\mu}{\sqrt{n} \sigma}&lt;/script&gt;

&lt;p&gt;which has mean $E(Z_n)=0$ and variance $Var(Z_n)=1$, i.e the random variable $Z_n$ converges in distribution to the standard normal random variable as n goes to infinity, i.e&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lim_{n \rightarrow \infty} P(Z_{\large n} \leq x)=\Phi(x), \qquad \textrm{ for all }x \in \mathbb{R}&lt;/script&gt;

&lt;p&gt;where $\Phi(x)$ is the standard normal CDF.&lt;/p&gt;

&lt;p&gt;Let’s say $X$ follows Binomial distribution. Then as ${n \rightarrow \infty}$, the distribution of $Z_n$ looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/CLT.png&quot; alt=&quot;CLT&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.probabilitycourse.com/chapter7/7_1_2_central_limit_theorem.php&quot;&gt;(source)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=JNm3M9cqWyc&quot;&gt;Khan Academy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=FXZ2O1Lv-KE&quot;&gt;Khan Academy: Sampling Ditbn of Sample Mean&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;law-of-large-number&quot;&gt;Law of Large Number&lt;/h2&gt;

&lt;h2 id=&quot;variational-inference&quot;&gt;Variational Inference:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Variational-Inference-Foundations-and-Modern-Methods&quot;&gt;Nips 2016 Tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=iQNaAoXfKZ4&quot;&gt;Building Machines that Imagine and Reason Principles and Applications of Deep Generative Models Shak&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;expectation-maximization-em-gaussian-mixture-model&quot;&gt;Expectation Maximization (EM), Gaussian Mixture Model&lt;/h2&gt;

&lt;p&gt;Gaussian mixture models are a probabilistic model for representing normally distributed subpopulations within an overall population. Mixture models in general don’t require knowing which subpopulation a data point belongs to, allowing the model to learn the subpopulations automatically. Since subpopulation assignment is not known, this constitutes a form of &lt;code class=&quot;highlighter-rouge&quot;&gt;unsupervised learning&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For example, in modeling human height data, height is typically modeled as a normal distribution for each gender with a mean of approximately 5’10” for males and 5’5” for females. Given only the height data and not the gender assignments for each data point, the distribution of all heights would follow the sum of two scaled (different variance) and shifted (different mean) normal distributions. A model making this assumption is an example of a Gaussian mixture model (GMM), though in general a GMM may have more than two components. Estimating the parameters of the individual normal distribution components is a canonical problem in modeling data with GMMs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ds055uzetaobb.cloudfront.net/image_optimizer/d47c612ae8c3dc7f5aef1fc66458456f4eea4145.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A Gaussian mixture model is parameterized by two types of values, the mixture component weights and the component means and variances/covariances. For a Gaussian mixture model with $K$ components, the $k^{\text{th}}$ component has a mean of $\mu_k$ and variance of $\sigma_k$ for the &lt;code class=&quot;highlighter-rouge&quot;&gt;univariate case&lt;/code&gt; and a mean of $\vec{\mu}&lt;em&gt;k$ and covariance matrix of $\vec{\sigma}_k$ for the &lt;code class=&quot;highlighter-rouge&quot;&gt;multivariate case&lt;/code&gt;. The mixture component weights are defined as $\phi_k$ for component $C_k$, with the constraint that $\sum&lt;/em&gt;{i=1}^k \phi_i = 1$, so that the total probability distribution normalizes to $1$. If the component weights aren’t learned, they can be viewed as an &lt;code class=&quot;highlighter-rouge&quot;&gt;a-priori&lt;/code&gt; distribution over components such that . If they are instead learned, they are the &lt;code class=&quot;highlighter-rouge&quot;&gt;a-posteriori&lt;/code&gt; estimates of the component probabilities given the data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;One-dimensional Model:&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x) = \sum_{i=1}^K\phi_i \mathcal{N}(x \;|\; \mu_i, \sigma_i)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x \;|\; \mu_i, \sigma_i) = \frac{1}{\sigma_i\sqrt{2\pi}} \exp\left(-\frac{(x-\mu_i)^2}{2\sigma_i^2}\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^K\phi_i = 1&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Multi-dimensional Model&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\vec{x}) = \sum_{i=1}^K\phi_i \mathcal{N}(\vec{x} \;|\; \vec{\mu}_i, \Sigma_i)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(\vec{x} \;|\; \vec{\mu}_i, \Sigma_i) = \frac{1}{\sqrt{(2\pi)^K|\Sigma_i|}} \exp\left(-\frac{1}{2}(\vec{x}-\vec{\mu}_i)^\mathrm{T}{\Sigma_i}^{-1}(\vec{x}-\vec{\mu}_i)\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^K\phi_i = 1&lt;/script&gt;

&lt;h3 id=&quot;learning-the-model&quot;&gt;Learning the Model&lt;/h3&gt;

&lt;p&gt;If the number of components $K$ is known, expectation maximization is the technique most commonly used to estimate the mixture model’s parameters. In frequentist probability theory, models are typically learned by using maximum likelihood estimation techniques, which seek to maximize the probability, or likelihood, of the observed data given the model parameters. Unfortunately, finding the maximum likelihood solution for mixture models by differentiating the log likelihood and solving for 0 is usually analytically impossible.&lt;/p&gt;

&lt;p&gt;Expectation maximization (EM) is a numerical technique for maximum likelihood estimation, and is usually used when closed form expressions for updating the model parameters can be calculated (which will be shown below). Expectation maximization is an iterative algorithm and has the convenient property that the maximum likelihood of the data strictly increases with each subsequent iteration, meaning it is guaranteed to approach a local maximum or saddle point.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Read section &lt;code class=&quot;highlighter-rouge&quot;&gt;8.5.1&lt;/code&gt; from book &lt;code class=&quot;highlighter-rouge&quot;&gt;Element of Statistical Learning&lt;/code&gt; for an easy understanding.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Two Component Mixture Model:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Say we have $\mid Y \mid$ number of data-_points coming from 2 normal distribution, i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;mixture of 2 gaussian distribution&lt;/code&gt;. Where $Y_1 \sim N(\mu_1, \sigma_1)$ and $Y_2 \sim N(\mu_2, \sigma_2)$ and $\mid Y \mid = \mid Y_1 \mid + \mid Y_2 \mid$. Our task is to figure out those 2 distribution. More formally we want to estimate $\hat\mu_1, \hat\sigma_1$ and $\hat\mu_2, \hat\sigma_2$. We want to model $Y$ as follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y = (1-\Delta)Y_1 + \Delta Y_2&lt;/script&gt;

&lt;p&gt;where $\Delta \in {0,1}$ with $P(\Delta=1)=\pi$.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;For simplicity we can think that $\mid Y \mid$ number of blue and yellow balls are there, where number of blue balls $\mid Y_1 \mid$ and number of yellow balls $Y_2$. Blue balls come from 1 normal distribution and yellow balls come from another normal distribution. Now given a ball we need to figure out whether the balls come from 1st Normal distribution (blue) or 2nd normal distribution. We can easily solve this using &lt;code class=&quot;highlighter-rouge&quot;&gt;K-Means&lt;/code&gt; clustering. But K-Means is a &lt;code class=&quot;highlighter-rouge&quot;&gt;Hard-Clustering&lt;/code&gt; problem. Where assignment probability is 0 or 1. But Mixture model is a &lt;code class=&quot;highlighter-rouge&quot;&gt;Soft Clustering&lt;/code&gt; problem and an iterative process and the assignment probability is $\in [0,1]$.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the above notation we can think of $\pi$ as the mixing coefficient. We can think it as a &lt;code class=&quot;highlighter-rouge&quot;&gt;prior probability&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Let $\phi_\theta(x)$ denotes the normal density with parameters $\theta = (\mu, \sigma^2)$. Then the density of $Y$ is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g_Y=(1-\pi)\phi_\theta^1(y) + \pi\phi_\theta^2(y)&lt;/script&gt;

&lt;p&gt;Now we wish to fit this model to data by maximum likelihood estimation. The parameters are $\theta = (\pi, \theta^1, \theta^2) = (\pi, \mu_1, \sigma_1^2, \mu_2, \sigma_2^2)$.&lt;/p&gt;

&lt;p&gt;The log likelihood based on all the training case is:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\theta; Z) = \sum_{i=1}^{N}log[(1-\pi)\phi_\theta^1(y_i) + \pi\phi_\theta^2(y_i)]&lt;/script&gt;

&lt;p&gt;But direct maximization (by taking grad and set to 0) is difficult due to the sum inside log.&lt;/p&gt;

&lt;p&gt;So we apply Expectation Maximization (EM) algorithm.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;s1: guess: $\pi, \theta_1, \theta_2$&lt;/li&gt;
  &lt;li&gt;s2: While $(oldLogLikelihood - Likelihood) &amp;gt; tolerance$
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;s3: &lt;strong&gt;Expectaton Step:&lt;/strong&gt; Find Posterior Probability (&lt;code class=&quot;highlighter-rouge&quot;&gt;Responsibility&lt;/code&gt;)&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat\gamma_i = p(\theta_2|y_i) = \frac{p(y_i | \theta_2)p(\theta_2)}{p(y_i | \theta_1)p(\theta_1) + p(y_i |\theta_2)p(\theta_2)}&lt;/script&gt;

        &lt;ul&gt;
          &lt;li&gt;Where $p(\theta_2) = \pi$, $p(\theta_1) = 1-\pi$&lt;/li&gt;
          &lt;li&gt;$p(y_i) = p(y_i \vert \theta_1)p(\theta_1) + p(y_i \vert\theta_2)p(\theta_2)$&lt;/li&gt;
          &lt;li&gt;$p(y_i \vert \theta_2) = N(\mu_2, \sigma_2)$ [use from s1 (1st time) and s4 (later)]&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;s4: &lt;strong&gt;Maximization Step:&lt;/strong&gt; compute the weighted mean and variances&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat\mu_1 = \frac{\Sigma(1-\hat\gamma_i)y_i}{\Sigma{1-\hat\gamma_i}}&lt;/script&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat\mu_2 = \frac{\Sigma(\hat\gamma_i)y_i}{\Sigma{\hat\gamma_i}}&lt;/script&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat\sigma_1 = \frac{\Sigma (1-\hat\gamma_i) (y_i - \hat\mu_1)^2}{\Sigma(1-\hat\gamma_i)}&lt;/script&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat\sigma_2 = \frac{\Sigma \hat\gamma_i (y_i - \hat\mu_2)^2}{\Sigma{\hat\gamma_i}}&lt;/script&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat\pi = \Sigma \frac{\hat\gamma_i}{N}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;s5: calculate loglikelihood:&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;l(\theta; Z) = \sum_{i=1}^{N}log[(1-\hat\pi)\hat\phi_\theta^1(y_i) + \hat\pi\hat\phi_\theta^2(y_i)]&lt;/script&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So we are trying to maximize the loglikelihood through this iterative approach. The above algorithm is a simple version for 1D data. If the data is 2D then instead of variance $\sigma^2$, we will have covariance matrix $\Sigma$ because of Multivariate Gaussian. But the basic procedure is like this.&lt;/p&gt;

&lt;p&gt;For more details, read the following references.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=REypj2sy_5U&quot;&gt;YouTube&lt;/a&gt;, 
[Elements of Statistical Learning, section &lt;code class=&quot;highlighter-rouge&quot;&gt;8.5.1&lt;/code&gt;]&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.blackarbs.com/blog/intro-to-expectation-maximization-k-means-gaussian-mixture-models-with-python-sklearn/3/20/2017&quot;&gt;code_implementation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://people.duke.edu/~ccc14/sta-663/EMAlgorithm.html&quot;&gt;EM_algo_python_code&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://emmanuel-klinger.net/expectation-maximization-in-python.html&quot;&gt;code_other&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;[book_Simon_Prince_ch7]&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html&quot;&gt;imp_source_1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://brilliant.org/wiki/gaussian-mixture-model/&quot;&gt;imp_source_2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;monte-carlo-gradient-estimate&quot;&gt;&lt;strong&gt;Monte carlo Gradient Estimate&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Comprehensive Survey&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1906.10652.pdf&quot;&gt;ArXiv: Shakir&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Sankarshan Mridha</name></author><summary type="html">What is P Value? In statistical hypothesis testing, the p-value or probability value is, for a given statistical model, the probability that, when the null hypothesis is true, the statistical summary (such as the absolute value of the sample mean difference between two compared groups) would be greater than or equal to the actual observed results. Hypothesis Testing Normal Distribution What is P-value? Statistical Significance Before we talk about what p-value means, let’s begin by understanding hypothesis testing where p-value is used to determine the statistical significance of our results. Hypothesis testing is used to test the validity of a claim (null hypothesis) that is made about a population using sample data. The alternative hypothesis is the one you would believe if the null hypothesis is concluded to be untrue. The lower the p-value, the more surprising the evidence is, the more ridiculous our null hypothesis looks. If the p-value is lower than a predetermined significance level (people call it alpha, I call it the threshold of being ridiculous — don’t ask my why, I just find it easier for me to understand), then we reject the null hypothesis. In my opinion, p-values are used as a tool to challenge our initial belief (null hypothesis) when the result is statistically significant. The moment we feel ridiculous with our own belief (provided the p-value shows the result is statistically significant), we discard our initial belief (reject the null hypothesis) and make a reasonable decision. Statistical Significance Finally, this is the final stage where we put everything together and test if the result is statistically significant. Having just the p-value is not enough, we need to set a threshold (aka significance level — alpha). The alpha should always be set before an experiment to avoid bias. If the observed p-value is lower than alpha, then we conclude that the result is statistically significant. The rule of thumb is to set alpha to be either 0.05 or 0.01 (again, the value depends on your problems at hand). Reference: Very Very IMP TDS What is p-values? How it is decided for rejecting null hypothesis? Why it’s called null hypothesis? Null hypothesis means the hypothesis which you want to nullify. So there is an alternate hypothesis, which will be accepted if the null hypothesis is rejected. A p-value is the probability of finding some sample outcome or a more extreme one if the null hypothesis is true. Example: I want to know if happiness is related to wealth among Dutch people. One approach to find this out is to formulate a null hypothesis. Since “related to” is not precise, we choose the opposite statement as our null hypothesis: The correlation between wealth and happiness is zero among all Dutch people. We’ll now try to refute this hypothesis in order to demonstrate that happiness and wealth are related all right. Now, we can’t reasonably ask all 17,142,066 Dutch people how happy they generally feel. So we’ll ask a sample (say, 100 people) about their wealth and their happiness. The correlation between happiness and wealth turns out to be 0.25 in our sample. Now we’ve one problem: sample outcomes tend to differ somewhat from population outcomes. How we can ever say anything about our population if we only have a tiny sample from it. So how does that work? Well, basically, some sample outcomes are highly unlikely given our null hypothesis. If our population correlation really is zero, then we can find a sample correlation of 0.25 in a sample of N = 100. The probability of this happening is only 0.012. So it’s very unlikely. A reasonable conclusion is that our population correlation wasn’t zero after all. Conclusion: we reject the null hypothesis. Given our sample outcome, we no longer believe that happiness and wealth are unrelated. However, we still can’t state this with certainty. source, link2 Why bayesian inference can be difficult? Determining the posterior distribution directly from the Byes’ rule involes coputing the evidence i.e marginal likelihood. For continutios parameters, the integral can be impossible to solve analytically. Historically the difficulty of the integration was bypassed by restricting the models to relitevely simple likelihood functions with corresponding formulas for prior distribution - conjugate prior, that played nice with the likelihood functions to give us a tractable solution. When the conjugate prior approach doesn’t work, another approach is the approximate the original functions with other functions, which are easier to work with and then show that the approximate is reasonably good under typical conditions. This is known as Variational Approximation. Another kind of approximation involves, randomly sampling large number of representative combinations of parameter values from the posterior distribution. These types of algorithms are known as MCMC algorithm. These methods help to calculate the posterior distribution without calculating the integral. How does Gibbs Sampling work? Gibbs Sampling is a MCMC method to draw samples from a potentially really really complicated, high dimensional distribution, where analytically, it’s hard to draw samples from it. The usual suspect would be those nasty integrals when computing the normalizing constant of the distribution, especially in Bayesian inference. Now Gibbs Sampler can draw samples from any distribution, provided you can provide all of the conditional distributions of the joint distribution analitically. Github Video Lecture Importance Sampling and Monte Carlo Shakir Blog Shakir Md - MLSS 2019 London Note Distribution While the concept of probability gives us the mathematical calculations, distributions help us actually visualize what’s happening underneath. (AVB) Uniform Distribution A variable X is said to be uniformly distributed if the density function is: where $-\infty&amp;lt;a&amp;lt;=x&amp;lt;=b&amp;lt;\infty$ Bernoulli Distribution Story: All you cricket junkies out there! At the beginning of any cricket match, how do you decide who is going to bat or ball? A toss! It all depends on whether you win or lose the toss, right? Let’s say if the toss results in a head, you win. Else, you lose. There’s no midway. Formulation: A Bernoulli distribution has only two possible outcomes, namely 1 (success) and 0 (failure), and a single trial. So the random variable $X$ which has a Bernoulli distribution can take value 1 with the probability of success, say p, and the value 0 with the probability of failure, say q or 1-p. The probability mass function is given by: where $x \in (0, 1)$. The expected value of a random variable X from a Bernoulli distribution is found as follows: The variance of a random variable from a bernoulli distribution is: Binomial Distribution Story: Let’s get back to cricket. Suppose that you won the toss today and this indicates a successful event. You toss again but you lost this time. If you win a toss today, this does not necessitate that you will win the toss tomorrow. Let’s assign a random variable, say X, to the number of times you won the toss. What can be the possible value of X? It can be any number depending on the number of times you tossed a coin. Formulation: An experiment with only two possible outcomes repeated n number of times is called binomial. The parameters of a binomial distribution are n and p where n is the total number of trials and p is the probability of success in each trial. If x is the total number fof success, we can write: On the basis of the above explanation, the properties of a Binomial Distribution are: Each trial is independent. There are only two possible outcomes in a trial- either a success or a failure. A total number of n identical trials are conducted. The probability of success and failure is same for all trials. (Trials are identical.) Mean: $\mu = n*p$ Variance: Normal Distribution: Normal distribution represents the behavior of most of the situations in the universe (That is why it’s called a “normal” distribution. I guess!). The large sum of (small) random variables often turns out to be normally distributed, contributing to its widespread application ~ Central Limit Theoram Any distribution is known as Normal distribution if it has the following characteristics: The mean, median and mode of the distribution coincide. The curve of the distribution is bell-shaped and symmetrical about the line $x=\mu$ . The total area under the curve is 1. Exactly half of the values are to the left of the center and the other half to the right. A normal distribution is highly different from Binomial Distribution. However, if the number of trials approaches infinity then the shapes will be quite similar. The PDF of a random variable X following a normal distribution is given by: where $-\infty&amp;lt;x&amp;lt;\infty$ A standard normal distribution looks as follows: Poisson Distribution Story: Suppose you work at a call center, approximately how many calls do you get in a day? It can be any number. Now, the entire number of calls at a call center in a day is modeled by Poisson distribution. Some more examples are The number of emergency calls recorded at a hospital in a day. The number of thefts reported in an area on a day. The number of customers arriving at a salon in an hour. Poisson Distribution is applicable in situations where events occur at random points of time and space wherein our interest lies only in the number of occurrences of the event. A distribution is called Poisson distribution when the following assumptions are valid: Any successful event should not influence the outcome of another successful event. The probability of success over a short interval must equal the probability of success over a longer interval. The probability of success in an interval approaches zero as the interval becomes smaller. Formulation: Now, if any distribution validates the above assumptions then it is a Poisson distribution. Some notations used in Poisson distribution are: $\lambda$ is the rate at which an event occurs, $t$ is the length of a time interval, $X$ is the number of events in that time interval. Here, $X$ is called a Poisson Random Variable and the probability distribution of $X$ is called Poisson distribution. Let $\mu$ denote the mean number of events in an interval of length t. Then, $\mu = \lambda*t$. The PMF of X following a Poisson distribution is given by: where $x=0,1,2,3,…$ Mean: $E(X) = \mu$ Variance: $Var(X) = \mu$ Exponential Distribution Story: Let’s consider the call center example one more time. What about the interval of time between the calls? Here, exponential distribution comes to our rescue. Exponential distribution models the interval of time between the calls. Other examples are: Length of time beteeen metro arrivals, Length of time between arrivals at a gas station The life of an Air Conditioner Exponential distribution is widely used for survival analysis. From the expected life of a machine to the expected life of a human, exponential distribution successfully delivers the result. Formulation: A random variable $X$ is said to have an exponential distribution with PDF: where $x ≥ 0$ and parameter $\lambda&amp;gt;0$ which is also called the rate. For survival analysis, $\gamma$ is called the failure rate of a device at any time t, given that it has survived up to t. Mean and Variance of a random variable X following an exponential distribution: Mean: $E(X) = \frac{1}{\lambda}$ Variance: $Var(X) = \frac{1}{\lambda^2}$ Gamma Distribution: We now define the gamma distribution by providing its PDF: A continuous random variable $X$ is said to have a gamma distribution with parameters $\alpha&amp;gt;0$ and $\lambda&amp;gt;0$, shown as $X \sim Gamma(\alpha,\lambda)$, if its PDF is given by: If we let $\alpha=1$, we obtain: Thus, we conclude $Gamma(1,\lambda)=Exponential(\lambda)$. More generally, if you sum n independent $Exponential(\lambda)$ random variables, then you will get a $Gamma(n,\lambda)$ random variable. (gamma) How all the distributions are related? (blog) Relation between Exponential and Poisson Distribution: If the times between random events follow exponential distribution with rate $\lambda$, then the total number of events in a time period of length t follows the Poisson distribution with parameter $\lambda t$. Moments - Shape of You !! Moments try to measure the shape of the probability distribution function. The zeroth moment is the total probability of the distribution which is 1. The first moment is the mean. The second moment is the variance. The third moment is the skew which measures how lopsided the distribution is. The fourth moment is kurtosis which is the measure of how sharp is the peak of the graph. (medium) Moments are important because, under some assumptions, moments are a good estimate of how the population probability distribution is based on the sample distribution. We can even have a good feel of how far off the population moments are from our sample moments under some realistic assumptions. And once the population moments are known that means the shape of the population probability distribution is known as well. Central Limit Theoram In probability theory, the central limit theorem (CLT) establishes that, in some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a “bell curve”) even if the original variables themselves are not normally distributed. The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions. (wiki) Formulation: Suppose that $[X_1, X_2 ,…,X_n]$ are i.i.d. random variables with expected values $E(X_{\large i})=\mu &amp;lt; \infty$ and variance $\mathrm{Var}(X_{\large i})=\sigma^2 &amp;lt; \infty$. Then as we saw above, the sample mean $\overline{X}={\large\frac{X_1+X_2+…+X_n}{n}}$ has mean $E(\overline{X})=\mu$ and variance $\mathrm{Var}(\overline{X})={\large \frac{\sigma^2}{n}}$. Thus the normalized random variable is: which has mean $E(Z_n)=0$ and variance $Var(Z_n)=1$, i.e the random variable $Z_n$ converges in distribution to the standard normal random variable as n goes to infinity, i.e where $\Phi(x)$ is the standard normal CDF. Let’s say $X$ follows Binomial distribution. Then as ${n \rightarrow \infty}$, the distribution of $Z_n$ looks like this: (source) Khan Academy Khan Academy: Sampling Ditbn of Sample Mean Law of Large Number Variational Inference: Nips 2016 Tutorial Building Machines that Imagine and Reason Principles and Applications of Deep Generative Models Shak Expectation Maximization (EM), Gaussian Mixture Model Gaussian mixture models are a probabilistic model for representing normally distributed subpopulations within an overall population. Mixture models in general don’t require knowing which subpopulation a data point belongs to, allowing the model to learn the subpopulations automatically. Since subpopulation assignment is not known, this constitutes a form of unsupervised learning. For example, in modeling human height data, height is typically modeled as a normal distribution for each gender with a mean of approximately 5’10” for males and 5’5” for females. Given only the height data and not the gender assignments for each data point, the distribution of all heights would follow the sum of two scaled (different variance) and shifted (different mean) normal distributions. A model making this assumption is an example of a Gaussian mixture model (GMM), though in general a GMM may have more than two components. Estimating the parameters of the individual normal distribution components is a canonical problem in modeling data with GMMs. A Gaussian mixture model is parameterized by two types of values, the mixture component weights and the component means and variances/covariances. For a Gaussian mixture model with $K$ components, the $k^{\text{th}}$ component has a mean of $\mu_k$ and variance of $\sigma_k$ for the univariate case and a mean of $\vec{\mu}k$ and covariance matrix of $\vec{\sigma}_k$ for the multivariate case. The mixture component weights are defined as $\phi_k$ for component $C_k$, with the constraint that $\sum{i=1}^k \phi_i = 1$, so that the total probability distribution normalizes to $1$. If the component weights aren’t learned, they can be viewed as an a-priori distribution over components such that . If they are instead learned, they are the a-posteriori estimates of the component probabilities given the data. One-dimensional Model: Multi-dimensional Model Learning the Model If the number of components $K$ is known, expectation maximization is the technique most commonly used to estimate the mixture model’s parameters. In frequentist probability theory, models are typically learned by using maximum likelihood estimation techniques, which seek to maximize the probability, or likelihood, of the observed data given the model parameters. Unfortunately, finding the maximum likelihood solution for mixture models by differentiating the log likelihood and solving for 0 is usually analytically impossible. Expectation maximization (EM) is a numerical technique for maximum likelihood estimation, and is usually used when closed form expressions for updating the model parameters can be calculated (which will be shown below). Expectation maximization is an iterative algorithm and has the convenient property that the maximum likelihood of the data strictly increases with each subsequent iteration, meaning it is guaranteed to approach a local maximum or saddle point. Note: Read section 8.5.1 from book Element of Statistical Learning for an easy understanding. Two Component Mixture Model: Say we have $\mid Y \mid$ number of data-_points coming from 2 normal distribution, i.e. mixture of 2 gaussian distribution. Where $Y_1 \sim N(\mu_1, \sigma_1)$ and $Y_2 \sim N(\mu_2, \sigma_2)$ and $\mid Y \mid = \mid Y_1 \mid + \mid Y_2 \mid$. Our task is to figure out those 2 distribution. More formally we want to estimate $\hat\mu_1, \hat\sigma_1$ and $\hat\mu_2, \hat\sigma_2$. We want to model $Y$ as follows where $\Delta \in {0,1}$ with $P(\Delta=1)=\pi$. For simplicity we can think that $\mid Y \mid$ number of blue and yellow balls are there, where number of blue balls $\mid Y_1 \mid$ and number of yellow balls $Y_2$. Blue balls come from 1 normal distribution and yellow balls come from another normal distribution. Now given a ball we need to figure out whether the balls come from 1st Normal distribution (blue) or 2nd normal distribution. We can easily solve this using K-Means clustering. But K-Means is a Hard-Clustering problem. Where assignment probability is 0 or 1. But Mixture model is a Soft Clustering problem and an iterative process and the assignment probability is $\in [0,1]$. In the above notation we can think of $\pi$ as the mixing coefficient. We can think it as a prior probability. Let $\phi_\theta(x)$ denotes the normal density with parameters $\theta = (\mu, \sigma^2)$. Then the density of $Y$ is Now we wish to fit this model to data by maximum likelihood estimation. The parameters are $\theta = (\pi, \theta^1, \theta^2) = (\pi, \mu_1, \sigma_1^2, \mu_2, \sigma_2^2)$. The log likelihood based on all the training case is: But direct maximization (by taking grad and set to 0) is difficult due to the sum inside log. So we apply Expectation Maximization (EM) algorithm. s1: guess: $\pi, \theta_1, \theta_2$ s2: While $(oldLogLikelihood - Likelihood) &amp;gt; tolerance$ s3: Expectaton Step: Find Posterior Probability (Responsibility) Where $p(\theta_2) = \pi$, $p(\theta_1) = 1-\pi$ $p(y_i) = p(y_i \vert \theta_1)p(\theta_1) + p(y_i \vert\theta_2)p(\theta_2)$ $p(y_i \vert \theta_2) = N(\mu_2, \sigma_2)$ [use from s1 (1st time) and s4 (later)] s4: Maximization Step: compute the weighted mean and variances s5: calculate loglikelihood: So we are trying to maximize the loglikelihood through this iterative approach. The above algorithm is a simple version for 1D data. If the data is 2D then instead of variance $\sigma^2$, we will have covariance matrix $\Sigma$ because of Multivariate Gaussian. But the basic procedure is like this. For more details, read the following references. YouTube, [Elements of Statistical Learning, section 8.5.1] code_implementation EM_algo_python_code code_other [book_Simon_Prince_ch7] imp_source_1 imp_source_2 Monte carlo Gradient Estimate Comprehensive Survey ArXiv: Shakir</summary></entry><entry><title type="html">Blog 001: Learning Curve</title><link href="http://localhost:4000/blog/2019/07/28/blog_001_Learning_Curve" rel="alternate" type="text/html" title="Blog 001: Learning Curve" /><published>2019-07-28T00:11:31+05:30</published><updated>2019-07-28T00:11:31+05:30</updated><id>http://localhost:4000/blog/2019/07/28/blog_001_Learning_Curve</id><content type="html" xml:base="http://localhost:4000/blog/2019/07/28/blog_001_Learning_Curve">&lt;h1 id=&quot;learning-curve&quot;&gt;Learning Curve&lt;/h1&gt;

&lt;p&gt;With the tremendous accelaration in the Artificial Intelligence domain, it’s difficult to cover everything. Depending on my personal interest, below image shows my target to achieve. It’s like tuning different hyperparameter to create a desiarable profile in Data Science field.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ML_DL_LEARNING_CURVE.jpg&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;</content><author><name>Sankarshan Mridha</name></author><summary type="html">Learning Curve</summary></entry><entry><title type="html">Blog 100: Machine Learning Concepts</title><link href="http://localhost:4000/blog/2019/07/28/blog_100_ML_Concepts" rel="alternate" type="text/html" title="Blog 100: Machine Learning Concepts" /><published>2019-07-28T00:11:31+05:30</published><updated>2019-07-28T00:11:31+05:30</updated><id>http://localhost:4000/blog/2019/07/28/blog_100_ML_Concepts</id><content type="html" xml:base="http://localhost:4000/blog/2019/07/28/blog_100_ML_Concepts">&lt;h1 id=&quot;machine-learning-concepts-clearing&quot;&gt;Machine Learning Concepts Clearing&lt;/h1&gt;

&lt;h2 id=&quot;linear-regression&quot;&gt;Linear Regression&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;It’s linear with respect to weight &lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt;, but not with respect to input &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;….&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;posterior ~ liekelihood * prior&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Ordinary Least Square (OLS) approach to find the model parameters is a special case of maximum likelihood estimation and the overfitting problem is a general property of the MLE. But by adopting the Bayesian approach, the overfitting problem can be avoided. &lt;code class=&quot;highlighter-rouge&quot;&gt;[p9, Bishop]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Also from Bayesian perspective we can use model for which number of parameters can exceed the number of training data. In Bayesian learning, the effective number of parameters adapts automatically to the size of the data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Point Estimate of W vs W Distribution&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Consider D is our dataset and w is the parameter set. Now in both Bayesian and frequentist paradigm, the likelihood function &lt;code class=&quot;highlighter-rouge&quot;&gt;p(D|w)&lt;/code&gt; plays a central role. In frequentist approach, w is considered to be fixed parameter, whose value is determined by some form of estimator and the error bars on the estimator are obtained by considering the distribution of possible data sets D.&lt;/p&gt;

&lt;p&gt;However, in Bayesian setting we have only one sigle datasets D (the observed datasets), and the uncertainty in parameters is expressed through a probability distribution over &lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;[p22-p23]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;A widely used frequentist estimator is the maximul likelihood, in which w is set to the value that maximizes &lt;code class=&quot;highlighter-rouge&quot;&gt;p(D|w)&lt;/code&gt;. In the ML literature, the negative log of the likelihood is considered as &lt;em&gt;error function&lt;/em&gt;. As the negative log is a monotonically decreasing function, so maximizing the likelihood is equivalent to minimizing the error.&lt;/p&gt;

&lt;p&gt;However Bayesian approach has a very common criticism, i.e. the inclusion of prior belief. Therefore, people try to use &lt;em&gt;noninformative prior&lt;/em&gt; to get rid of the prior dependences. &lt;code class=&quot;highlighter-rouge&quot;&gt;[p23]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Criticism for MLE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;MLE suffers from &lt;code class=&quot;highlighter-rouge&quot;&gt;overfitting&lt;/code&gt; problem. In general overfitting means the model fitted to the training data so perfectly that if we slightly change the data and get prediction, test error will be very high. That means the model is sensitive to the variance of data. The theoretical reason is MLE systematically undermines the variance of the distribution. See proof &lt;code class=&quot;highlighter-rouge&quot;&gt;[[p27], [figure 1.15,p26, p28]]&lt;/code&gt;.  Because here &lt;code class=&quot;highlighter-rouge&quot;&gt;sample variance&lt;/code&gt; is measured using the &lt;code class=&quot;highlighter-rouge&quot;&gt;sample mean&lt;/code&gt;, instead of the population mean.&lt;/li&gt;
  &lt;li&gt;Sample mean is an unbiased estimator of the population mean, but sample variance is a biased estimator of the population variance. &lt;code class=&quot;highlighter-rouge&quot;&gt;[p27], [figure 1.15,p26]&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;If you see the image &lt;code class=&quot;highlighter-rouge&quot;&gt;p28&lt;/code&gt;, the 3 red curve shows 3 different dataset and the green curve shows the true dataset. And the mean of the 3 red curves coincide with the mean of the green curve, but their variances are different.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In Bayesian curve fitting setting, the sum of squared error function has arisen as a consequence of maximizing the likelihood under the assumption of Gaussian noise distribution.&lt;/p&gt;

&lt;p&gt;Regularization allows complex models to be trained on the datasets of limited size without severe overfitting, essentially by limiting the model complexity. &lt;code class=&quot;highlighter-rouge&quot;&gt;[p145, bishop]&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bias Variance Trade-off from Frequentist viewpoint&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In frequentist viewpoint, &lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt; is fixed and error bars on the estimators are obtained by considering the distribution over the data D. &lt;code class=&quot;highlighter-rouge&quot;&gt;[p22-23; Bishop]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Suppose we have large number of &lt;strong&gt;data sets&lt;/strong&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;[D1,...,Dn]&lt;/code&gt;, each of size N and each drawn independently from distribution of &lt;code class=&quot;highlighter-rouge&quot;&gt;p(t,x)&lt;/code&gt;. For any given data set &lt;code class=&quot;highlighter-rouge&quot;&gt;Di&lt;/code&gt; we can run our learning algorithm and get a prediction function &lt;code class=&quot;highlighter-rouge&quot;&gt;y(x;Di)&lt;/code&gt;. Different datasets from the ensemble will give different prediction functions and consequently different values of squared loss. The performance of a particular learning algorithm is then assessed by averaging over this ensemble of datasets. &lt;code class=&quot;highlighter-rouge&quot;&gt;[p148; Bishop]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Our original regression function is &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; and say for &lt;code class=&quot;highlighter-rouge&quot;&gt;Di&lt;/code&gt; we got our predictive function $\hat{Y_i}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bias&lt;/strong&gt; = $(E[\hat{Y_i}(x;D_i)] - Y)^2$, where $E[\hat{Y_i}(x;D_i)]$ is average (expected) performance over all the datasets. So, Bias represents the extent to which the average prediction over all the datasets $D_i$ differ from the desired regression function $Y$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Variance&lt;/strong&gt; = $E[(\hat{Y_i}(x;D_i) - E[\hat{Y_i}(x;D_i)])^2]$, where $(\hat{Y_i}(x;D_i)$ is the predictive function over data set $D_i$ and $E[\hat{Y_i}(x;D_i)])$ is the average performance over all the datasets. So variance represents, the extent to which the individual predictive functions $\hat{Y_i}$ for dataset $D_i$ varies around their average. And thus we measure the extent by which the function $Y(x;D)$ is sensitive to the particular choice of the data sets. &lt;code class=&quot;highlighter-rouge&quot;&gt;[p149; Bishop]&lt;/code&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;linear-models&quot;&gt;Linear Models&lt;/h2&gt;

&lt;h3 id=&quot;general-linear-model&quot;&gt;General Linear Model&lt;/h3&gt;

&lt;p&gt;Indeed, the general linear model can be seen as an extension of linear multiple regression for a single dependent variable. Understanding the multiple regression model is fundamental to understand the general linear model.&lt;/p&gt;

&lt;h4 id=&quot;single-regression&quot;&gt;Single Regression&lt;/h4&gt;
&lt;p&gt;One independent variable and one dependent variable&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y=\theta_0+\theta x_1&lt;/script&gt;

&lt;h4 id=&quot;multiple-regression&quot;&gt;Multiple Regression&lt;/h4&gt;
&lt;p&gt;Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Multiple linear regression is the most common form of linear regression analysis.  As a predictive analysis, the multiple linear regression is used to explain the relationship between one continuous dependent variable and two or more independent variables&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y=\theta_0+\theta x_1+\theta x_2+...+\theta x_n&lt;/script&gt;

&lt;h4 id=&quot;additive-model&quot;&gt;Additive Model:&lt;/h4&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y=\theta_0+\theta f_1(x_1)+\theta f_2(x_2)+...+\theta f_n(x_n)&lt;/script&gt;

&lt;p&gt;A generalization of the multiple regression model would be to maintain the additive nature of the model, but to replace the simple terms of the linear equation $\theta_i * x_i$ with $f_i(x_i)$ where $f_i()$ is a non-parametric function of the predictor $x_i$.  In other words, instead of a single coefficient for each variable (additive term) in the model, in additive models an unspecified (non-parametric) function is estimated for each predictor, to achieve the best prediction of the dependent variable values.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;General Linear Model - Revisited&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One way in which the &lt;code class=&quot;highlighter-rouge&quot;&gt;general linear model&lt;/code&gt; differs from the &lt;code class=&quot;highlighter-rouge&quot;&gt;multiple regression model&lt;/code&gt; is in terms of the number of dependent variables that can be analyzed. The $Y$ vector of $n$ observations of a single $Y$ variable can be replaced by a $Y$ matrix of $n$ observations of $m$ different $Y$ variables. Similarly, the $w$ vector of regression coefficients for a single $Y$ variable can be replaced by a $W$ matrix of regression coefficients, with one vector of $w$ coefficients for each of the m dependent variables. These substitutions yield what is sometimes called the multivariate regression model,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{Y}={XW}+{E}&lt;/script&gt;

&lt;p&gt;where $Y$ is a matrix with series of multivariate measurements (each column being a set of measurements on one of the dependent variables), $X$ is a matrix of observations on independent variables that might be a design matrix (each column being a set of observations on one of the independent variables), $W$ is a matrix containing parameters that are usually to be estimated and $E$ is a matrix containing errors (noise). The errors are usually assumed to be uncorrelated across measurements, and follow a multivariate normal distribution. If the errors do not follow a multivariate normal distribution, generalized linear models may be used to relax assumptions about $Y$ and $W$.&lt;/p&gt;

&lt;h3 id=&quot;generalized-linear-model&quot;&gt;Generalized Linear Model&lt;/h3&gt;

&lt;p&gt;To summarize the basic idea, the &lt;code class=&quot;highlighter-rouge&quot;&gt;generalized linear model&lt;/code&gt; differs from the &lt;code class=&quot;highlighter-rouge&quot;&gt;general linear model&lt;/code&gt; (of which multiple regression is a special case) in two major respects:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;The distribution of the dependent or response variable &lt;em&gt;can be (explicitly) non-normal&lt;/em&gt;, and does not have to be continuous, e.g., it can be binomial;&lt;/li&gt;
  &lt;li&gt;The dependent variable values are predicted from a linear combination of predictor variables, which are “connected” to the dependent variable via a &lt;code class=&quot;highlighter-rouge&quot;&gt;link function&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;General Linear Model&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y=\theta_0+\theta x_1+\theta x_2+...+\theta x_n&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Generalized Linear Model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;y=g(\theta_0+\theta x_1+\theta x_2+...+\theta x_n)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;g^{-i}(y)=\theta_0+\theta x_1+\theta x_2+...+\theta x_n&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where $g^{-i}()$ is the inverse of $g()$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Link Function:&lt;/strong&gt;
Inverse of $g()$, say $g^{-i}()$ is the link function.&lt;/p&gt;

&lt;h3 id=&quot;generalized-additive-model&quot;&gt;Generalized Additive Model:&lt;/h3&gt;

&lt;p&gt;We can combine the notion of &lt;code class=&quot;highlighter-rouge&quot;&gt;additive models&lt;/code&gt; with &lt;code class=&quot;highlighter-rouge&quot;&gt;generalized linear models&lt;/code&gt;, to derive the notion of &lt;code class=&quot;highlighter-rouge&quot;&gt;generalized additive models&lt;/code&gt;, as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;g^{-i}(y)=\theta_0+\theta f_1(x_1)+\theta f_2(x_2)+...+\theta f_n(x_n)&lt;/script&gt;

&lt;p&gt;&lt;a href=&quot;http://www.statsoft.com/Textbook/Generalized-Additive-Models&quot;&gt;(link)&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;why-do-we-call-it-glm-when-its-clearly-non-linear&quot;&gt;&lt;strong&gt;Why do we call it GLM when it’s clearly non-linear?&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Linear Model:&lt;/strong&gt; They are linear in parameters $\theta$
&lt;script type=&quot;math/tex&quot;&gt;y_i = \theta_0 + \Sigma_{i=1}^{n} \theta_i x_i&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Non Linear Model:&lt;/strong&gt; They are non-linear in parameters $\theta$
&lt;script type=&quot;math/tex&quot;&gt;y_i = \theta_0 + \Sigma_{i=1}^{n} g(\theta_i) x_i&lt;/script&gt;
where $g()$ is any non linear function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;GLM:&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;”..&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;inherently nonlinear&lt;/code&gt;&lt;/strong&gt; (they are nonlinear in the parameters), yet &lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;transformably linear&lt;/code&gt;&lt;/strong&gt;*, and thus fall under the GLM framework..”&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now GLM in &lt;code class=&quot;highlighter-rouge&quot;&gt;non-linear&lt;/code&gt; due to the presence of $g()$ but it can be transformed into &lt;code class=&quot;highlighter-rouge&quot;&gt;linears in parameters&lt;/code&gt; using &lt;code class=&quot;highlighter-rouge&quot;&gt;link function&lt;/code&gt; $g^{-i}()$.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;stackexchange&quot;&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/120047/nonlinear-vs-generalized-linear-model-how-do-you-refer-to-logistic-poisson-e&quot;&gt;Stackexchange&lt;/a&gt;&lt;/h2&gt;
    &lt;h2 id=&quot;eigen-decomposition&quot;&gt;Eigen Decomposition&lt;/h2&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Matrices acts as linear transformations. Some matrices will rotate your space, others will rescale it etc. So when we apply a matrix to a vector, we end up with a transformed version of the vector. When we say that we ‘apply’ the matrix to the vector it means that we calculate the dot product of the matrix with the vector.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eigenvector:&lt;/strong&gt; Now imagine that the transformation of the initial vector gives us a new vector that has the exact same direction. The scale can be different but the direction is the same. Applying the matrix didn’t change the direction of the vector. Thefere this type of initial vector is special and called an eigenvector of the matrix.&lt;/p&gt;

&lt;p&gt;We can decompose the matrix A with eigenvectors and eigenvalues. It is done with: $A=V* \Sigma * V^{−1}$ , where $\Sigma = diag(\lambda)$ and each column of $V$ is the eigenvector of $A$.&lt;/p&gt;

&lt;h3 id=&quot;real-symmetric-matrix&quot;&gt;Real symmetric matrix:&lt;/h3&gt;

&lt;p&gt;In the case of real symmetric matrices, the eigendecomposition can be expressed as
$A=Q* \Sigma *Q^T$&lt;/p&gt;

&lt;p&gt;where Q is the matrix with eigenvectors as columns and Λ is diag(λ).&lt;/p&gt;

&lt;h3 id=&quot;why-eigenvalue-and-eigenvectors-are-so-important&quot;&gt;Why Eigenvalue and Eigenvectors are so important?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;They are quite helpful for optimization algorithm or more clearly in constrained optimization problem. In optimization problem we are solving a system of linear euaqtion. Typical &lt;code class=&quot;highlighter-rouge&quot;&gt;Gaussian Elemination Technique&lt;/code&gt; has time complexity $O(n^3)$ but this can be solved with Eigenvalue Decomposition which needs $O(n^2)$. So they are more efficient. [for more explanation follow the deeep learning lecture 6 of prof. Mitesh from IIT Madrass]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;References:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Follow lecture 6 of &lt;a href=&quot;https://www.cse.iitm.ac.in/~miteshk/CS7015.html&quot;&gt;Prof.Mitesh_IITM&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.7-Eigendecomposition/&quot;&gt;(Book_IMPORTANT_link)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;pca-and-svd&quot;&gt;PCA and SVD&lt;/h2&gt;

&lt;p&gt;Follow lecture 6 of &lt;a href=&quot;https://www.cse.iitm.ac.in/~miteshk/CS7015.html&quot;&gt;Prof.Mitesh_IITM&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Eigenvectors can only be found for Square matrix. But, not ever square matrix has eigen vectors.&lt;/li&gt;
  &lt;li&gt;All eigen vectors are perpendicular, i.e orthogonal.&lt;/li&gt;
  &lt;li&gt;orthonormal vectors are orthogonal and they have unit length.&lt;/li&gt;
  &lt;li&gt;If &lt;code class=&quot;highlighter-rouge&quot;&gt;V&lt;/code&gt; is an orthonormal matrix then, &lt;code class=&quot;highlighter-rouge&quot;&gt;V'V=I&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;PCA&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;PCA decomposes a &lt;strong&gt;real, symmetric matrix $A$&lt;/strong&gt; into &lt;code class=&quot;highlighter-rouge&quot;&gt;eigenvectors&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;eigenvalues&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Every &lt;strong&gt;real, symmetric matrix $A$&lt;/strong&gt; can be decomposed into the following expression: &lt;code class=&quot;highlighter-rouge&quot;&gt;A=VSV'&lt;/code&gt;. Where &lt;code class=&quot;highlighter-rouge&quot;&gt;V&lt;/code&gt; is an orthogonal matrix. &lt;code class=&quot;highlighter-rouge&quot;&gt;S&lt;/code&gt; is a diagonal matrix with all the eigen values.&lt;/li&gt;
  &lt;li&gt;Though, any real symmetric matrix is &lt;strong&gt;guranteed&lt;/strong&gt; to have an &lt;strong&gt;eigen decomposition&lt;/strong&gt;, the decomposition may not be unique.&lt;/li&gt;
  &lt;li&gt;If a matrix is not square, then it’s eigen decomposition is not defined.&lt;/li&gt;
  &lt;li&gt;A matrix is singular &lt;strong&gt;if and only if&lt;/strong&gt;, any of the eigenvalue is zero.&lt;/li&gt;
  &lt;li&gt;Consider A as real, symmetric matrix and $\lambda_i$ are the eigen values.
    &lt;ul&gt;
      &lt;li&gt;if all $\lambda_i&amp;gt;0$, then A is called &lt;code class=&quot;highlighter-rouge&quot;&gt;positive definite&lt;/code&gt; matrix.&lt;/li&gt;
      &lt;li&gt;if all $\lambda_i&amp;gt;=0$, then A is called &lt;code class=&quot;highlighter-rouge&quot;&gt;positive semidefinite&lt;/code&gt; (PSD) matrix.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;PSD matricies are interesting because the gurantee that &lt;code class=&quot;highlighter-rouge&quot;&gt;for all x&lt;/code&gt;, $x’Ax&amp;gt;=0$.&lt;/li&gt;
  &lt;li&gt;PSD additionally gurantees that if $x’Ax=0$ then $x=0$. &lt;a href=&quot;https://medium.com/@SeoJaeDuk/principal-component-analysis-pooling-in-tensorflow-with-interactive-code-pcap-43aa2cee9bb&quot;&gt;[source]&lt;/a&gt;, &lt;a href=&quot;https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.12-Example-Principal-Components-Analysis/&quot;&gt;[IMP_link]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;SVD&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.8-Singular-Value-Decomposition/&quot;&gt;(IMP_source)&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;A  is a matrix that can be seen as a linear transformation. This transformation can be decomposed in three sub-transformations: 1. rotation, 2. re-scaling, 3. rotation. These three steps correspond to the three matrices U, D, and V.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A = U D V^T&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Every matrix can be seen as a linear transformation&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;The transformation associated with diagonal matrices imply only a rescaling of each coordinate without rotation&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;The SVD can be seen as the decomposition of one complex transformation in 3 simpler transformations (a rotation, a scaling and another rotation).&lt;/li&gt;
  &lt;li&gt;SVD is more generic.&lt;/li&gt;
  &lt;li&gt;SVD provides another way for factorizing a matrix, into &lt;code class=&quot;highlighter-rouge&quot;&gt;singular values&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;singular vectors&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Every real matrix has SVD but same is not true for PCA.&lt;/li&gt;
  &lt;li&gt;If a matrix is not square then PCA not applicable.&lt;/li&gt;
  &lt;li&gt;During PCA we write &lt;code class=&quot;highlighter-rouge&quot;&gt;A=VSV'&lt;/code&gt;. However, for SVD we write &lt;code class=&quot;highlighter-rouge&quot;&gt;A=UDV'&lt;/code&gt;, where A is &lt;code class=&quot;highlighter-rouge&quot;&gt;m x n&lt;/code&gt;, U is &lt;code class=&quot;highlighter-rouge&quot;&gt;m x m&lt;/code&gt;, D is &lt;code class=&quot;highlighter-rouge&quot;&gt;m x n&lt;/code&gt; and V is &lt;code class=&quot;highlighter-rouge&quot;&gt;n x n&lt;/code&gt;.
    &lt;ul&gt;
      &lt;li&gt;U, V orthogonal matricies.&lt;/li&gt;
      &lt;li&gt;D is diagonal matrix and not necessarily a square&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;diag(D)&lt;/code&gt; are the &lt;code class=&quot;highlighter-rouge&quot;&gt;singular values&lt;/code&gt; of the matrix A&lt;/li&gt;
      &lt;li&gt;Columns of &lt;code class=&quot;highlighter-rouge&quot;&gt;U&lt;/code&gt; (&lt;code class=&quot;highlighter-rouge&quot;&gt;V&lt;/code&gt;) are &lt;code class=&quot;highlighter-rouge&quot;&gt;left (right) singular vectors&lt;/code&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Relation between SVD and PCA&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;The matrices U, D and V can be found by transforming A in a square matrix and by computing the eigenvectors of this square matrix. The square matrix can be obtain by multiplying the matrix A by its transpose in one way or the other.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;left singular vectors&lt;/code&gt; i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;U&lt;/code&gt; are the eigen vectors of &lt;code class=&quot;highlighter-rouge&quot;&gt;AA'&lt;/code&gt;. Similarly, the &lt;code class=&quot;highlighter-rouge&quot;&gt;right singular vectors&lt;/code&gt; i.e. &lt;code class=&quot;highlighter-rouge&quot;&gt;V&lt;/code&gt; are the eigen vectors of &lt;code class=&quot;highlighter-rouge&quot;&gt;A'A&lt;/code&gt;. Note that A might not be a square matrix but &lt;code class=&quot;highlighter-rouge&quot;&gt;A'A&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;AA'&lt;/code&gt; are both square.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;A'A = VDV'&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;AA' = UDU'&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;D&lt;/code&gt;  corresponds to the eigenvalues &lt;code class=&quot;highlighter-rouge&quot;&gt;AA'&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;A'A&lt;/code&gt; which are the same.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://www.deeplearningbook.org/contents/linear_algebra.html&quot;&gt;source: chapter 2, deep learning book - Goodfellow, p42&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;BONUS: Apply SVD on Images:&lt;/strong&gt;
&lt;a href=&quot;https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.8-Singular-Value-Decomposition/&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;determinant&quot;&gt;Determinant&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;The determinant of a matrix A is a number corresponding to the multiplicative change you get when you transform your space with this matrix.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;A negative determinant means that there is a change in orientation (and not just a rescaling and/or a rotation).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.11-The-determinant/&quot;&gt;(Important_link)&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;constrained-optimization-lagrangian&quot;&gt;Constrained optimization (Lagrangian)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;minimize&lt;/strong&gt; $f(x)$ such that &lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;g(x)&amp;lt;=0&lt;/code&gt;&lt;/strong&gt; and $h(x)=0$.
    &lt;ul&gt;
      &lt;li&gt;Our target is to bring a new equation where we will combine $f(x), g(x), h(x)$ in a single equation. We will do this by introducing Lagrange Multiplier $\lambda$ and $\mu$. The new equation looks like: $L(x,\lambda,\mu)=f(x)+\lambda g(x)+ \mu h(x)$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;maximize&lt;/strong&gt; $f(x)$ such that &lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;g(x)&amp;gt;=0&lt;/code&gt;&lt;/strong&gt; and $h(x)=0$.
    &lt;ul&gt;
      &lt;li&gt;Our target is to bring a new equation where we will combine $f(x), g(x), h(x)$ in a single equation. We will do this by introducing Lagrange Multiplier $\lambda$ and $\mu$. The new equation looks like: $L(x,\lambda,\mu)=f(x)+\lambda g(x)+ \mu h(x)$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; In the above formulation, pay special attention to the &lt;code class=&quot;highlighter-rouge&quot;&gt;minimize&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;maximize&lt;/code&gt; kewords and the change in inequality constrains. So given any minimization or maximization problem, convert its constraints to $g(x)&amp;lt;=0$ or $g(x)&amp;gt;=0$ accordingly and then formulate the Lagrangian form. The $h(x)=0$ may or may not be there. Finally apply KKT conditions for finding the solution.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;KKT Conditions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Stationarity $\nabla_x L(x,\lambda,\mu)=0$&lt;/li&gt;
  &lt;li&gt;Primal feasibility, $g(x)&amp;lt;=0$ (for minimization problem)&lt;/li&gt;
  &lt;li&gt;Dual feasibility, $\lambda&amp;gt;=0, \mu&amp;gt;=0$&lt;/li&gt;
  &lt;li&gt;Complementary slackness, $\lambda g(x) = 0$ and $\mu h(x)=0$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;http://mat.gsia.cmu.edu/classes/QUANT/NOTES/chap4.pdf&quot;&gt;notes&lt;/a&gt;, &lt;a href=&quot;http://cse.iitkgp.ac.in/~sourangshu/coursefiles/ML15A/svm.pdfz&quot;&gt;sb slides&lt;/a&gt;, &lt;a href=&quot;https://www.cse.iitk.ac.in/users/rmittal/prev_course/s14/notes/lec11.pdf&quot;&gt;iitK_notes&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=vwUV2IDLP8Q&amp;amp;list=PLSQl0a2vh4HC5feHa6Rc5c0wbRTx56nF7&amp;amp;index=92&quot;&gt;Khan Academy - Constrained Optimization&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;solve-linear-programming&quot;&gt;Solve Linear Programming&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;min $c^Tx$ subject to: $Ax = b$, $x ≥ 0$.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;The linear programming problem is usually solved through the use of one of two
algorithms: either simplex, or an algorithm in the family of interior point methods.
In this article two representative members of the family of interior point methods are
introduced and studied. We discuss the design of these interior point methods on a high
level, and compare them to both the simplex algorithm and the original algorithms in
nonlinear constrained optimization which led to their genesis.
&lt;a href=&quot;https://www.cs.toronto.edu/~robere/paper/interiorpoint.pdf&quot;&gt;[survey paper]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Simplex Method&lt;/strong&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Let $A, b, c$ be an instance of the LPP, defining a convex polytope in $R^n$. Then there exists an optimal solution to this program at one of the vertices of the &lt;strong&gt;polytope&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The simplex algorithm works roughly as follows. We begin with a feasible point at one
of the vertices of the polytope. Then we “walk” along the edges of the polytope from vertex
to vertex, in such a way that the value of the objective function monotonically decreases at
each step. When we reach a point in which the objective value can decrease no more, we are
finished. &lt;a href=&quot;https://www.youtube.com/watch?v=vVzjXpwW2xI&quot;&gt;(youtube)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2017/02/lintroductory-guide-on-linear-programming-explained-in-simple-english/&quot;&gt;(source)&lt;/a&gt;, &lt;a href=&quot;https://www.cs.toronto.edu/~robere/paper/interiorpoint.pdf&quot;&gt;(link2)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Interior Point Method&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Our above question about the complexity of the LPP was answered by Khachiyan in 1979. He demonstrated a worst-case polynomial time algorithm for linear programming dubbed the ellipsoid method [Kha79] in which the algorithm moved across the &lt;strong&gt;interior of the feasible region, and not along the boundary like simplex&lt;/strong&gt;. Unfortunately the worst case running time for the ellipsoid method was high: $O(n^6L^2)$, where n is the number of variables in the problem
and L is the number of the bits in the input. Moreover, this method tended to approach the
worst-case complexity on nearly all inputs, and so the simplex algorithm remained dominant
in practice. This algorithm was only partially satisfying to researchers: was there a worst-case
polynomial time algorithm for linear programming which had a performance that rivalled
the performance of simplex on day-to-day problems?
This question was answered by Karmarkar in 1984. He produced a polynomial-time
algorithm — soon called the projective algorithm — for linear programming that ran in
much better time: $O(n^{3.5}L^2)$ [Kar84].&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.inf.ethz.ch/personal/fukudak/lect/opt2011/aopt11note4.pdf&quot;&gt;(link1)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;First Order Method -  The KKT Conditions and Duality Theory&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;gradient-decent&quot;&gt;Gradient Decent&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;The main idea is that the sign of the derivative of the function at a specific value of x tells you if you need to increase (if negative slope) or decrease (if positive slope) x to reach the minimum. When the slope is near 0, the minimum should have been reached. 
&lt;a href=&quot;https://hadrienj.github.io/posts/Deep-Learning-Book-Series-2.12-Example-Principal-Components-Analysis/&quot;&gt;(Minimizing Function)&lt;/a&gt;&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function. Depending on the amount of data, we make a trade-off between the accuracy of the parameter update and the time it takes to perform an update.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Batch gradient descent&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters θ for the entire training dataset:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta = \theta - \eta \cdot \nabla_\theta J( \theta)&lt;/script&gt;

&lt;p&gt;As we need to calculate the gradients for the whole dataset to perform just one update, batch gradient descent can be very slow and is intractable for datasets that don’t fit in memory.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Stochastic gradient descent:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Stochastic gradient descent (SGD) in contrast performs a parameter update for each training example $x^{(i)}$ and label $y^{(i)}$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})&lt;/script&gt;

&lt;p&gt;Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Mini-batch gradient descent&lt;/strong&gt;
Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n training examples:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})&lt;/script&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nb_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_batches&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;evaluate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params_grad&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Momentum&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress along the bottom towards the local optimum.&lt;/p&gt;

&lt;p&gt;Momentum is a method that helps accelerate SGD in the relevant direction and dampens oscillations. It does this by adding a fraction $\gamma$ of the update vector of the past time step to the current update vector:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta)&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\theta = \theta - v_t&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. γ&amp;lt;1). The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://ruder.io/optimizing-gradient-descent/&quot;&gt;(more details, see Sebastian Ruder blog)&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;roc-curve-analysis&quot;&gt;ROC Curve Analysis&lt;/h2&gt;
&lt;h3 id=&quot;roc-curve&quot;&gt;ROC curve&lt;/h3&gt;
&lt;p&gt;An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;True Positive Rate:
    &lt;ul&gt;
      &lt;li&gt;True Positive Rate (TPR) is a synonym for recall and is therefore defined as follows:&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;TPR = \frac{TP} {TP + FN}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;False Positive Rate&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;FPR = \frac{FP} {FP + TN}&lt;/script&gt;

&lt;p&gt;To compute the points in an ROC curve, we could evaluate a logistic regression model many times with different classification thresholds, but this would be inefficient. Fortunately, there’s an efficient, sorting-based algorithm that can provide this information for us, called AUC.&lt;/p&gt;

&lt;h3 id=&quot;auc-area-under-the-roc-curve&quot;&gt;AUC: Area Under the ROC Curve&lt;/h3&gt;
&lt;p&gt;AUC stands for “Area under the ROC Curve.” That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1).&lt;/p&gt;

&lt;p&gt;AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. 
&lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc&quot;&gt;(source)&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;recommendation-system&quot;&gt;Recommendation System:&lt;/h2&gt;

&lt;h3 id=&quot;content-based-recommendation&quot;&gt;Content based recommendation:&lt;/h3&gt;

&lt;h4 id=&quot;using-feature-vector-and-regression-based-analysis&quot;&gt;Using Feature vector and regression based analysis&lt;/h4&gt;
&lt;p&gt;Here the assumption is that for each of the &lt;code class=&quot;highlighter-rouge&quot;&gt;item&lt;/code&gt; you have the corresponding features available.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt; Below are a table, of movie-user rating. 5 users and 4 movies. And the ratings are also provided (1-5) for some of them. We want to predict the rating for all the &lt;code class=&quot;highlighter-rouge&quot;&gt;?&lt;/code&gt; unknown. It means that user $u_j$ has not seen that movie $m_i$ and if we can predict the rating for cell (i,j) then it will help us to decide whether or not to recommend the movie to the user $u_j$. For example, cell (3,4) is unknown. If by some algorithm, we can predict the rating for that cell, say the predicted rating is 4, it means if the user &lt;strong&gt;would have seen this movie, then he would have rated the movie 4&lt;/strong&gt;. So we must definitely recommend this movie to the user.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;u1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;u2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;u3&lt;/th&gt;
      &lt;th&gt;u4&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;u5&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;m1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;?&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;m2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;?&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;m3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;?&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;m4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;?&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Now for content based movie recommendation it’s assumed that the features available for the content. For example if we see closely, then we can see the following patterns in the table. The bold ratings have segmented the table into 4 sub parts based on rating clustering.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;u1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;u2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;u3&lt;/th&gt;
      &lt;th&gt;u4&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;u5&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;m1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;?&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;m2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;?&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;4&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;?&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;m3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;?&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;?&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;m4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;5&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;strong&gt;?&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;as if movie $m_1$, $m_2$ belong to type 1 (say romance) and $m_3$, and $m_4$ belong to type 2 (say action) and there is a clear discrimination is the rating as well.&lt;/p&gt;

&lt;p&gt;Now for content based recommendation this types are available and then the datasets actually looks as follows&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;u1&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;u2&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;u3&lt;/th&gt;
      &lt;th&gt;u4&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;u5&lt;/th&gt;
      &lt;th&gt;T1&lt;/th&gt;
      &lt;th&gt;T2&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;m1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;?&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;0.1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;m2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;?&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;?&lt;/td&gt;
      &lt;td&gt;0.8&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;m3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;?&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td&gt;?&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;0.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;m4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;?&lt;/td&gt;
      &lt;td&gt;0.1&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;where $T_1$ and $T_2$ columns are already known. Then for each of the user we can learn a regression problem with the known rating as the target vector $u_j$ and $A = [T_1,T_2]$
is the feature matrix and we need to learn the $\theta_j$ for user $j$ such that $A \theta_j = u_j$. Create the loss function and solve the optimization problem.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=c0ZPDKbYzx0&amp;amp;list=PLnnr1O8OWc6ZYcnoNWQignIiP5RRtu3aS&amp;amp;index=2&quot;&gt;(Content Based Recom -A.Ng)&lt;/a&gt;,
&lt;a href=&quot;https://www.youtube.com/watch?v=2uxXPzm-7FY&amp;amp;index=42&amp;amp;list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV&quot;&gt;(MMD - Stanford )&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;colaborative-filtering&quot;&gt;Colaborative FIltering&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Story:&lt;/strong&gt; Unlike the &lt;code class=&quot;highlighter-rouge&quot;&gt;content based recommendation&lt;/code&gt;, where the feature columns ($T_1$, $T_2$) were already given, here the features are not present. Rather they are being learnt by the algorithm. Here we assume $\theta_j$ is given i.e we know the users liking for $T_1$ and $T_2$ movies and almost similarly we formulate the regression problem but now we try to estimate the feature columns $T_k$. Then using the learnt feature we estimate the unknown ratings and then recommend those movies. Here knowing $\theta_j$
 means that each user has given inofrmation regarding his/her preferences based on subset of movies and thus all the users are colaboratively helping to learn the features.&lt;/p&gt;

&lt;p&gt;Naive Algorithm:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Given $\theta_j$, learn features $T_k$. [loss function $J(T_k)$]&lt;/li&gt;
  &lt;li&gt;Given $T_k$, learn $\theta_j$. [loss function $J(\theta_j)$]&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;and start with a random initialization of $\theta$ and then move back and forth between step 1 and 2.&lt;/p&gt;

&lt;p&gt;Better Algorithm:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;combine step 1 and 2 into a single loss function $J(\theta_j,T_k)$ and solve that.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;user-user-cf&quot;&gt;User User CF&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;fix a similarity function (jaccard similarity, cosine similarity) for two vectors. and then pick any two users profile (their rating for different movies) and calculate the similarity function which helps you to cluster the users.
    &lt;ul&gt;
      &lt;li&gt;jaccard similarity doesn;t consider the rating&lt;/li&gt;
      &lt;li&gt;cosine similarity consider the unknow entries as 0, which causes problem as rating range is (0-5).&lt;/li&gt;
      &lt;li&gt;cetered cosine similarity (pearson correlation): substract $\mu_{user}^{rating}$ from $rating_{user}$.
        &lt;ul&gt;
          &lt;li&gt;missing ratings are treated as average&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;item-item-cf&quot;&gt;Item Item CF&lt;/h4&gt;
&lt;p&gt;Almost similar to User User CF.&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;works for any kind of item&lt;/li&gt;
  &lt;li&gt;No need data for other user&lt;/li&gt;
  &lt;li&gt;It’s personalized recommendation as for each user a regression problem is learnt.&lt;/li&gt;
  &lt;li&gt;No firt-rater problem, i.e. we can recommend an item to the user, as soon as it’s available in the market.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Finding the correcct feature is hard&lt;/li&gt;
  &lt;li&gt;cold start problem for new user&lt;/li&gt;
  &lt;li&gt;Sparsiry&lt;/li&gt;
  &lt;li&gt;Popularity bias&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=-Fptv3NZtmE&amp;amp;index=4&amp;amp;list=PLnnr1O8OWc6ZYcnoNWQignIiP5RRtu3aS&quot;&gt;(Collaborative Filtering-A.Ng)&lt;/a&gt;, &lt;a href=&quot;https://www.youtube.com/watch?v=2uxXPzm-7FY&amp;amp;index=42&amp;amp;list=PLLssT5z_DsK9JDLcT8T62VtzwyW9LNepV&quot;&gt;(MMD - Stanford )&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;colaborative-filtering---low-rank-matrix-factorization-svd&quot;&gt;Colaborative Filtering - Low Rank Matrix Factorization (SVD)&lt;/h3&gt;

&lt;h3 id=&quot;evaluation-of-recommending-system&quot;&gt;Evaluation of Recommending System&lt;/h3&gt;

&lt;p&gt;RMSE: Root Mean Square Error. However it doesn’t distinguish between high rating and low rating.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Alternative: Precision $@k$ i.e. get the precision for top k items.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;clustering-algorithm&quot;&gt;Clustering Algorithm&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;When data set is very large, can’t fit into memory, then K means algorithm is not a good algorithm. Rather use Bradley-Fayyad-Reina (BFR) algorithm.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bfr&quot;&gt;BFR&lt;/h3&gt;

&lt;p&gt;Assumption:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Each cluster is normally distributed around a centroid in Eucledian space&lt;/li&gt;
  &lt;li&gt;Normal distribution assumption implies that clusters looks like axis-aligned ellipses, i.e. standard deviation is maximum along one axis and minimum w.r.t other.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[watch MMD Youtube]&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;how-to-implement-regularization-in-decision-tree-random-forest&quot;&gt;How to implement regularization in decision tree, random forest?&lt;/h2&gt;

&lt;h2 id=&quot;why-random-column-selection-helps-random-forest&quot;&gt;Why random column selection helps random forest?&lt;/h2&gt;

&lt;p&gt;Random forest algorithm comes under the &lt;code class=&quot;highlighter-rouge&quot;&gt;Bootstrap Algorithm&lt;/code&gt; a.k.a &lt;code class=&quot;highlighter-rouge&quot;&gt;Bagging&lt;/code&gt; category whose primary objective is to &lt;code class=&quot;highlighter-rouge&quot;&gt;reduce the variance&lt;/code&gt; of an estimate by averaging many estimates.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sample datasets without replacement
    &lt;ul&gt;
      &lt;li&gt;In bootstarp algorithm &lt;code class=&quot;highlighter-rouge&quot;&gt;M&lt;/code&gt; decision trees are trained on &lt;code class=&quot;highlighter-rouge&quot;&gt;M&lt;/code&gt; datasets which are sampled &lt;code class=&quot;highlighter-rouge&quot;&gt;without replacement&lt;/code&gt; from the original datasets. Hence each of the sampled data will have duplicate data as well as some missing data, which are present in the original data, as all the sampled datasets have same length. Thuse these bootstrap samples will have diversity among themselves, resulting the model to be diverse as well.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Along with the above methods &lt;code class=&quot;highlighter-rouge&quot;&gt;Random Forest&lt;/code&gt; also does the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Random subset of feature selection for building the tree
    &lt;ul&gt;
      &lt;li&gt;This is known as subspace sampling&lt;/li&gt;
      &lt;li&gt;Increases the diversity in the ensemble method even more&lt;/li&gt;
      &lt;li&gt;Tree training time reduces.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;IMPORTANT:&lt;/strong&gt; Also simply running the same model on different sampled data will produce highly correlated predictors, which limits the amount of variance reduction that is possible. So RF tries to &lt;strong&gt;decorrelate&lt;/strong&gt; the base learners by learning trees based on a randomly chosen subset of input variables, as well as, randomly chosen subset of data cases.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;source: 1. Kevin Murphy, 2. Peter Flach&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/How-does-bagging-avoid-overfitting-in-Random-Forest-classification&quot;&gt;ref&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;overfitting-in-random-forest&quot;&gt;Overfitting in Random Forest&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;To avoid over-fitting in random forest, the main thing you need to do is optimize a tuning parameter that governs the number of features that are randomly chosen to grow each tree from the bootstrapped data. Typically, you do this via k-fold cross-validation, where k∈{5,10}, and choose the tuning parameter that minimizes test sample prediction error. In addition, growing a larger forest will improve predictive accuracy, although there are usually diminishing returns once you get up to several hundreds of trees.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For decision trees there are two ways of handling overfitting: (a) don’t grow the trees to their entirety (b) prune. The same applies to a forest of trees - don’t grow them too much and prune.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/34997134/random-forest-tuning-tree-depth-and-number-of-trees/35012011#35012011&quot;&gt;ref 1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/111968/random-forest-how-to-handle-overfitting&quot;&gt;ref 2&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-avoid-overfitting&quot;&gt;How to avoid Overfitting?&lt;/h2&gt;

&lt;p&gt;Overfitting means High Variance&lt;/p&gt;

&lt;p&gt;Steps to void overfitting:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Cross Validation&lt;/li&gt;
  &lt;li&gt;Train with more data&lt;/li&gt;
  &lt;li&gt;Remove feature&lt;/li&gt;
  &lt;li&gt;Early stopping&lt;/li&gt;
  &lt;li&gt;Regularizations&lt;/li&gt;
  &lt;li&gt;Ensembling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://elitedatascience.com/overfitting-in-machine-learning&quot;&gt;ref 1&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-avoid-underfitting&quot;&gt;How to avoid Underfitting?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Add more features&lt;/li&gt;
  &lt;li&gt;Add more data&lt;/li&gt;
  &lt;li&gt;Decrease the amount of regularizations&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html&quot;&gt;ref 1&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;bayesian-machine-learning&quot;&gt;Bayesian Machine Learning&lt;/h2&gt;

&lt;p&gt;Coursera - Bayesian Methods for Machine Learning&lt;/p&gt;

&lt;h2 id=&quot;1st-order-optimization-2nd-order-optimization&quot;&gt;1st order optimization, 2nd order optimization&lt;/h2&gt;

&lt;h2 id=&quot;underfitting-overfitting-bias-variance&quot;&gt;Underfitting, Overfitting, Bias, Variance&lt;/h2&gt;

&lt;h2 id=&quot;some-other-topic&quot;&gt;Some other Topic&lt;/h2&gt;

&lt;h3 id=&quot;finding-similar-sets-from-millions-or-billions-data&quot;&gt;Finding similar sets from millions or billions data&lt;/h3&gt;

&lt;p&gt;Techniques:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Shingling: converts documents, emails etc to set.&lt;/li&gt;
  &lt;li&gt;Minhashing: Convert large sets to short signature, while preserving similarity&lt;/li&gt;
  &lt;li&gt;Locality-Sensitive-Hashing: Focus on pair of signatures likely to be similar.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;minhashing&quot;&gt;Minhashing&lt;/h4&gt;</content><author><name>Sankarshan Mridha</name></author><summary type="html">Machine Learning Concepts Clearing</summary></entry><entry><title type="html">Blog 101: ML QnA1</title><link href="http://localhost:4000/blog/2019/07/28/blog_101_Typical_ML_QA1" rel="alternate" type="text/html" title="Blog 101: ML QnA1" /><published>2019-07-28T00:11:31+05:30</published><updated>2019-07-28T00:11:31+05:30</updated><id>http://localhost:4000/blog/2019/07/28/blog_101_Typical_ML_QA1</id><content type="html" xml:base="http://localhost:4000/blog/2019/07/28/blog_101_Typical_ML_QA1">&lt;h1 id=&quot;q-source&quot;&gt;Q source:&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://appliedmachinelearning.wordpress.com/2018/04/13/my-data-science-machine-learning-job-interview-experience-list-of-ds-ml-dl-questions/&quot;&gt;link_1&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-are-the-parameters-in-training-a-decision-tree&quot;&gt;&lt;strong&gt;What are the parameters in training a decision tree?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-the-philosophy-behind-decision-tree&quot;&gt;&lt;strong&gt;What is the philosophy behind Decision Tree?&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;A decision tree is a tree where each node represents a feature(attribute), each link(branch) represents a decision(rule) and each leaf represents an outcome(categorical or continues value)… &lt;a href=&quot;https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Find the feature that best splits the target class into the &lt;code class=&quot;highlighter-rouge&quot;&gt;purest possible&lt;/code&gt; children nodes (ie: nodes that don’t contain a mix of both classes, rather pure nodes with only one class).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Entropy&lt;/code&gt; on the other hand is a measure of impurity. It is defined for a classification problem with N classes as:&lt;/li&gt;
  &lt;li&gt;Entropy = $-\Sigma_i C_i * \log(C_i)$, where &lt;code class=&quot;highlighter-rouge&quot;&gt;i=1,...,N&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;Say we have a dataset &lt;code class=&quot;highlighter-rouge&quot;&gt;D&lt;/code&gt; and we are looking for a potential feature &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt;, on which we will split the dataset w.r.t &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt; into 2 parts &lt;code class=&quot;highlighter-rouge&quot;&gt;Dl&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Dr&lt;/code&gt; for left and right dataset respectively, such that those two datasets are at their purest form. Finally we use information gain to decide how good that feature is i.e how much pure the split is w.r.t &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt; using &lt;code class=&quot;highlighter-rouge&quot;&gt;Information Gain&lt;/code&gt;.
        &lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;n&quot;&gt;Df&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; \
&lt;span class=&quot;n&quot;&gt;Dl&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dr&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;        &lt;/div&gt;
        &lt;ul&gt;
          &lt;li&gt;Information Gain: It is the difference of entropy before the split and after the split.
&lt;code class=&quot;highlighter-rouge&quot;&gt;EntropyBefore_f = Entropy(Df)&lt;/code&gt; and entropy after is 
&lt;code class=&quot;highlighter-rouge&quot;&gt;EntropyAfter_f = Entropy(Dl)+Entropy(Dr)&lt;/code&gt; and finaly 
&lt;code class=&quot;highlighter-rouge&quot;&gt;InformationGain_f = EntropyBefore_f - EntropyAfter_f&lt;/code&gt;.
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/1859554/what-is-entropy-and-information-gain&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-to-build-decision-tree&quot;&gt;&lt;strong&gt;How to build decision tree?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;There are couple of algorithms there to build a decision tree. Some of the important ones are
    &lt;ul&gt;
      &lt;li&gt;CART (Classification and Regression Trees) → uses Gini Index(Classification) as metric. Lower the Gini Index, higher the purity of the split.&lt;/li&gt;
      &lt;li&gt;ID3 (Iterative Dichotomiser 3) → uses Entropy function and Information gain as metrics. Higher the Information Gain, better the split is.
      1. What are the criteria for splitting at a node in decision trees ?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Gini Index [&lt;a href=&quot;https://dni-institute.in/blogs/cart-decision-tree-gini-index-explained/&quot;&gt;link&lt;/a&gt;]
    &lt;ul&gt;
      &lt;li&gt;CART uses Gini index as a split metric. For &lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt; classes, the Gini Index is defined as: 
$1-\Sigma_i P_i^2$, where &lt;code class=&quot;highlighter-rouge&quot;&gt;i=1,...,N&lt;/code&gt; and $P_i=P(target=i)$ [&lt;a href=&quot;https://medium.com/deep-math-machine-learning-ai/chapter-4-decision-trees-algorithms-b93975f7a1f1&quot;&gt;source&lt;/a&gt;]&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Information Gain&lt;/li&gt;
  &lt;li&gt;Cross Entropy
    &lt;ul&gt;
      &lt;li&gt;Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1.&lt;/li&gt;
      &lt;li&gt;In binary classification, where the number of classes M equals 2, cross-entropy can be calculated as:
$-{(y\log(p) + (1 - y)\log(1 - p))}$&lt;/li&gt;
      &lt;li&gt;If M&amp;gt;2 (i.e. multiclass classification), we calculate a separate loss for each class label per observation and sum the result.
$H(y, \hat{y}) = \sum_i y_i \log \frac{1}{\hat{y}_i} = -\sum_i y_i \log \hat{y}_i$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Entropy&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;CHI Square:&lt;/strong&gt; It is an algorithm to find out the statistical significance between the differences between sub-nodes and parent node.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.mathsisfun.com/data/chi-square-test.html&quot;&gt;link_1&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb&quot;&gt;link_2&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reduction of Variance&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://clearpredictions.com/Home/DecisionTree&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-the-formula-of-gini-index-criteria&quot;&gt;&lt;strong&gt;What is the formula of Gini index criteria?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://dni-institute.in/blogs/cart-decision-tree-gini-index-explained/&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-the-formula-for-entropy-criteria&quot;&gt;&lt;strong&gt;What is the formula for Entropy criteria?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Entropy = $-\Sigma_i C_i * \log(C_i)$, where &lt;code class=&quot;highlighter-rouge&quot;&gt;i=1,...,N&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;what-is-kl-divergence&quot;&gt;&lt;strong&gt;What is KL Divergence?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;KL Divergence is the measure of &lt;strong&gt;relative entropy&lt;/strong&gt;. It is a measure of the “distance” between two distributions. In statistics, it arises as an expected logarithm of the likelihood ratio. The relative entropy 
   ${KL}(p\sim||\sim q)$ 
   is a measure of the inefficiency of assuming that the distribution is q, when the true distribution is p. The KL divergence from p to q is simply the difference between cross entropy and entropy:
   &lt;script type=&quot;math/tex&quot;&gt;{KL}(y~||~\hat{y}) = \sum_i y_i \log \frac{1}{\hat{y}_i} - \sum_i y_i \log \frac{1}{y_i} = \sum_i y_i \log \frac{y_i}{\hat{y}_i}&lt;/script&gt;. 
   Where $y_i \sim p$ and $\hat{y}_i \sim q$, i.e. they come from two different probability distribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-is-it-decided-that-on-which-features-it-has-to-split&quot;&gt;&lt;strong&gt;How is it decided that on which features it has to split?&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Based on for which feature the information gain is maximum.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-do-you-calculate-information-gain-mathematically&quot;&gt;&lt;strong&gt;How do you calculate information gain mathematically?&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www3.nd.edu/~rjohns15/cse40647.sp14/www/content/lectures/23%20-%20Decision%20Trees%202.pdf&quot;&gt;clear explanation, slides&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;If &lt;code class=&quot;highlighter-rouge&quot;&gt;H&lt;/code&gt; is the entropy of the original data D and it has undergone &lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt; splits for feature &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt;, then Information Gain: $IG(D,f) = H - \Sigma \frac{S_i}{S}H_i$ , where &lt;code class=&quot;highlighter-rouge&quot;&gt;i=1,...,N&lt;/code&gt; and $S$ is the size of total datasets and $S_i$ is the size of the $i_{th}$ split data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-the-advantage-with-random-forest-&quot;&gt;&lt;strong&gt;What is the advantage with random forest ?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quora.com/What-are-some-advantages-of-using-a-random-forest-over-a-decision-tree-given-that-a-decision-tree-is-simpler&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;why-ensemble-is-good&quot;&gt;&lt;strong&gt;Why ensemble is good?&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Suppose we have 10 independent classifiers, each with error rate of $0.3$ i.e $\epsilon=0.3$,&lt;/p&gt;

&lt;p&gt;In this setting, the error rate of the ensemble can be computed as below (we are taking a majority vote on the predictions. An ensemble makes a wrong prediction only when more than half of the base classifiers are wrong)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\epsilon_{ensemble}=\Sigma_{i=6}^{i=10} {10 \choose i}\epsilon^i(1-\epsilon)^{10-i} \approx 0.05&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quora.com/What-are-some-advantages-of-using-a-random-forest-over-a-decision-tree-given-that-a-decision-tree-is-simpler&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ensemble-learning-algorithm&quot;&gt;&lt;strong&gt;Ensemble Learning algorithm&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2017/02/introduction-to-ensembling-along-with-implementation-in-r/&quot;&gt;link1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2015/08/introduction-ensemble-learning/&quot;&gt;link2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;boosting-algorithms&quot;&gt;&lt;strong&gt;Boosting algorithms&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The term ‘Boosting’ refers to a family of algorithms which converts weak learner to strong learners.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;AdaBoost (Adaptive Boosting)&lt;/li&gt;
  &lt;li&gt;Gradient Tree Boosting&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;XGBoost&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/&quot;&gt;link1&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;how-does-gradient-boosting-works-&quot;&gt;&lt;strong&gt;How does gradient boosting works ?&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2015/09/complete-guide-boosting-methods/&quot;&gt;link&lt;/a&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; Bagging and Boosting both are ensemble learning algorithm,
        &lt;ul&gt;
          &lt;li&gt;where a collection of weak learner build the strong learner. While Bagging 
works on resampling data with replacement and create different dataset and 
the week learners are learnt on them, and final predictions are taken by 
averaging or majority voting. E.g. Random Forest.&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Bagging&lt;/strong&gt; is a simple ensembling technique in which we build many independent predictors/models/learners on sampled data with replacement from the original data (&lt;strong&gt;Bootstrap Aggregatoin&lt;/strong&gt;) and combine them using some model averaging techniques. (e.g. weighted average, majority vote or normal average). E.g: &lt;code class=&quot;highlighter-rouge&quot;&gt;Random Forest&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Boosting&lt;/strong&gt; is also an ensemble learning method in which the predictors are not made independently, but sequentially. &lt;a href=&quot;https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;AdaBoost:&lt;/strong&gt; First a weak learner is applied and all the training 
 examples, which are misclassified, are given higher weight. 
 So while building the dataset for training the next learner, the 
 previously misclassified training examples will appear in the 
 dataset (as high weightage has been given to them). Now on this new 
 dataset another learner is trained. 
 Obviously this learner will correctly classify those previously 
 misclassified examples + some more misclassification in this step. 
 Now repeat.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Gradiant Boosting:&lt;/strong&gt; It 
 works differently. It learns a weak learner &lt;code class=&quot;highlighter-rouge&quot;&gt;F(x)+noise&lt;/code&gt;. Then on the noise 
 it builds another weak learner &lt;code class=&quot;highlighter-rouge&quot;&gt;H(x)+error2&lt;/code&gt; and so on… Thus it becomes 
 &lt;code class=&quot;highlighter-rouge&quot;&gt;F(x)+H(x)+G(x)+....+noise&lt;/code&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;do-you-know-about-adaboost-algorithm--how-and-why-does-it-work-&quot;&gt;&lt;strong&gt;Do you know about Adaboost algorithm ? How and why does it work ?&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;mathematical explanation [&lt;a href=&quot;http://math.mit.edu/~rothvoss/18.304.3PM/Presentations/1-Eric-Boosting304FinalRpdf.pdf&quot;&gt;link&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;difference-of-adaboost-and-gradiant-boosting&quot;&gt;&lt;strong&gt;Difference of adaboost and gradiant boosting:&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quora.com/What-is-the-difference-between-gradient-boosting-and-adaboost&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Mathemetical Explanation [&lt;a href=&quot;http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf&quot;&gt;Imp_link&lt;/a&gt;]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bagging-boosting-difference&quot;&gt;&lt;strong&gt;Bagging boosting difference:&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;svm-summary&quot;&gt;&lt;strong&gt;SVM Summary:&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;A Support Vector Machine (SVM) performs classification by finding the hyperplane that maximizes the margin between the two classes. The vectors (cases) that define the hyperplane are the support vectors.&lt;/p&gt;

&lt;h3 id=&quot;algorithm&quot;&gt;&lt;strong&gt;Algorithm&lt;/strong&gt;&lt;/h3&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Define&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;an&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimal&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hyperplane&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maximize&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;margin&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Extend&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;above&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;definition&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;non&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linearly&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;separable&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;problems&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;have&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;penalty&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;term&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;misclassifications&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Map&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;high&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dimensional&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;space&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;where&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;it&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;easier&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;classify&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;decision&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;surfaces&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reformulate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;problem&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;so&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mapped&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;implicitly&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;this&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;space&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;https://saedsayad.com/images/SVM_optimize.png&quot; alt=&quot;image&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://saedsayad.com/images/SVM_optimize_1.png&quot; alt=&quot;image&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We find w and b by solving the following objective function using Quadratic Programming.&lt;/p&gt;

&lt;h3 id=&quot;hard-margin&quot;&gt;&lt;strong&gt;Hard Margin&lt;/strong&gt;&lt;/h3&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;min \frac{1}{2}w^Tw&lt;/script&gt;

&lt;p&gt;s.t $y_i(w.x_i+b)\ge 1, \forall x_i$&lt;/p&gt;

&lt;h3 id=&quot;soft-margin&quot;&gt;&lt;strong&gt;Soft Margin&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The beauty of SVM is that if the data is linearly separable, there is a unique global minimum value. An ideal SVM analysis should produce a hyperplane that completely separates the vectors (cases) into two non-overlapping classes. However, perfect separation may not be possible, or it may result in a model with so many cases that the model does not classify correctly. In this situation SVM finds the hyperplane that maximizes the margin and minimizes the misclassifications.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://saedsayad.com/images/SVM_3.png&quot; alt=&quot;image&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://saedsayad.com/images/SVM_optimize_3.png&quot; alt=&quot;image&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The simplest way to separate two groups of data is with a straight line (1 dimension), flat plane (2 dimensions) or an N-dimensional hyperplane. However, there are situations where a nonlinear region can separate the groups more efficiently. SVM handles this by using a &lt;strong&gt;kernel function&lt;/strong&gt; (nonlinear) to map the data into a different space where a hyperplane (linear) cannot be used to do the separation.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It means a non-linear function is learned by a linear learning machine in a high-dimensional &lt;strong&gt;feature space&lt;/strong&gt; while the capacity of the system is controlled by a parameter that does not depend on the dimensionality of the space. This is called kernel trick which means the kernel function transform the data into a higher dimensional feature space to make it possible to perform the linear separation.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://saedsayad.com/support_vector_machine.htm&quot;&gt;Blog&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-do-you-adjust-the-cost-parameter-for-the-svm-regularizer&quot;&gt;&lt;strong&gt;How do you adjust the cost parameter for the SVM regularizer?&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Regularization problems are typically formulated as optimization problems involving the desired objective(classification lo  ss in our case)and a regularization penalty.The regularization penalty is used to help stabilize the minimization of the ob­jective or infuse prior knowledge we might have about desirable solutions.Many machine learning methods can be viewed as regularization methods in this manner.For later utility we will cast SVM optimization problem as a 
regularization problem.&lt;/p&gt;

&lt;p&gt;Re write the soft margin problem using hinge loss $(z)$ defined as the positive part of $1-z$, written as $(1-z)^+$. The relaxed optimization problem (soft margin) can be reformulated as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;min \frac{1}{2}\vert \vert w \vert \vert^2 + C \Sigma_{t=1}^{n}(1 - y_t(w^T x_t + w_0))^+&lt;/script&gt;

&lt;p&gt;Here $\frac{1}{2}\vert \vert w \vert \vert^2$, the &lt;code class=&quot;highlighter-rouge&quot;&gt;inverse squared&lt;/code&gt; &lt;strong&gt;geometric margin&lt;/strong&gt; is viewed as a regularization penalty that helps stabilizes the objective $C \Sigma_{t=1}^{n}(1 - y_t(w^T x_t + w_0))^+$ .&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/lecture-notes/lec4.pdf&quot;&gt;MOT OCW Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-sort-of-optimization-problem-would-you-be-solving-to-train-a-support-vector-machine&quot;&gt;&lt;strong&gt;What sort of optimization problem would you be solving to train a support vector machine?&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Maximize Margin&lt;/code&gt; (best answer), quadratic program, quadratic with linear constraints, reference to solving the primal or dual form.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-are-the-kernels-used-in-svm-&quot;&gt;&lt;strong&gt;What are the kernels used in SVM ?&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Kernel $K(X_i, X_j)$ are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Linear Kernel: $X_i.X_j$&lt;/li&gt;
  &lt;li&gt;Polynomial Kernel: $(\gamma X_i.X_j + C)^d$&lt;/li&gt;
  &lt;li&gt;RBF Kernel: $\exp (-\gamma\vert X_i - X_j\vert ^2)$&lt;/li&gt;
  &lt;li&gt;Sigmoid Kernel: $\tanh(\gamma X_i.X_j + C)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;where $K(X_i, X_j) = \phi(X_i).\phi(X_j)$&lt;/p&gt;

&lt;p&gt;that is, the kernel function, represents a dot product of input data points mapped into the higher dimensional feature
space by transformation $\phi()$&lt;/p&gt;

&lt;p&gt;$\gamma$ is an adjustable parameter of certain kernel functions.&lt;/p&gt;

&lt;p&gt;The RBF is by far the most popular choice of kernel types used in Support Vector Machines. This is mainly because of their localized and finite responses across the entire range of the real x-axis.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.statsoft.com/Textbook/Support-Vector-Machines&quot;&gt;Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-is-the-optimization-technique-of-svm&quot;&gt;&lt;strong&gt;What is the optimization technique of SVM?&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://drona.csa.iisc.ac.in/~shivani/Teaching/E0370/Aug-2011/Lectures/2.pdf&quot;&gt;Imp_link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-does-svm-learns-the-hyperplane--talk-more-about-mathematical-details&quot;&gt;&lt;strong&gt;How does SVM learns the hyperplane ? Talk more about mathematical details?&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://drona.csa.iisc.ac.in/~shivani/Teaching/E0370/Aug-2011/Lectures/2.pdf&quot;&gt;Imp_link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;why-bring-lagrange-multiplier-for-solving-the-svm-problem&quot;&gt;&lt;strong&gt;Why bring Lagrange Multiplier for solving the SVM problem?&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Constrained Optimization Problem easier to solve with Lagrange Multiplier&lt;/li&gt;
  &lt;li&gt;The existing constraints will be replaced by the constraints of the Lagrange Multiplier, which are easier to handle&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;By this reformulation of the problem, the data will appear only as dot product, which will be very helpful while generalizing the SVM for non linearly separable class. [source page 129, last paragraph]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=yuqB-d5MjZA&amp;amp;list=PLg9_rXni6UXmjc7Cxw8HpYWRlFg72v15a&amp;amp;index=7&quot;&gt;Youtube:Lagrange Multiplier Intuition&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;kkt-condition-for-svm&quot;&gt;&lt;strong&gt;KKT Condition for SVM?&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://cse.iitkgp.ac.in/~sourangshu/coursefiles/ML15A/svm.pdf&quot;&gt;SB course&lt;/a&gt;, &lt;a href=&quot;http://www.csc.kth.se/utbildning/kth/kurser/DD3364/Lectures/KKT.pdf&quot;&gt;link2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;geometric-analysis-of-lagrangian-kkt-dual&quot;&gt;&lt;strong&gt;Geometric analysis of Lagrangian, KKT, Dual&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://anie.me/Lagrangian-And-Dual-Problem/&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-does-svm-learns-non-linear-boundaries--explain&quot;&gt;&lt;strong&gt;How does SVM learns non-linear boundaries ? Explain.&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;using &lt;code class=&quot;highlighter-rouge&quot;&gt;kernel trick&lt;/code&gt;, it maps the examples from &lt;code class=&quot;highlighter-rouge&quot;&gt;input space&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;feature space&lt;/code&gt;. 
In the higher dimension, they are separated linearly.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;talking-about-unsupervised-learning--what-are-the-algorithms-&quot;&gt;Talking about unsupervised learning ? What are the algorithms ?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Clustering&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.saedsayad.com/clustering_kmeans.htm&quot;&gt;K Means&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;K-Means clustering intends to partition n objects into k clusters in which each object belongs to the cluster with the nearest mean. This method produces exactly k different clusters of greatest possible distinction. The best number of clusters k leading to the greatest separation (distance) is not known as a priori and must be computed from the data. The objective of K-Means clustering is to minimize total intra-cluster variance, or, the squared error function:&lt;/li&gt;
      &lt;li&gt;$J = \Sigma_j \Sigma_i \vert\vert x_i - c_j \vert\vert^2$ where &lt;code class=&quot;highlighter-rouge&quot;&gt;j=1,...,K&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;i=1,...,N&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;N&lt;/code&gt; total number of observations and &lt;code class=&quot;highlighter-rouge&quot;&gt;K&lt;/code&gt; total number of classes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Algorithm:&lt;/strong&gt;&lt;/p&gt;
&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Clusters&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;into&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;groups&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;where&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;  &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predefined&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Select&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;points&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Assign&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;objects&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;their&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;closest&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;according&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Euclidean&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Calculate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centroid&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;objects&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;mf&quot;&gt;5.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Repeat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;steps&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;until&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;same&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;points&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;assigned&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cluster&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;consecutive&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rounds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Seed K-Means:&lt;/strong&gt; For seeding, i.e, to decide the insitial set of &lt;code class=&quot;highlighter-rouge&quot;&gt;K&lt;/code&gt; centroids, use &lt;strong&gt;K-Means++&lt;/strong&gt; algorithm.&lt;/li&gt;
  &lt;li&gt;K Medoids&lt;/li&gt;
  &lt;li&gt;Agglomerative Clustering
    &lt;ul&gt;
      &lt;li&gt;Hierarchichal Clustering&lt;/li&gt;
      &lt;li&gt;Dimensionality Reduction&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;PCA&lt;/li&gt;
  &lt;li&gt;ICA&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-do-you-decide-k-in-k-means-clustering-algorithm-tell-me-at-least&quot;&gt;How do you decide K in K-Means clustering algorithm ?Tell me at least&lt;/h2&gt;

&lt;p&gt;3 ways of deciding K in clustering ?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Elbow Method&lt;/li&gt;
  &lt;li&gt;Average Silhouette Method&lt;/li&gt;
  &lt;li&gt;Gap Statistics&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.sthda.com/english/articles/29-cluster-validation-essentials/96-determining-the-optimal-number-of-clusters-3-must-know-methods/&quot;&gt;link1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://uc-r.github.io/kmeans_clustering&quot;&gt;link2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-do-you-seed-k-means-algorithmie-how-to-decide-the-first-k-clusters&quot;&gt;How do you &lt;code class=&quot;highlighter-rouge&quot;&gt;seed&lt;/code&gt; k-means algorithm,i.e. how to decide the first &lt;code class=&quot;highlighter-rouge&quot;&gt;k&lt;/code&gt; clusters?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;k-means++ is an algorithm for choosing the initial values (or “seeds”) for the k-means clustering algorithm. It was proposed in 2007 by David Arthur and Sergei Vassilvitskii, as an approximation algorithm for the NP-hard k-means problem—a way of avoiding the sometimes poor clusterings found by the standard k-means algorithm.&lt;/li&gt;
  &lt;li&gt;The intuition behind this approach is that spreading out the k initial cluster centers is a good thing: the first cluster center is chosen uniformly at random from the data points that are being clustered, after which each subsequent cluster center is chosen from the remaining data points with probability proportional to its squared distance from the point’s closest existing cluster center.&lt;/li&gt;
  &lt;li&gt;The exact algorithm is as follows:
    &lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Algorithm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;
    &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Choose&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uniformly&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;among&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;points&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
    &lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;For&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compute&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;between&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nearest&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;already&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;been&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chosen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
    &lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Choose&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;at&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weighted&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;probability&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distribution&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;where&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;point&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chosen&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;probability&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proportional&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;^&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.&lt;/span&gt;
    &lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Repeat&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Steps&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;until&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;have&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;been&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chosen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
    &lt;span class=&quot;mf&quot;&gt;5.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Now&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initial&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;centers&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;have&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;been&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;chosen&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proceed&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;standard&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;means&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clustering&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/K-means%2B%2B&quot;&gt;resource_wiki&lt;/a&gt;, &lt;a href=&quot;https://datasciencelab.wordpress.com/2014/01/15/improved-seeding-for-clustering-with-k-means/&quot;&gt;link2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;when-k-means-will-fail&quot;&gt;When K-means will fail?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;When data is &lt;strong&gt;non linearly separable&lt;/strong&gt;. It works best when data clusters are discrete.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-other-clustering-algorithms-do-you-know&quot;&gt;What other clustering algorithms do you know?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ans: &lt;a href=&quot;https://sites.google.com/site/dataclusteringalgorithms/&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Unsupervised &lt;strong&gt;linear clustering algorithm&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;k-means clustering algorithm&lt;/strong&gt; &lt;a href=&quot;https://home.deib.polimi.it/matteucc/Clustering/tutorial_html/kmeans.html&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Fuzzy c-means clustering algorithm&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://sites.google.com/site/dataclusteringalgorithms/hierarchical-clustering-algorithm&quot;&gt;Hierarchical clustering algorithm&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Hierarchical Agglomerative Clustering (bottom up)&lt;/li&gt;
  &lt;li&gt;Hierarchical DIvisive Clustering (top down)&lt;/li&gt;
  &lt;li&gt;Gaussian(EM) clustering algorithm&lt;/li&gt;
  &lt;li&gt;Quality threshold clustering algorithm&lt;/li&gt;
  &lt;li&gt;Unsupervised &lt;strong&gt;non-linear clustering algorithm&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;MST based clustering algorithm&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Basic Idea:&lt;/strong&gt; Apply MST on the data points. Use the &lt;em&gt;Euclidean&lt;/em&gt; distance as the weight between two data points. After building the MST removes the longest edge, then the 2nd longest and so on. And thus clusters will be formed. &lt;a href=&quot;http://shodhganga.inflibnet.ac.in/bitstream/10603/9728/10/10_chapter%203.pdf&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;kernel k-means clustering algorithm&lt;/li&gt;
  &lt;li&gt;Density based clustering algorithm&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/DBSCAN&quot;&gt;DBSCAN&lt;/a&gt;&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://plot.ly/scikit-learn/plot-dbscan/&quot;&gt;code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-db-scan-algorithm-&quot;&gt;What is DB-SCAN algorithm ?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;It is a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions&lt;/li&gt;
  &lt;li&gt;A point p is a core point if at least &lt;code class=&quot;highlighter-rouge&quot;&gt;minPts&lt;/code&gt; points are within distance &lt;code class=&quot;highlighter-rouge&quot;&gt;ε&lt;/code&gt; (&lt;code class=&quot;highlighter-rouge&quot;&gt;ε&lt;/code&gt; is the maximum radius of the neighborhood from p) of it (including p). Those points are said to be directly reachable from p.&lt;/li&gt;
  &lt;li&gt;A point q is &lt;code class=&quot;highlighter-rouge&quot;&gt;directly reachable&lt;/code&gt; from p if point q is within distance &lt;code class=&quot;highlighter-rouge&quot;&gt;ε&lt;/code&gt; from point p and p must be a core point.&lt;/li&gt;
  &lt;li&gt;A point q is &lt;code class=&quot;highlighter-rouge&quot;&gt;reachable&lt;/code&gt; from p if there is a path &lt;code class=&quot;highlighter-rouge&quot;&gt;p1, ..., pn&lt;/code&gt; with &lt;code class=&quot;highlighter-rouge&quot;&gt;p1 = p&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;pn = q&lt;/code&gt;, where each &lt;code class=&quot;highlighter-rouge&quot;&gt;pi+1&lt;/code&gt; is directly reachable from &lt;code class=&quot;highlighter-rouge&quot;&gt;pi&lt;/code&gt; (all the points on the path must be core points, with the possible exception of q).&lt;/li&gt;
  &lt;li&gt;All points not reachable from any other point are outliers.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-does-hac-hierarchical-agglomerative-clustering-work-&quot;&gt;How does HAC (Hierarchical Agglomerative clustering) work ?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://newonlinecourses.science.psu.edu/stat505/node/143/&quot;&gt;link1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-agglomerative-clustering-1.html&quot;&gt;link2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html&quot;&gt;code&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;explain-pca-tell-me-the-mathematical-steps-to-implement-pca&quot;&gt;&lt;strong&gt;Explain PCA? Tell me the mathematical steps to implement PCA?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;In PCA, we are interested to find the directions (components) that maximize the variance in our dataset)&lt;/li&gt;
  &lt;li&gt;Let’s assume that our goal is to reduce the dimensions of a d-dimensional 
dataset by projecting it onto a &lt;code class=&quot;highlighter-rouge&quot;&gt;k&lt;/code&gt;-dimensional subspace (where &lt;code class=&quot;highlighter-rouge&quot;&gt;k&amp;lt;d&lt;/code&gt;). So, 
how do we know what size we should choose for &lt;code class=&quot;highlighter-rouge&quot;&gt;k&lt;/code&gt;, and how do we know if we 
have a feature space that represents our data &lt;strong&gt;well&lt;/strong&gt;?
    &lt;ul&gt;
      &lt;li&gt;We will compute eigenvectors (the components) from our data set and 
collect them in a so-called scatter-matrix (or alternatively calculate 
them from the &lt;strong&gt;covariance matrix&lt;/strong&gt;). Each of those eigenvectors is associated 
with an eigenvalue, which tell us about the &lt;code class=&quot;highlighter-rouge&quot;&gt;length&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;magnitude&lt;/code&gt; of the 
eigenvectors. If we observe that all the eigenvalues are of very similar 
magnitude, this is a good indicator that our data is already in a “good” 
subspace. Or if some of the eigenvalues are much much higher than others, 
we might be interested in keeping only those eigenvectors with the much 
larger eigenvalues, since they contain more information about our data 
distribution. Vice versa, eigenvalues that are close to 0 are less 
informative and we might consider in dropping those when we construct 
the new feature subspace.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kth&lt;/code&gt; Eigenvector determines the &lt;code class=&quot;highlighter-rouge&quot;&gt;kth&lt;/code&gt; direction that maximizes the variance in that direction.&lt;/li&gt;
  &lt;li&gt;the corresponding &lt;code class=&quot;highlighter-rouge&quot;&gt;kth&lt;/code&gt; Eigenvalue determines the variance along &lt;code class=&quot;highlighter-rouge&quot;&gt;kth&lt;/code&gt; Eigenvector&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.math.ucsd.edu/~gptesler/283/slides/pca_15-handout.pdf&quot;&gt;pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c&quot;&gt;link1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rstudio-pubs-static.s3.amazonaws.com/249839_48d65d85396a465986f2d7df6db73c3d.html&quot;&gt;link2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tl;DR&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;let $X$ is the original dataset (&lt;code class=&quot;highlighter-rouge&quot;&gt;n x d&lt;/code&gt;), and $M$ is the centered data wrt mean.&lt;/li&gt;
      &lt;li&gt;Now compute covariance matrix, diagonalize it, compute principal component P.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;C=(MM')/(n-1)=VDV'=PP'&lt;/code&gt; where &lt;code class=&quot;highlighter-rouge&quot;&gt;P=Vsqrt(D)&lt;/code&gt;. [C: Cov Matrix]&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;M* = M'P&lt;/code&gt;, [M*: new dataset after the PCA]&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-is-disadvantage-of-using-pca&quot;&gt;&lt;strong&gt;What is disadvantage of using PCA?&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;one disadvantage of PCA lies in interpreting the results of dimension reduction analysis. This challenge will become particularly telling when the data needs to be normalized.&lt;/li&gt;
  &lt;li&gt;PCA assumaes approximate normality of the input space distribution. &lt;a href=&quot;http://www.stat.columbia.edu/~fwood/Teaching/w4315/Fall2009/pca.pdf&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;for more reading &lt;a href=&quot;https://www.quora.com/What-are-some-of-the-limitations-of-principal-component-analysis&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;why-pca-needs-normalization&quot;&gt;&lt;strong&gt;Why PCA needs normalization?&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A reason why we need to normalize before applying PCA is to mitigate the effects of scale. For example, if one of the attributes is orders of magnitude higher than others, PCA tends to ascribe the highest amount of variance to this attribute and thus skews the results of the analysis. By normalizing, we can get rid of this effect. However normalizing results in spreading the influence across many more principal components. In others words, more PCs are required to explain the same amount of variance in data. The interpretation of analysis gets muddied. 
&lt;a href=&quot;http://www.simafore.com/blog/bid/105347/Feature-selection-with-mutual-information-Part-2-PCA-disadvantages&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-do-you-deploy-machine-learning-models-&quot;&gt;&lt;strong&gt;How do you deploy Machine Learning models ?&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Microservice&lt;/li&gt;
  &lt;li&gt;Docker&lt;/li&gt;
  &lt;li&gt;Kubernetes&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lot-of-times-we-may-have-to-write-ml-models-from-scratch-in-c--will-you-be-able-to-do-that&quot;&gt;&lt;strong&gt;Lot of times, we may have to write ML models from scratch in C++ ? Will you be able to do that?&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quora.com/I-want-to-use-C++-to-learn-Machine-Learning-instead-of-Python-or-R-is-it-fine&quot;&gt;quora&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-stochastic-gradient-decent-with-momentum-works&quot;&gt;&lt;strong&gt;How Stochastic Gradient Decent with momentum works?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;An SGD can be thought of as a ball rolling down the hill where the velocity of the ball is influenced by the gradient of the 
curve. However, in this approach, the ball has a chance to get stuck in any ravine. So if the ball can have enough momentum to get past
get past the ravine would have been better. based on this idea, SGD with Momentum works. Where the ball has been given 
some added momentum which is based on the previous velocity and gradient.
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;velocity = momentum*past_velocity + learning_rate*gradient&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;w=w-velocity&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;past_velocity = velocity&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/&quot;&gt;easy explanation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;book: deep learning in Python - by Cholet, page 51, but the equation looks suspicious&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://distill.pub/2017/momentum/&quot;&gt;through explanation, distill pub&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-the-model-varies-in-knn-for-k1-and-kn&quot;&gt;&lt;strong&gt;How the model varies in KNN for K=1 and K=N?&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;When K equals 1 or other small number the model is prone to overfitting (high variance), while when K equals number of data points or other large number the model is prone to underfitting (high bias)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;generative-model-vs-discriminative-model&quot;&gt;&lt;strong&gt;Generative model vs Discriminative model.&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Discriminative algorithms model &lt;code class=&quot;highlighter-rouge&quot;&gt;P(y|x; w)&lt;/code&gt;, that is, given the dataset and learned parameter, what is the probability of y belonging to a specific class. A discriminative algorithm doesn’t care about how the data was generated, it simply categorizes a given example.&lt;/li&gt;
  &lt;li&gt;Generative algorithms try to model &lt;code class=&quot;highlighter-rouge&quot;&gt;P(x|y)&lt;/code&gt;, that is, the distribution of features given that it belongs to a certain class. A generative algorithm models how the data was generated. &lt;a href=&quot;https://github.com/ShuaiW/data-science-question-answer#knn&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;scenario-based-question&quot;&gt;&lt;strong&gt;Scenario based Question&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Let’s say, you are given a scenario where you have terabytes of data files consisting of pdfs, text files, images, scanned pdfs etc. What approach will you take in understanding or classifying them ?&lt;/p&gt;

&lt;h3 id=&quot;how-will-you-read-the-content-of-scanned-pdfs-or-written-documents-in-image-formats&quot;&gt;&lt;strong&gt;How will you read the content of scanned pdfs or written documents in image formats?&lt;/strong&gt;&lt;/h3&gt;

&lt;h3 id=&quot;why-is-naive-bayes-called-naive-tell-me-about-naive-bayes-classifier&quot;&gt;&lt;strong&gt;Why is naive bayes called “naive”? Tell me about naive bayes classifier?&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Because it’s assumed that all the features are independent of each other. This is a very &lt;em&gt;naive&lt;/em&gt; assumption.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;logistic-regression-loss-function&quot;&gt;&lt;strong&gt;Logistic Regression loss function?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://ml-cheatsheet.readthedocs.io/en/latest/logistic_regression.html&quot;&gt;ml-cheatsheet quick summary&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cs.cmu.edu/~mgormley/courses/10701-f16/slides/lecture5.pdf&quot;&gt;CMU ML slides&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-do-you-mean-by-mutable-and-immutable-objects-in-python-&quot;&gt;&lt;strong&gt;What do you mean by mutable and immutable objects in python ?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@meghamohan/mutable-and-immutable-side-of-python-c2145cf72747&quot;&gt;Everything in Python is an object&lt;/a&gt;. 
Since everything in Python is an Object, every variable holds 
an object instance. When an object is initiated, it is assigned a unique object id. Its type is 
defined at runtime and once set can never change, however its state can be changed if it is 
mutable. Simple put, a mutable object can be changed after it is created, and an immutable object can’t.&lt;/li&gt;
  &lt;li&gt;Objects of built-in types like (&lt;code class=&quot;highlighter-rouge&quot;&gt;int, float, complex, bool, str, tuple, unicode, frozen set&lt;/code&gt;) are immutable. Objects of 
built-in types like (&lt;code class=&quot;highlighter-rouge&quot;&gt;list, set, dict,byte array&lt;/code&gt;) are mutable. Custom classes are generally mutable.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-are-the-data-structures-you-have-used-in-python-&quot;&gt;What are the data structures you have used in python ?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;set,list,tuple,dictionary, string, frozen set. &lt;a href=&quot;https://docs.python.org/3/tutorial/datastructures.html&quot;&gt;link1&lt;/a&gt;,
&lt;a href=&quot;http://thomas-cokelaer.info/tutorials/python/data_structures.html&quot;&gt;link2&lt;/a&gt; 
——&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-do-you-handle-multi-class-classification-with-unbalanced-dataset-&quot;&gt;How do you handle multi-class classification with unbalanced dataset ?&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/pulse/multi-class-classification-imbalanced-data-using-random-burak-ozen/&quot;&gt;link1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;handling imbalanced data by resampling original data to provide balanced classes.
    &lt;ul&gt;
      &lt;li&gt;Random under sampling&lt;/li&gt;
      &lt;li&gt;Random over sampling&lt;/li&gt;
      &lt;li&gt;Cluster based over sampling&lt;/li&gt;
      &lt;li&gt;Informed Over Sampling: Synthetic Minority Over-sampling Technique (&lt;strong&gt;SMOTE&lt;/strong&gt;)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Modifying existing classification algorithms to make them appropriate for imbalanced data sets.
    &lt;ul&gt;
      &lt;li&gt;Bagging based&lt;/li&gt;
      &lt;li&gt;Boosting based: AdaBoost, Gradient Boost, XGBoost&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/&quot;&gt;imp source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;how-do-you-select-between-2-models-model-selection-techniques&quot;&gt;&lt;strong&gt;How do you select between 2 models (Model Selection techniques)?&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;To choose between 2 model generally &lt;code class=&quot;highlighter-rouge&quot;&gt;AIC&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;BIC&lt;/code&gt; are used.
Generally, the most commonly used metrics, for measuring regression model quality and for comparing models, are: &lt;code class=&quot;highlighter-rouge&quot;&gt;Adjusted R2&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;AIC&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;BIC&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Cp&lt;/code&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;AIC&lt;/strong&gt; stands for (Akaike’s Information Criteria), a metric developped by the Japanese Statistician, Hirotugu Akaike, 1970. The basic idea of AIC is to &lt;code class=&quot;highlighter-rouge&quot;&gt;penalize the inclusion of additional variables&lt;/code&gt; to a model. It adds a penalty that increases the error when including additional terms. &lt;code class=&quot;highlighter-rouge&quot;&gt;The lower the AIC, the better the model.&lt;/code&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;AICc&lt;/code&gt; is a version of AIC corrected for small sample sizes.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;BIC&lt;/strong&gt; (or Bayesian information criteria) is a variant of AIC with a stronger penalty for including additional variables to the model.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mallows Cp:&lt;/strong&gt; A variant of AIC developed by Colin Mallows&lt;/li&gt;
  &lt;li&gt;$R^2$ not a good criterion.Always increase with model size –&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;optimum&lt;/code&gt; is to take the biggest model.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Adjusted&lt;/code&gt; $R^2$: better. It &lt;code class=&quot;highlighter-rouge&quot;&gt;penalized&lt;/code&gt; bigger models.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Source&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.sthda.com/english/articles/38-regression-model-validation/158-regression-model-accuracy-metrics-r-square-aic-bic-cp-and-more/&quot;&gt;Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/topics/medicine-and-dentistry/akaike-information-criterion&quot;&gt;Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://statweb.stanford.edu/~jtaylo/courses/stats203/notes/selection.pdf&quot;&gt;ppt-Stanford&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;okay-great-bic--aic--how-does-it-work-mathematicallyexplain-the-intuition-behind-bic-or-aic-&quot;&gt;&lt;strong&gt;Okay great BIC &amp;amp; AIC !! How does it work mathematically?Explain the intuition behind BIC or AIC ?&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;In general, it might be best to use AIC and BIC together in model selection.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For example, in selecting the number of latent classes in a model, if BIC points to a three-class model and AIC points to a five-class model, it makes sense to select from models with 3, 4 and 5 latent classes.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;AIC is better in situations when a false negative finding would be considered more misleading than a false positive&lt;/code&gt;,&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;BIC is better in situations where a false positive is as misleading as, or more misleading than, a false negative&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;AIC=-2\log L(\hat \theta) + 2k&lt;/script&gt;

&lt;p&gt;where,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\theta$= the set (vector) of model parameters&lt;/li&gt;
  &lt;li&gt;$L(\theta)$ =  the  likelihood  of  the  candidate  model  given  the  data  when  evaluated at the maximum likelihood estimate of $\theta$&lt;/li&gt;
  &lt;li&gt;$k$ = the number of estimated parameters in the candidate model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first compo-nent, $−2\log L(\hat \theta)$, is the value of the likelihood function, $\log L(\theta)$, which is the probability of obtaining the data given the candidate model.
The  more  parameters,  the  greater the amount added to the first component, increasing the value for the AIC and penalizing the model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;BIC&lt;/strong&gt; is  another  model  selection  criterion  based  on  infor-mation theory but set within a Bayesian context. The difference between the BIC and the AIC is the greater penalty imposed for the number of param-eters  by  the BIC  than the AIC.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;BIC=-2\log L(\hat \theta) + k \log n&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Source:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.methodology.psu.edu/resources/aic-vs-bic/&quot;&gt;Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118856406.app5&quot;&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;what-is-precision-and-recall--which-one-of-this-do-you-think-is-important-in-medical-diagnosis&quot;&gt;&lt;strong&gt;What is precision and recall ? Which one of this do you think is important in medical diagnosis?&lt;/strong&gt;&lt;/h2&gt;

&lt;h2 id=&quot;type-i-and-type-ii-errors&quot;&gt;&lt;strong&gt;Type I and Type II Errors&lt;/strong&gt;&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;One fine morning, Jack got a phone call. It was a stranger on the line. Jack, still sipping his freshly brewed morning coffee, was barely in a position to understand what was coming for him. The stranger said, “Congratulations Jack! You have won a lottery of $10 Million! I just need you to provide me your bank account details, and the money will be deposited in your bank account right way…”&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;What are the odds of that happening? What should Jack do? What would you have done?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/454/1*t_t7cMq3FGqDk6gbwfA4EA.png&quot; alt=&quot;image&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Type I: False Positive&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Type II: False Negative&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*pOtBHai4jFd-ujaNXPilRg.png&quot; alt=&quot;image&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/precision-vs-recall-386cf9f89488&quot;&gt;blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;what-does-auc-roc-curve-signify-&quot;&gt;&lt;strong&gt;What does AUC-ROC curve signify ?&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;AUC - ROC curve is a performance measurement for classification problem &lt;code class=&quot;highlighter-rouge&quot;&gt;at various thresholds settings&lt;/code&gt;. ROC is a &lt;strong&gt;probability curve&lt;/strong&gt; and AUC represents &lt;strong&gt;degree or measure of separability&lt;/strong&gt;. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.&lt;/p&gt;

&lt;h3 id=&quot;how-do-you-draw-auc-roc-curve-&quot;&gt;&lt;strong&gt;How do you draw AUC-ROC curve ?&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*k65OKy7TOhBWRIfx0u6JqA.png&quot; alt=&quot;image&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*hf2fRUKfD-hCSw1ifUOCpg.png&quot; alt=&quot;image&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;True positive&lt;/code&gt; is the area designated as “bad” on the right side of the threshold (mild sky blue region). &lt;code class=&quot;highlighter-rouge&quot;&gt;False positive&lt;/code&gt; denotes the area designated as “good” on the right of the threshold.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Total positive&lt;/code&gt; is the total area under the “bad” curve while total negative is the total area under the “good” curve.&lt;/li&gt;
  &lt;li&gt;We divide the value as shown in the diagram to derive TPR and FPR.&lt;/li&gt;
  &lt;li&gt;We derive the TPR and FPR at different threshold values (by sliding the black vertical bar in the above image) to get the ROC curve. Using this knowledge, we create the ROC plot function.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Bonus Question:&lt;/strong&gt; Write pseudo-code to generate the data for such a curve. [Check the below blog]&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/receiver-operating-characteristic-curves-demystified-in-python-bd531a4364d0&quot;&gt;Imp Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The ROC curve is plotted with &lt;code class=&quot;highlighter-rouge&quot;&gt;TPR&lt;/code&gt; against the &lt;code class=&quot;highlighter-rouge&quot;&gt;FPR&lt;/code&gt; where TPR is on y-axis and FPR is on the x-axis.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/361/1*pk05QGzoWhCgRiiFbz-oKQ.png&quot; alt=&quot;image&quot; width=&quot;250&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;TPR == RECALL&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{TP}{TP+FN}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Specificity:&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{TN}{TN+FP}&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;FPR == 1-Specificity&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{FP}{TN+FP}&lt;/script&gt;

&lt;h3 id=&quot;how-will-you-draw-roc-for-multi-class-classification-problem&quot;&gt;&lt;strong&gt;How will you draw ROC for multi class classification problem&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;In multi-class model, we can plot N number of AUC ROC Curves for N number classes using One vs ALL methodology. So for Example, If you have three classes named X, Y and Z, you will have one ROC for X classified against Y and Z, another ROC for Y classified against X and Z, and a third one of Z classified against Y and X.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5&quot;&gt;Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;what-is-random-about-random-forest-&quot;&gt;&lt;strong&gt;What is random about Random Forest ?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;for different &lt;code class=&quot;highlighter-rouge&quot;&gt;tree&lt;/code&gt;, a different datasets (build from original using &lt;em&gt;random resampling with replacement&lt;/em&gt;) are given as input.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ol&gt;
  &lt;li&gt;Tell me any other metric to measure multi-class classification result ?
What is sensitivity and specificity ?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Name the package of scikit-learn that implements logistic regression&lt;/strong&gt; ?&lt;/li&gt;
  &lt;li&gt;What is mean and variance of standard normal distribution ?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is central limit theoram&lt;/strong&gt;?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Law of Large Number&lt;/strong&gt;?&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;What are the data structures you have used in python ?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;What is naive bayes classifier ?&lt;/li&gt;
  &lt;li&gt;What is the probability of heads coming 4 times when a coin is tossed 10 number of times ?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How do you get an index of an element of a list in python&lt;/strong&gt; ?
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/176918/finding-the-index-of-an-item-given-a-list-containing-it-in-python&quot;&gt;link1&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;How do you merge two data-set with pandas?
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;frames = [df1, df2, df3];result = pd.concat(frames)&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://pandas.pydata.org/pandas-docs/stable/merging.html&quot;&gt;link1&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;From user behavior, you need to model fraudulent activity. How are you going to solve this ? May be anomaly detection problem or a classification problem !!&lt;/li&gt;
  &lt;li&gt;What will you prefer a decision tree or a random forest ?&lt;/li&gt;
  &lt;li&gt;How is using a logistic regression different from using a random forest ?
    &lt;ul&gt;
      &lt;li&gt;If your data is linearly separable, go with logistic regression. However, in real world, data is rarely linearly separable. Most of the time data would be a jumbled mess.
In such scenarioes, Decision trees would be a better fit as DT essentially is a non-linear classifier. As DT is prone to over fitting, Random Forests are used in practice to better generalize the fitment. RF provide a good balance between precision and overfitting.
        &lt;ul&gt;
          &lt;li&gt;If your problem/data is linearly separable, then first try logistic regression. If you don’t know, then still start with logistic regression because that will be your baseline, followed by non-linear classifier such as random forest. Do not forget to tune the parameters of logistic regression / random forest for maximizing their performance on your data.&lt;/li&gt;
          &lt;li&gt;If your data is categorical, then random forest should be your first choice; however, logistic regression can be dealt with categorical data[1].&lt;/li&gt;
          &lt;li&gt;If you want easy to understand results, logistic regression is a better choice because it leads to simple interpretation of the explanatory variables.&lt;/li&gt;
          &lt;li&gt;If speed is your criteria, then logistic regression should be your choice[2].&lt;/li&gt;
          &lt;li&gt;If your data is unbalanced, then random forest may be a better choice[3].&lt;/li&gt;
          &lt;li&gt;If number of data objects are less than the number of features, logistic regression should not be used[4].&lt;/li&gt;
          &lt;li&gt;Lastly, as noted in this paper, either of the random forest or logistic regression “models appear to perform similarly across the datasets with performance more influenced by choice of dataset rather than model selection”&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.quora.com/When-should-random-forest-be-used-over-logistic-regression-for-classification-and-vice-versa&quot;&gt;link1&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Will you use decision tree or random forest for a classification problem ? 
What is advantage of using  random forest ?
——&lt;/li&gt;
  &lt;li&gt;Which model would you use in case of unbalanced dataset: Random Forest or Boosting ? Why ?
    &lt;ul&gt;
      &lt;li&gt;Gradient boosting is also a good choice here. You can use the gradient boosting classifier in sci-kit learn for example. Gradient boosting is a principled method of dealing with class imbalance by constructing successive training sets based on incorrectly classified examples.&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/&quot;&gt;link1&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;An alternative could be a cost-sensitive algorithm like C5.0 that doesn’t need balanced data. You could also think about applying Markov chains to your problem.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;What are the boosting techniques you know ?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Which model would you like to choose if you have many classes in a supervised learning problem ? Say 40-50 classes !!&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;How do you perform ensemble technique?&lt;/li&gt;
  &lt;li&gt;How does SVM work ?&lt;/li&gt;
  &lt;li&gt;What is Kernel ? Explain a few.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How do you perform non-linear regression&lt;/strong&gt;?
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.statisticshowto.com/nonlinear-regression/&quot;&gt;link1&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;\frac{a}{b}&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;what-are-lasso-and-ridge-regression-&quot;&gt;What are Lasso and Ridge regression ?&lt;/h2&gt;
  &lt;/li&gt;
  &lt;li&gt;What is Gaussian Mixture model ? How does it perform clustering ?&lt;/li&gt;
  &lt;li&gt;How is Expectation Maximization performed ? Explain both the steps ?&lt;/li&gt;
  &lt;li&gt;How is likelihood calculated in GMM ?&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Sankarshan Mridha</name></author><summary type="html">Q source:</summary></entry><entry><title type="html">Blog 102: ML QnA2</title><link href="http://localhost:4000/blog/2019/07/28/blog_102_Typical_ML_QA2" rel="alternate" type="text/html" title="Blog 102: ML QnA2" /><published>2019-07-28T00:11:31+05:30</published><updated>2019-07-28T00:11:31+05:30</updated><id>http://localhost:4000/blog/2019/07/28/blog_102_Typical_ML_QA2</id><content type="html" xml:base="http://localhost:4000/blog/2019/07/28/blog_102_Typical_ML_QA2">&lt;h1 id=&quot;question-source&quot;&gt;&lt;strong&gt;Question Source&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quora.com/What-are-some-common-machine-learning-interview-questions&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-inductive-bias-selection-bias-statistical-bias&quot;&gt;&lt;strong&gt;What is inductive bias, selection bias, statistical bias?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Inductive bias&lt;/strong&gt; is the set of assumptions a learner uses to predict results given inputs it has not yet encountered.
    &lt;ul&gt;
      &lt;li&gt;Generally speaking, the closer your assumptions are to reality the better your results will be.  Additionally, more and stronger assumptions tend to make solving problems easier. If an assumption is correct then it’s always helpful to use it. However, it’s also common to assume things that obviously aren’t true because they’re practical.  Every machine learning algorithm has some sort of inductive bias, and it’s those underlying assumptions you may not even realize you’re making that determine how well your algorithms are going to work in practice.&lt;/li&gt;
      &lt;li&gt;Let’s suppose you don’t know anything about swans. You decide to visit Swan River in Australia. You see a swan for the first time, and that swan is black.  What color will the next Swan you see be? Black seems like a pretty good guess. It’s not a logical guess, but given what we know, black is a better guess than anything else.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Selection bias&lt;/strong&gt; is the bias introduced by the selection of individuals, groups or data for analysis in such a way that proper randomization is not achieved, thereby ensuring that the sample obtained is not representative of the population intended to be analyzed.[1] It is sometimes referred to as the selection effect. The phrase “selection bias” most often refers to the distortion of a statistical analysis, resulting from the method of collecting samples. If the selection bias is not taken into account, then some conclusions of the study may not be accurate. Example: Sampling Bias
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Sampling Bias&lt;/strong&gt; is systematic error due to a non-random sample of a population,causing some members of the population to be less likely to be included than others, resulting in a biased sample, defined as a statistical sample of a population (or non-human factors) in which all participants are not equally balanced or objectively represented. It is mostly classified as a subtype of selection bias.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Statistical&lt;/strong&gt;l Bias: For a point estimator, statistical bias is defined as the difference between the parameter to be estimated and the mathematical expectation of the estimator.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Source&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://data36.com/statistical-bias-types-explained/&quot;&gt;source&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://newonlinecourses.science.psu.edu/stat509/node/28/&quot;&gt;source2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-the-difference-between-inductive-machine-learning-and-deductive-machine-learning&quot;&gt;&lt;strong&gt;What is the difference between inductive machine learning and deductive machine learning?&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Inductive Learning:&lt;/strong&gt; We are given input samples (x) and output samples (f(x)) and the problem is to estimate the function (f). Specifically, the problem is to generalize from the samples and the mapping to be useful to estimate the output for new samples in the future. &lt;a href=&quot;https://machinelearningmastery.com/basic-concepts-in-machine-learning/&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-will-you-know-which-machine-learning-algorithm-to-choose-for-your-classification-problem&quot;&gt;&lt;strong&gt;How will you know which machine learning algorithm to choose for your classification problem?&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;If your training set is small, &lt;code class=&quot;highlighter-rouge&quot;&gt;high bias/low variance classifiers&lt;/code&gt; (e.g., Naive Bayes) have an advantage over &lt;code class=&quot;highlighter-rouge&quot;&gt;low bias/high variance classifiers&lt;/code&gt; (e.g., kNN), since the latter will overfit. But low bias/high variance classifiers start to win out as your training set grows (they have lower asymptotic error), since high bias classifiers aren’t powerful enough to provide accurate models.&lt;/p&gt;

&lt;h3 id=&quot;advantages-of-some-particular-algorithms&quot;&gt;&lt;strong&gt;Advantages of some particular algorithms&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Advantages of Naive Bayes:&lt;/strong&gt; Super simple, you’re just doing a bunch of counts. If the NB &lt;code class=&quot;highlighter-rouge&quot;&gt;conditional independence&lt;/code&gt; assumption actually holds, a Naive Bayes classifier will converge quicker than &lt;code class=&quot;highlighter-rouge&quot;&gt;discriminative models&lt;/code&gt; like logistic regression, so you need less training data. And even if the NB assumption doesn’t hold, a NB classifier still often does a great job in practice. A good bet if want something fast and easy that performs pretty well.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Main Disadvantage&lt;/strong&gt; is that it can’t learn &lt;code class=&quot;highlighter-rouge&quot;&gt;interactions between features&lt;/code&gt; (e.g., it can’t learn that although you love movies with Brad Pitt and Tom Cruise, you hate movies where they’re together).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Advantages of Logistic Regression:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Lots of ways to regularize your model, and you don’t have to worry as much about your features being correlated, like you do in Naive Bayes.&lt;/li&gt;
  &lt;li&gt;You also have a nice probabilistic interpretation, unlike decision trees or SVMs, and you can easily update your model to take in new data (using an online gradient descent method), again unlike decision trees or SVMs. Use it if you want a probabilistic framework (e.g., to easily adjust &lt;code class=&quot;highlighter-rouge&quot;&gt;classification thresholds&lt;/code&gt;, to say when you’re unsure, or to get confidence intervals) or if you expect to receive more training data in the future that you want to be able to quickly incorporate into your model.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Advantages of Decision Trees:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Easy to interpret and explain (for some people – I’m not sure I fall into this camp). They easily handle feature interactions and they’re &lt;strong&gt;non-parametric&lt;/strong&gt;, so you don’t have to worry about outliers or whether the data is linearly separable (e.g., decision trees easily take care of cases where you have class A at the low end of some feature x, class B in the mid-range of feature x, and A again at the high end).
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Disadvantage&lt;/strong&gt; is that they don’t support online learning, so you have to rebuild your tree when new examples come on.&lt;/li&gt;
      &lt;li&gt;Another &lt;strong&gt;disadvantage&lt;/strong&gt; is that they &lt;code class=&quot;highlighter-rouge&quot;&gt;easily overfit&lt;/code&gt;, but that’s where ensemble methods like random forests (or boosted trees) come in. Plus, random forests are often the winner for lots of problems in classification (usually slightly ahead of SVMs, I believe), they’re fast and scalable, and you don’t have to worry about tuning a bunch of parameters like you do with SVMs, so they seem to be quite popular these days.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Advantages of SVMs:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;High accuracy, nice theoretical guarantees regarding overfitting, and with an appropriate kernel they can work well even if you’re data isn’t linearly separable in the base feature space. Especially popular in text classification problems where very high-dimensional spaces are the norm.
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Disadvantage:&lt;/strong&gt; Memory-intensive, hard to interpret, and kind of annoying to run and tune, though, so I think random forests are starting to steal the crown.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/&quot;&gt;must read&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-choice&quot;&gt;cheat sheet&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bias-and-variance-tradeoff&quot;&gt;&lt;strong&gt;Bias and Variance Tradeoff&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Bias:&lt;/strong&gt; Bias is the difference between the average prediction of our model and the correct value which we are trying to predict.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Variance:&lt;/strong&gt; Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn’t seen before.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mathematically:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Let the variable we are trying to predict as &lt;code class=&quot;highlighter-rouge&quot;&gt;Y&lt;/code&gt; and other covariates as &lt;code class=&quot;highlighter-rouge&quot;&gt;X&lt;/code&gt;. We assume there is a relationship between the two such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y = f(X) + error&lt;/script&gt;

&lt;p&gt;So the expected squared error at a point x is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Err(x) = E[(Y - \hat f(x))^2]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Err(x)=(E[\hat f(x)]-f(x))^2 + E[(\hat f(x) - E[\hat f(x)])^2] + \sigma_e^2&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Err(x)=Bias^2 + Variance + Irreducible error&lt;/script&gt;

&lt;p&gt;In the above diagram, center of the target is a model that perfectly predicts correct values. As we move away from the bulls-eye our predictions become get worse and worse.&lt;/p&gt;

&lt;p&gt;In &lt;strong&gt;supervised learning&lt;/strong&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;underfitting&lt;/code&gt; happens when a model unable to capture the underlying pattern of the data. These models usually have &lt;code class=&quot;highlighter-rouge&quot;&gt;high bias and low variance&lt;/code&gt;. It happens when we have &lt;code class=&quot;highlighter-rouge&quot;&gt;very less amount of data&lt;/code&gt; to build an accurate model or when we try to build a linear model with a nonlinear data. Also, these kind of models are very simple to capture the complex patterns in data like Linear and logistic regression.&lt;/p&gt;

&lt;p&gt;In &lt;strong&gt;supervised learning&lt;/strong&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;overfitting&lt;/code&gt; happens when our model &lt;code class=&quot;highlighter-rouge&quot;&gt;captures the noise&lt;/code&gt; along with the underlying pattern in data. It happens when we train our model a lot over noisy dataset. These models have &lt;code class=&quot;highlighter-rouge&quot;&gt;low bias and high variance&lt;/code&gt;. These models are very complex like Decision trees which are prone to overfitting.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*9hPX9pAO3jqLrzt0IE3JzA.png&quot; alt=&quot;image&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If our model is too simple and has very few parameters then it may have &lt;code class=&quot;highlighter-rouge&quot;&gt;high bias and low variance&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;On the other hand if our model has large number of parameters then it’s going to have &lt;code class=&quot;highlighter-rouge&quot;&gt;high variance and low bias&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;So we need to find the right/good balance without overfitting and underfitting the data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/562/1*RQ6ICt_FBSx6mkAsGVwx8g.png&quot; alt=&quot;image&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Source&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229&quot;&gt;TDS: Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;different-types-of-bias-in-ml&quot;&gt;&lt;strong&gt;Different types of Bias in ML?&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Source&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/math_rachel/status/1121224794823806976&quot;&gt;Tweet Thread: Rachel Thomas&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;why-do-we-needwant-the-bias-term&quot;&gt;&lt;strong&gt;Why do we need/want the bias term?&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;In linear regression, without the bias term your solution has to go through the origin. That is, when all of your features are zero, your predicted value would also have to be zero. However, that may not be the answer the training data suggests. Adding a bias weight that does not depend on any of the features allows the hyperplane desbribed by your learned weights to more easily fit data that doesn’t pass through the origin.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quora.com/Why-do-we-need-the-bias-term-in-ML-algorithms-such-as-linear-regression-and-neural-networks&quot;&gt;Quora&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-are-kernel-methods-different-from-neural-network&quot;&gt;&lt;strong&gt;How are kernel methods different from Neural Network?&lt;/strong&gt;&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;they are more than simply related, they are duals of each other ~Shakir M&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;One way to think about an &lt;code class=&quot;highlighter-rouge&quot;&gt;NN&lt;/code&gt; with &lt;code class=&quot;highlighter-rouge&quot;&gt;L&lt;/code&gt; layers is that the first &lt;code class=&quot;highlighter-rouge&quot;&gt;L - 1&lt;/code&gt; layers perform some &lt;code class=&quot;highlighter-rouge&quot;&gt;non-linear transformation&lt;/code&gt; $\phi(x_i;\theta)$ on the input data point $x_i$ parameterized by $\theta$, while the last layer is just a linear model on these non-linear representations parameterized by a weights vector $w$.&lt;/p&gt;

&lt;p&gt;It can be shown that some class of NN models is mathematically equivalent to a &lt;code class=&quot;highlighter-rouge&quot;&gt;kernelized ridge-regression&lt;/code&gt; model. Further steps in the &lt;code class=&quot;highlighter-rouge&quot;&gt;back-propagation&lt;/code&gt; adjust the parameters $\theta$ of the &lt;code class=&quot;highlighter-rouge&quot;&gt;non-linear transformation&lt;/code&gt; $\phi$, which has the effect of adjusting the equivalent kernel matrix at the $t$ step of the next iteration of back-propagation. So it could be said that this class of NNs is mathematically equivalent to a kernelized ridge-regression that learns its own kernel from the data. Of course, these NNs are only able to learn kernels in finite-dimensional spaces, unlike the kernel based-methods which can use kernels in infinite-dimensional spaces. For full math details check the below blogs.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.shakirm.com/2015/04/a-statistical-view-of-deep-learning-iii-memory-and-kernels/&quot;&gt;Blog by Shakir&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;quora&quot;&gt;&lt;a href=&quot;https://www.quora.com/What-are-the-mathematical-relationship-between-kernel-methods-and-Neural-Networks-NN&quot;&gt;Quora&lt;/a&gt;&lt;/h2&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;statistical-view-of-deep-learning&quot;&gt;&lt;strong&gt;Statistical view of Deep Learning&lt;/strong&gt;&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;In deep learning, the link function is referred to as the activation function&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.shakirm.com/2015/01/a-statistical-view-of-deep-learning-i-recursive-glms/&quot;&gt;Blog: Shakir M&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;different-types-of-models-and-their-example&quot;&gt;&lt;strong&gt;Different types of models and their example&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Parametric Model:&lt;/strong&gt; A parametric modelis one that can be parametrized by a finite number of parameters&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Non-Parametric Model:&lt;/strong&gt; A nonparametric modelis one which cannot be parametrized by a fixed number of parameters.&lt;/li&gt;
  &lt;li&gt;Low-bias High-Variance Model&lt;/li&gt;
  &lt;li&gt;High-Biad Low-Variance Model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Source:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://pages.cs.wisc.edu/~jerryzhu/cs731/stat.pdf&quot;&gt;Note&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;how-are-neural-nets-related-to-fourier-transforms-what-are-fourier-transforms-for-that-matter&quot;&gt;&lt;strong&gt;How are neural nets related to Fourier transforms? What are Fourier transforms, for that matter?&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.quora.com/How-are-neural-networks-related-to-Fourier-transforms&quot;&gt;Quora&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;how-to-seed-k-means-clustering-algorithm&quot;&gt;&lt;strong&gt;How to &lt;code class=&quot;highlighter-rouge&quot;&gt;seed&lt;/code&gt; K-means clustering algorithm?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Important:&lt;/strong&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;seed&lt;/code&gt;ing meens, how to initialize the first &lt;code class=&quot;highlighter-rouge&quot;&gt;k&lt;/code&gt; centres? Use the algorithm &lt;code class=&quot;highlighter-rouge&quot;&gt;K-Means++&lt;/code&gt;. &lt;a href=&quot;https://en.wikipedia.org/wiki/K-means%2B%2B&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;the-blas-library-and-how-does-it-work-what-are-the-other-option&quot;&gt;&lt;strong&gt;The &lt;code class=&quot;highlighter-rouge&quot;&gt;BLAS&lt;/code&gt; library and how does it work? what are the other option?&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Basic Linear Algebra Subprograms (BLAS) is a specification that prescribes a set of low-level routines for performing common linear algebra operations such as vector addition, scalar multiplication, dot products, linear combinations, and matrix multiplication. They are the de facto standard low-level routines for linear algebra libraries; the routines have bindings for both C and Fortran. Although the BLAS specification is general, BLAS implementations are often optimized for speed on a particular machine, so using them can bring substantial performance benefits. BLAS implementations will take advantage of special floating point hardware such as vector registers or SIMD instructions. (&lt;a href=&quot;https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms&quot;&gt;wiki&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Alternative to BLAS is &lt;code class=&quot;highlighter-rouge&quot;&gt;LAPACK&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;ATLAS&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Source:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/1303182/how-does-blas-get-such-extreme-performance&quot;&gt;SO1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/17858104/what-is-the-relation-between-blas-lapack-and-atlas&quot;&gt;SO2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;compare-stochastic-gradient-descent-to-interior-point-methods&quot;&gt;&lt;strong&gt;Compare stochastic gradient descent to interior point methods&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://mlss.tuebingen.mpg.de/2013/wright_slides.pdf&quot;&gt;mlss tuebingen slide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;what-areas-of-machine-learning-are-you-most-familiar-with&quot;&gt;&lt;strong&gt;What areas of machine learning are you most familiar with?&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Very generic Qusestion. Prepare well.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Supervised learning&lt;/li&gt;
  &lt;li&gt;Unsupervised learning&lt;/li&gt;
  &lt;li&gt;Anomaly Detection&lt;/li&gt;
  &lt;li&gt;Active Learning&lt;/li&gt;
  &lt;li&gt;Bandits:
    &lt;ul&gt;
      &lt;li&gt;In the multi-armed bandit problem, at each stage, an agent (or decision maker) chooses one action (or arm), and receives a reward from it. The agent aims at maximizing his rewards. Since he does not know the process generating the rewards, he needs to explore (try) the different actions and yet, exploit (concentrate its draws on) the seemingly most rewarding arms. The bandit problem has been increasingly popular in the machine learning community.&lt;/li&gt;
      &lt;li&gt;It can be a central building block of larger systems, like in &lt;code class=&quot;highlighter-rouge&quot;&gt;evolutionary programming&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;reinforcement learning&lt;/code&gt;, in particular in large state space Markovian Decision Problems. ((ICML Tutorial)[https://sites.google.com/site/banditstutorial/])&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Gaussian Processes&lt;/li&gt;
  &lt;li&gt;Kernel Methods&lt;/li&gt;
  &lt;li&gt;Deep Networks&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;tell-about-positives-and-negatives-of-using-gaussian-processes--general-kernel-methods-approach-to-learning&quot;&gt;&lt;strong&gt;Tell about positives and negatives of using Gaussian processes / general kernel methods approach to learning.&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Positives - non-linear, non-parametric. Negatives - bad scaling with instances, need to do hyper-parameter tuning&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-does-a-kernel-method-scale-with-the-number-of-instances-eg-with-a-gaussian-rbf-kernel&quot;&gt;&lt;strong&gt;How does a kernel method scale with the number of instances (e.g. with a Gaussian rbf kernel)?&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Quadratic (referring to construction of the gram (kernel) matrix), cubic (referring to the matrix inversion)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;describe-ways-to-overcome-scaling-issues&quot;&gt;&lt;strong&gt;Describe ways to overcome scaling issues.&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;nystrom methods/low-rank kernel matrix approximations, random features, local by query/near neighbors&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-are-some-tools-for-parallelizing-machine-learning-algorithms&quot;&gt;&lt;strong&gt;What are some tools for parallelizing machine learning algorithms?&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;GPUs, Matlab parfor, write your own using low level primitives/RPC/MPI, mapreduce, spark, vowpal, graphlab, giraph, petuum, parameterserver&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;in-python-do-you-have-a-favoriteleast-favorite-pep&quot;&gt;&lt;strong&gt;In Python, do you have a favorite/least favorite PEP?&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;peps-are-python-enhancement-proposal-if-you-have-a-favorite-or-least-favorite-it-means-they-have-knowledge-of-python-follow-pep8&quot;&gt;Peps are &lt;strong&gt;python enhancement proposal&lt;/strong&gt;. If you have a favorite or least favorite, it means they have knowledge of Python. Follow PEP8&lt;/h2&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Mention the difference between Data Mining and Machine learning?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Is rotation necessary in PCA? If yes, Why? What will happen if you dont rotate the components?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;You are given a data set. The data set has missing values which spread along 1 standard deviation from the median. What percentage of data would remain unaffected? Why?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Why is Naive Bayes machine learning algorithm nave?&lt;/li&gt;
  &lt;li&gt;How will you explain machine learning in to a layperson?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is inductive machine learning?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;What are the different Algorithm techniques in Machine Learning?&lt;/li&gt;
  &lt;li&gt;List out some important methods of reducing dimensionality.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Explain prior probability, likelihood and marginal likelihood in context of na�ve Bayes algorithm?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;What are the three stages to build the hypotheses or model in machine learning?&lt;/li&gt;
  &lt;li&gt;What is the standard approach to supervised learning?&lt;/li&gt;
  &lt;li&gt;What is Training set and Test set?&lt;/li&gt;
  &lt;li&gt;List down various approaches for machine learning?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How to know that your model is suffering from low bias and high variance. Which algorithm should you use to tackle it? Why?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How is kNN different from kmeans clustering?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Name some feature extraction techniques used for dimensionality reduction.&lt;/li&gt;
  &lt;li&gt;List some use cases where classification machine learning algorithms can be used.&lt;/li&gt;
  &lt;li&gt;What kind of problems does regularization solve?&lt;/li&gt;
  &lt;li&gt;How much data will you allocate for your training, validation and test sets?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Which one would you prefer to choose � model accuracy or model performance?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;What is the most frequent metric to assess model accuracy for classification problems?&lt;/li&gt;
  &lt;li&gt;Describe some popular machine learning methods.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is not Machine Learning?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Explain what is the function of �Unsupervised Learning�?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;When will you use classification over regression?&lt;/li&gt;
  &lt;li&gt;How will you differentiate between supervised and unsupervised learning? Give few examples of algorithms for supervised learning?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Explain the tradeoff between bias and variance in a   regression problem.&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;In regression, the expected mean squared error of an estimator can be decomposed in terms of bias, variance and noise. On average over datasets of the regression problem, the bias term measures the average amount by which the predictions of the estimator differ from the predictions of the best possible estimator for the problem (i.e., the Bayes model). The variance term measures the variability of the predictions of the estimator when fit over different instances LS of the problem. Finally, the noise measures the irreducible part of the error which is due the variability in the data. &lt;a href=&quot;http://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Bias:&lt;/strong&gt; It represents the extent to which the average pridiction over all the datasets differs from the desired regression function.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Variance:&lt;/strong&gt; It measures the extent to which the solution to the particular datasets vary around their average, and hence it measures the extent to which the learnt function is sensitive to the particular choice of data. [source, bisop book, p149]&lt;/li&gt;
      &lt;li&gt;$Err(x) = E\left[(Y-\hat{f}(x))^2\right]$, then we can write $Err(x) = \left(E[\hat{f}(x)]-f(x)\right)^2 + E\left[\left(\hat{f}(x)-E[\hat{f}(x)]\right)^2\right] +\sigma_e^2$, i.e. $Err(x) = \mathrm{Bias}^2 + \mathrm{Variance} + \mathrm{Irreducible\ Error}$&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://scott.fortmann-roe.com/docs/BiasVariance.html&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;What is linear regression? Why is it called linear?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How does the variance of the error term change with the number of predictors, in OLS?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Do we always need the intercept term? When do we need it and when do we not?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How interpretable is the given machine learning model?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;What will you do if training results in very low accuracy?&lt;/li&gt;
  &lt;li&gt;Does the developed machine learning model have convergence problems?&lt;/li&gt;
  &lt;li&gt;Which tools and environments have you used to train and assess machine learning models?&lt;/li&gt;
  &lt;li&gt;How will you apply machine learning to images?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is collinearity and what to do with it?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How to remove multicollinearity?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;What is overfitting a regression model? What are ways to avoid it?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is loss function in a Neural Network?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Explain the difference between MLE and MAP inference.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;What is boosting?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;If the gradient descent does not converge, what could be the problem?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How will you check for a valid binary search tree?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How to check if the regression model fits the data well?&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;describe-some-of-the-different-splitting-rules-used-by-different-decision-tree-algorithms&quot;&gt;&lt;strong&gt;Describe some of the different splitting rules used by different decision tree algorithms.&lt;/strong&gt;&lt;/h2&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;distributed-systems&quot;&gt;&lt;strong&gt;Distributed systems&lt;/strong&gt;&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Discuss MapReduce (or your favorite parallelization abstraction). Why is MapReduce referred to as a “shared-nothing” architecture (clearly the nodes have to share something, no?) What are the advantages/disadvantages of “shared-nothing”?&lt;/li&gt;
  &lt;li&gt;Pick an algorithm. Write the pseudo-code for its parallel version.&lt;/li&gt;
  &lt;li&gt;What are the trade-offs between closed-form and iterative implementations of an algorithm, in the context of distributed systems?&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;experiance-based-question-hands-on-experience-past-accomplishments-etc&quot;&gt;&lt;strong&gt;Experiance based question (hands-on experience, past accomplishments, etc.):&lt;/strong&gt;&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Do you have experience with R (or Weka, Scikit-learn, SAS, Spark, etc.)? Tell me what you’ve done with that. Write some example data pipelines in that environment.&lt;/li&gt;
  &lt;li&gt;Tell me about a time when you … { worked on a project involving ML ; optimized an algorithm for performance/accuracy/etc. }&lt;/li&gt;
  &lt;li&gt;Estimate the amount of time in your past project spent on each segment of your data mining/machine learning work.&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;qna&quot;&gt;QnA&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/&quot;&gt;link1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://elitedatascience.com/machine-learning-interview-questions-answers&quot;&gt;link2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.educba.com/machine-learning-interview-questions/&quot;&gt;link3&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;qna-on-r&quot;&gt;QnA on R&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.educba.com/r-interview-questions/&quot;&gt;link1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://intellipaat.com/interview-question/r-interview-questions/&quot;&gt;link2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.edureka.co/blog/interview-questions/r-interview-questions/&quot;&gt;link3&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;qna-on-python&quot;&gt;QnA on Python&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.edureka.co/blog/interview-questions/python-interview-questions/&quot;&gt;link1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://intellipaat.com/interview-question/python-interview-questions/&quot;&gt;link2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://career.guru99.com/top-25-python-interview-questions/&quot;&gt;link3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.codementor.io/sheena/essential-python-interview-questions-du107ozr6&quot;&gt;link4&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mindmajix.com/python-interview-questions&quot;&gt;link5&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;</content><author><name>Sankarshan Mridha</name></author><summary type="html">Question Source</summary></entry><entry><title type="html">Blog 103: ML Puzzle</title><link href="http://localhost:4000/blog/2019/07/28/blog_103_ML_puzzle" rel="alternate" type="text/html" title="Blog 103: ML Puzzle" /><published>2019-07-28T00:11:31+05:30</published><updated>2019-07-28T00:11:31+05:30</updated><id>http://localhost:4000/blog/2019/07/28/blog_103_ML_puzzle</id><content type="html" xml:base="http://localhost:4000/blog/2019/07/28/blog_103_ML_puzzle">&lt;h1 id=&quot;interesting-ml-puzzle-curated-over-internet&quot;&gt;Interesting ML Puzzle (curated over internet)&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;[ML Questions] Overfitting&lt;/strong&gt;: I am training a Random Forest on 1 million ($10^6$) points having 10000 ($10^4$) dimensions. I have already trained 5000 trees and want to train another 10000. Should I go ahead and train 15000 trees or do I have danger of overfitting? [&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:6498386172857933824/&quot;&gt;Qlink&lt;/a&gt;]&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;[Ans]&lt;/strong&gt;: In his 1999 seminal paper (Theorem 1.2), Breiman already mathematically proved that Random Forest does not overfit on the number of trees. This is due to the &lt;code class=&quot;highlighter-rouge&quot;&gt;law of large numbers&lt;/code&gt;. The error will always converge with more number of trees. [&lt;a href=&quot;https://www.stat.berkeley.edu/~breiman/random-forests.pdf&quot;&gt;paper link&lt;/a&gt;]&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;[Q]&lt;/strong&gt;:  How many features (apprx) are used at a time to train single tree?
        &lt;ul&gt;
          &lt;li&gt;$\log_e(K)$, where K is the dimension of the input data, i.e number of features.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;[Observation]&lt;/strong&gt;: With more trees the training and prediction time will be higher. Why?
        &lt;ul&gt;
          &lt;li&gt;Prediction time is usually a big factor. Just increasing the number of trees will  soon become prohibitive. In my case, assume every tree was balanced, then I will need ($\log_2(10^6)$) 20 iteration on average to reach to leaf. With 10000 trees, I am doing 20 X 10000 threshold evaluations at prediction time. Very expensive.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;[ML Questions] Bias and Variance&lt;/strong&gt;: Assume that I am training a random forest, where each tree is grown fully. The training data consists of N samples. To train a tree I create a subset of size N by sampling with replacement from training data.
The original training data is composed of F features, and for determining the split at any node in a tree, I am using $\log_e(F)$ features as candidates.
Assume that the trees are grown fully.
If we consider the individual trees, will they have &lt;code class=&quot;highlighter-rouge&quot;&gt;high variance, high bias&lt;/code&gt;, or nothing can be said about individual trees?
When we combine the trees, are we trying to correct the variance or the bias or both? [&lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:6497416642216198144/&quot;&gt;Qlink&lt;/a&gt;]&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;[&lt;a href=&quot;https://towardsdatascience.com/random-forests-and-the-bias-variance-tradeoff-3b77fee339b4&quot;&gt;effect of different hyper-parameter&lt;/a&gt;]&lt;/li&gt;
      &lt;li&gt;A single decision tree if fully grown that means you are increasing the complexity of the tree. This implies the bias is reduced but variance increased.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;[My Ans]&lt;/strong&gt;: here even if each tree is grown fully, they are grown over a dataset of same size of original data, but sampled with replacement. So they haven’t seen the full data, so they shouldn’t overfit i.e variance will be low. But w.r.t each sampled dataset each tree is fully grown, so they have high variance and low bias (fully grown). By combining the trees we are correcting the variance.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;</content><author><name>Sankarshan Mridha</name></author><summary type="html">Interesting ML Puzzle (curated over internet)</summary></entry><entry><title type="html">Blog 200: Deep Learning: Natural Language Processing</title><link href="http://localhost:4000/blog/2019/07/28/blog_200_DL_NLP" rel="alternate" type="text/html" title="Blog 200: Deep Learning: Natural Language Processing" /><published>2019-07-28T00:11:31+05:30</published><updated>2019-07-28T00:11:31+05:30</updated><id>http://localhost:4000/blog/2019/07/28/blog_200_DL_NLP</id><content type="html" xml:base="http://localhost:4000/blog/2019/07/28/blog_200_DL_NLP">&lt;h1 id=&quot;quick-refresher-neural-network&quot;&gt;&lt;strong&gt;Quick Refresher: Neural Network&lt;/strong&gt;&lt;/h1&gt;

&lt;h2 id=&quot;deep-feed-forward-network&quot;&gt;&lt;strong&gt;Deep Feed Forward Network&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;These models are called Feed Forward Network because the information flows from $x$, through the intermediate computations used to define $f$ and finally to the output $y$.&lt;/li&gt;
  &lt;li&gt;When FFN are extended to include feedback loops, they are called recurrent neural network.&lt;/li&gt;
  &lt;li&gt;The dimensionality of the hidden layer decides the width of the model.&lt;/li&gt;
  &lt;li&gt;The strategy of deep learning is to learn $\phi$. In this approach, we have a model $y=f(x;\theta,w)=\phi(x;\theta)^Tx$. We now have $\theta$, that we use to learn $\phi$ from a broad class of functions and parameter $w$ that map from $\phi(x)$ to the desired output. This is an example of deep FNN and $\phi$ denotes the hidden layer.&lt;/li&gt;
  &lt;li&gt;This approach gives up the convexity of the training problem but the benefits out-weight the harm.&lt;/li&gt;
  &lt;li&gt;FFN has introduced the concept of &lt;strong&gt;hidden layer&lt;/strong&gt;, and this requires us to choose the &lt;strong&gt;activation function&lt;/strong&gt; to compute the the hidden layer values.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;gradient-based-learning&quot;&gt;&lt;strong&gt;Gradient-Based Learning&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The biggest difference between linear models and neural network is that the non-linearity of neural network causes the most interesting loss function to become &lt;strong&gt;non-convex&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Neural network models are trained by using iterative, gradient based optimizers, that merely drive the cost function to a very low value.&lt;/li&gt;
  &lt;li&gt;Where as linear equation solvers used to train linear regression models or the convex optimization algorithms, with &lt;strong&gt;global convergence guarantees&lt;/strong&gt; used to train logistic regression or SVMs.&lt;/li&gt;
  &lt;li&gt;SGD applied to non-convex loss function has &lt;strong&gt;no convergence guarantees&lt;/strong&gt;, and is sensitive to the initial parameter values.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;short-summary-of-cnn&quot;&gt;&lt;strong&gt;Short Summary of CNN&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;The interest in CNN started with AlexNet in 2012 and it has grown exponentially ever since.&lt;/li&gt;
  &lt;li&gt;The main advantage of CNN compared to its predecessors is that it automatically detects the important features without any human supervision.&lt;/li&gt;
  &lt;li&gt;CNN is also computationally efficient. It uses special convolution and pooling operations and performs parameter sharing. This enables CNN models to run on any device, making them universally attractive.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;architecture&quot;&gt;&lt;strong&gt;Architecture&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;All CNN models follow a similar architecture, as shown in the figure below.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*uulvWMFJMidBfbH9tMVNTw@2x.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;convolution&quot;&gt;&lt;strong&gt;Convolution&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;We perform a series &lt;code class=&quot;highlighter-rouge&quot;&gt;convolution&lt;/code&gt; + &lt;code class=&quot;highlighter-rouge&quot;&gt;pooling operations&lt;/code&gt;, followed by a number of fully connected layers.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*cTEp-IvCCUYPTT0QpE3Gjg@2x.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*ghaknijNGolaA3DpjvDxfQ@2x.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The green area where the convolution operation takes place is called the &lt;code class=&quot;highlighter-rouge&quot;&gt;receptive field&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; We perform multiple convolutions on an input, each using a different filter and resulting in a distinct feature map. We then stack all these feature maps together and that becomes the final output of the convolution layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*45GSvnTvpHV0oiRr78dBiw@2x.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;non-linearity&quot;&gt;&lt;strong&gt;Non-Linearity&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;We again pass the result of the convolution operation through &lt;code class=&quot;highlighter-rouge&quot;&gt;relu&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;Leaky-Relu&lt;/code&gt; activation function. So the values in the final feature maps are not actually the sums, but the relu function applied to them.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;stride-and-padding&quot;&gt;&lt;strong&gt;Stride and Padding&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Stride specifies how much we move the convolution filter at each step. By default the value is 1&lt;/li&gt;
  &lt;li&gt;We see that the size of the feature map is smaller than the input, because the convolution filter needs to be contained in the input. If we want to maintain the same dimensionality, we can use padding to surround the input with zeros.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*W2D564Gkad9lj3_6t9I2PA@2x.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The gray area around the input is the padding. We either pad with zeros or the values on the edge.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;pooling&quot;&gt;&lt;strong&gt;Pooling&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;After a convolution operation we usually perform pooling to &lt;code class=&quot;highlighter-rouge&quot;&gt;reduce the dimensionality&lt;/code&gt;. This enables us to reduce the number of parameters, which both shortens the training time and combats overfitting. Pooling layers downsample each feature map independently, reducing the height and width, keeping the depth intact.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*ReZNSf_Yr7Q1nqegGirsMQ@2x.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In CNN architectures, pooling is typically performed with 2x2 windows, stride 2 and no padding. While convolution is done with 3x3 windows, stride 1 and with padding.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;hyperparameters&quot;&gt;&lt;strong&gt;Hyperparameters&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Filter size: we typically use &lt;code class=&quot;highlighter-rouge&quot;&gt;3x3&lt;/code&gt; filters, but &lt;code class=&quot;highlighter-rouge&quot;&gt;5x5&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;7x7&lt;/code&gt; are also used depending on the application.&lt;/li&gt;
  &lt;li&gt;Filter count: this is the most variable parameter, it’s a power of two anywhere between 32 and 1024. Using more filters results in a more powerful model, but we risk overfitting due to increased parameter count.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Stride&lt;/code&gt;: we keep it at the default value 1.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Padding&lt;/code&gt;: we usually use padding.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;fully-connected&quot;&gt;&lt;strong&gt;Fully Connected&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;After the convolution + pooling layers we add a couple of fully connected layers to wrap up the CNN architecture.&lt;/li&gt;
  &lt;li&gt;Remember that the output of both convolution and pooling layers are 3D volumes, but a fully connected layer expects a 1D vector of numbers. So we flatten the output of the final pooling layer to a vector and that becomes the input to the fully connected layer.&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;training&quot;&gt;&lt;strong&gt;Training:&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;You do not fix the filter coefficients, rather you learn them over several iterations of training. The initialization can be random, or can be based on pre-trained model weights (such as those from the ‘modelzoo’ in github repos for popular models such as Alexnet, VGG, etc)&lt;/li&gt;
  &lt;li&gt;Once you decide the filter size, we randomly initialize the weight of the filter and allow back propagation algorithm to learn weights automatically.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;References:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2&quot;&gt;TDS: Applied-deep-learning-part-4&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html?source=post_page&quot;&gt;Adesh Pande&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;backpropagation-in-convolutional-neural-networks&quot;&gt;&lt;strong&gt;Backpropagation In Convolutional Neural Networks&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/?source=post_page&quot;&gt;Imp_Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vgg-model&quot;&gt;&lt;strong&gt;VGG Model&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Let’s now take a look at an example state-of-the art CNN model from 2014. VGG is a convolutional neural network from researchers at Oxford’s Visual Geometry Group, hence the name VGG. It was the runner up of the ImageNet classification challenge with &lt;code class=&quot;highlighter-rouge&quot;&gt;7.3%&lt;/code&gt; error rate.&lt;/p&gt;

&lt;p&gt;Among the best performing CNN models, VGG is remarkable for its simplicity. Let’s take a look at its architecture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*U8uoGoZDs8nwzQE3tOhfkw@2x.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It only uses 3x3 convolutions throughout the network.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; The two back to back &lt;code class=&quot;highlighter-rouge&quot;&gt;3x3&lt;/code&gt; convolutions have the effective receptive field of a &lt;code class=&quot;highlighter-rouge&quot;&gt;single 5x5&lt;/code&gt; convolution. And three stacked &lt;code class=&quot;highlighter-rouge&quot;&gt;3x3&lt;/code&gt; convolutions have the receptive field of a single 7x7 one. Here’s the visualization of two stacked 3x3 convolutions resulting in 5x5.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*YpXrr8bN5XyqOlztKPHvDw@2x.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Does that mean, if &lt;code class=&quot;highlighter-rouge&quot;&gt;n&lt;/code&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;3x3&lt;/code&gt; filters are used back to back that is equivalent to applying 1 filter of size &lt;code class=&quot;highlighter-rouge&quot;&gt;2n+1&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another advantage of stacking two convolutions instead of one is that we use &lt;strong&gt;two relu operations&lt;/strong&gt;, and &lt;strong&gt;more non-linearity&lt;/strong&gt; gives &lt;strong&gt;more power&lt;/strong&gt; to the model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The number of filters increase as we go deeper into the network. The spatial size of the feature maps decrease since we do pooling, but the depth of the volumes increase as we use more filters.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;visualizing-feature-maps&quot;&gt;&lt;strong&gt;Visualizing Feature Maps&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;VGG convolutional layers are named as follows: blockX_convY. For example the second filter in the third convolution block is called block3_conv2.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*OuxhgVj1WDDfo5UO5GIhgA@2x.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;As we go deeper into the network, the feature maps look less like the original image and more like an abstract representation of it. As you can see in block3_conv1 the cat is somewhat visible, but after that it becomes unrecognizable. The reason is that deeper feature maps encode high level concepts like “cat nose” or “dog ear” while lower level feature maps detect simple edges and shapes. That’s why deeper feature maps contain less information about the image and more about the class of the image. They still encode useful features, but they are less visually interpretable by us.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*A86wUjL-Z0SWDDI3slKqtg@2x.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;The feature maps become sparser as we go deeper, meaning the filters detect less features. It makes sense because the filters in the first layers detect simple shapes, and every image contains those. But as we go deeper we start looking for more complex stuff like “dog tail” and they don’t appear in every image. That’s why in the first figure with 8 filters per layer, we see more of the feature maps as blank as we go deeper (block4_conv1 and block5_conv1)&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Remember that each filter acts as a detector for a particular feature. The input image we generate will contain a lot of these features.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;filter-and-featuremap-visualization&quot;&gt;&lt;strong&gt;Filter and Featuremap Visualization&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/1*w41F9cu7vnvts1e06VoK0A@2x.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;As we go deeper into the network, the filters build on top of each other, and learn to encode more complex patterns. For example filter 41 in block5_conv3 seems to be a bird detector. You can see multiple heads in different orientations, because the particular location of the bird in the image is not important, as long as it appears somewhere the filter will activate. That’s why the filter tries to detect the bird head in several positions by encoding it in multiple locations in the filter.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2&quot;&gt;Very_Imp_Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-does-cnn-work--explain-the-implementation-details&quot;&gt;&lt;strong&gt;How does CNN work ? Explain the implementation details?&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/&quot;&gt;Adit Deshpande Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cs231n.github.io/&quot;&gt;cs231n&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;explain-the-back-propagation-steps-of-convolution-neural-network&quot;&gt;&lt;strong&gt;Explain the back propagation steps of Convolution Neural Network?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=i94OvYb6noo&quot;&gt;Andrej Karpathy YouTube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b&quot;&gt;Andrej Karpathy Medium&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199&quot;&gt;summary short&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cs231n.github.io/optimization-2/&quot;&gt;how to compute backprop in program&lt;/a&gt;, &lt;strong&gt;MUST read&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;short-summary-of-rnn-lstm-gru&quot;&gt;&lt;strong&gt;Short Summary of RNN, LSTM, GRU&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;RNN reads the document left to right and after every word updates the state. By the time it reaches the end of the word, the information obtained from the &lt;code class=&quot;highlighter-rouge&quot;&gt;first few&lt;/code&gt; words, are completely lost. So we need some mechanism to retain these information.&lt;/li&gt;
  &lt;li&gt;Now the most famous rchitecture is &lt;code class=&quot;highlighter-rouge&quot;&gt;LSTM&lt;/code&gt; to address the above issues of RNN. The general idea is to implement 3 primary operations &lt;code class=&quot;highlighter-rouge&quot;&gt;selective read&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;selective write&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;selective forget&lt;/code&gt;. i.e forget some old information, create new information and update the current information.&lt;/li&gt;
  &lt;li&gt;All the variations of &lt;code class=&quot;highlighter-rouge&quot;&gt;LSTM&lt;/code&gt; implemented these 3 operations with some tweak, like merging 2 operations into 1 and many more.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lstm&quot;&gt;&lt;strong&gt;LSTM:&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;In the most popular version of LSTM, in each LSTM cell these 3 operations are implemented as 3 &lt;code class=&quot;highlighter-rouge&quot;&gt;Gate&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;selective forget&lt;/code&gt; implemented by &lt;code class=&quot;highlighter-rouge&quot;&gt;forget gate&lt;/code&gt; $f_t$&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;selective read&lt;/code&gt; implemented by &lt;code class=&quot;highlighter-rouge&quot;&gt;input gate&lt;/code&gt; $i_t$&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;selective write&lt;/code&gt; implemented by &lt;code class=&quot;highlighter-rouge&quot;&gt;output gate&lt;/code&gt; $o_t$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Each LSTM cell also has 2 states
    &lt;ul&gt;
      &lt;li&gt;$h_t$: output/hidden state (similar to hidden state of RNN)&lt;/li&gt;
      &lt;li&gt;$C_t$: Cell state or &lt;code class=&quot;highlighter-rouge&quot;&gt;running state&lt;/code&gt; that acts as a memory containing a fraction of information from all the previous timestamps $t$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;gates&quot;&gt;&lt;strong&gt;Gates&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;f_t = \sigma (W_f  h_{t-1} + U_f x_t + b_f)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;i_t = \sigma (W_i  h_{t-1} + U_i x_t + b_i)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;i_t = \sigma (W_o  h_{t-1} + U_o x_t + b_o)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;$f_t$, $i_t$, $o_t$ decides the fraction of {$\in [0,1]$} forget, read and write respectively.&lt;/p&gt;

&lt;h4 id=&quot;states&quot;&gt;&lt;strong&gt;States&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\tilde{C_t} = \sigma(WH_{t-1} + Ux_t + b)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C_t}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;h_t = o_t \odot \sigma(C_t)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Combining all together:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{bmatrix} 
  i_t \\ 
  f_t \\
  o_t \\
  \tilde{C_t} 
\end{bmatrix}_{4x1}
 =
  \begin{bmatrix}
   \sigma  \\
   \sigma  \\
   \sigma  \\
   \sigma  
   \end{bmatrix}_{4x1} \odot (W_{4x2}
\begin{bmatrix}
   h_{t-1}  \\
   x_t  
   \end{bmatrix}_{2x1}
   + B_{4x1})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
W = \begin{bmatrix} 
  W_i &amp; U_i \\ 
  W_f &amp; U_f\\
  W_o &amp; U_o \\
  W_{\tilde{C_t}} &amp; U_{\tilde{C_t}} 
\end{bmatrix}_{4x2},
B = \begin{bmatrix} 
  b_{ih} + b_{ix} \\ 
  b_{fh} + b_{fx}\\
  b_{oh} + b_{ox} \\
  b_{\tilde{C_t}h} + b_{\tilde{C_t}i} 
\end{bmatrix}_{4x1} %]]&gt;&lt;/script&gt;

&lt;p&gt;Where $\tilde{C_t}$ is the new input or content which will be used to update the current cell state $C_t$ by forgetting a fraction $f_t$ of old cell state $C_{t-1}$ (&lt;code class=&quot;highlighter-rouge&quot;&gt;selective forget&lt;/code&gt;) + adding fraction $i_t$ of new cell input $\tilde{C_t}$ (&lt;code class=&quot;highlighter-rouge&quot;&gt;selective read&lt;/code&gt;). Finally $h_t$ is obtained by doing a &lt;code class=&quot;highlighter-rouge&quot;&gt;selective write&lt;/code&gt; with the fraction $o_t$ and cell state $C_t$. Note $\odot$ is called the &lt;code class=&quot;highlighter-rouge&quot;&gt;pointwise product&lt;/code&gt; or the &lt;code class=&quot;highlighter-rouge&quot;&gt;hadamard&lt;/code&gt; product.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/lstm.png&quot; alt=&quot;lstm&quot; /&gt;
&lt;a href=&quot;http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf&quot;&gt;image source, cs231n, lecture 10 page 96&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For reference check the blog of Chris Olah and the lecture of Prof. Mitesh K. However, in Chris Olah’s blog, he has concatenated $h_{t-1}$ and $x_t$, which leads to learning of fewer parameter.&lt;/li&gt;
  &lt;li&gt;In Prof.Mitesh K’s lecture, he has used notation $s_t$ for cell state.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;quick-summary&quot;&gt;&lt;strong&gt;Quick Summary&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/rnn_gru_lstm.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Reference:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf&quot;&gt;IMP, Lecture 10, Standford cs231 page-96 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Chrish Olah LSTM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=9TFnjJkfqmA&amp;amp;feature=youtu.be&quot;&gt;Lecture 15 by Prof. Mitesh K&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.echen.me/2017/05/30/exploring-lstms/&quot;&gt;Edward Chen&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-bptt-and-its-issues-with-sequence-learning-&quot;&gt;What is BPTT and it’s issues with sequence learning ?&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Total Loss:&lt;/strong&gt; In sequence learning (RNN, LSTM, GRU etc.) the total loss is simply the sum of the loss over all the time steps.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Take a pause and think that for non sequence learning, there is no concept of each time step and they have only single loss function.&lt;/li&gt;
  &lt;li&gt;In normal backpropagation, you need to get the gradient w.r.t weight and bias using chain rule and done.&lt;/li&gt;
  &lt;li&gt;Whereas, in sequence learning due to the presence of time step, you need to get the &lt;code class=&quot;highlighter-rouge&quot;&gt;loss gradient&lt;/code&gt; for each time step and then aggregate them.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So total loss for sequence learning is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(\theta) = \sum\limits‎_{t=1}^{T} L_t(\theta)&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/bptt.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the above image is a simple vanila RNN, where $t$ is from $1$ to $4$.  For back propagation we need to compute the graident w.r.t $W$, $U$, $V$.&lt;/p&gt;

&lt;p&gt;Now taking derivative w.r.t $V$ is straight forward.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\delta L(\theta)}{\delta V} = \sum\limits_{t=1}^{T} \frac{\delta L_t(\theta)}{\delta V}&lt;/script&gt;

&lt;p&gt;Probelm arrives now while taking derivative w.r.t $W$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\delta L(\theta)}{\delta W} = \sum\limits_{t=1}^{T} \frac{\delta L_t(\theta)}{\delta W}&lt;/script&gt;

&lt;p&gt;However, while applying chain rule $L_4(\theta)$ is dependent on $s_4$, $s_4$ is dependent on $W$ and $s_3$, $s_3$ is dependent on $s_2$ and so on. So while taking derivative of $s_i$ w.r.t $W$, we can’t take $s_{i-1}$ as constant. That’s the problem in such &lt;code class=&quot;highlighter-rouge&quot;&gt;ordered network&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Therefore, in such network the total derivative $\frac{\delta s_4}{\delta W}$ has 2 parts.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Explicit:&lt;/strong&gt; Treating all other input as constant&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Implicit:&lt;/strong&gt; Summing over all indirect paths from $s_4$ to $W$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/bptt_1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/bptt_2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Resource:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cse.iitm.ac.in/~miteshk/CS7015.html&quot;&gt;Lecture 14 by Prof.Mitesh K&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Xeb6OjnVn8g&amp;amp;feature=youtu.be&quot;&gt;Youtube lecture by Prof. Mitesh K&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/&quot;&gt;Blog by Danny Britz from WINDML&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vanishing-gradient-in-cnn-and-rnn&quot;&gt;Vanishing Gradient in CNN and RNN:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;When talking about RNN, the vanishing gradient problem refers to the &lt;code class=&quot;highlighter-rouge&quot;&gt;change of gradient in one RNN layer over different time steps&lt;/code&gt; (because the repeated use of the recurrent weight matrix).&lt;/li&gt;
  &lt;li&gt;On the contrary, when talking about CNN, it is about the &lt;code class=&quot;highlighter-rouge&quot;&gt;gradient change over different layers&lt;/code&gt;, and it is generally referred to as &lt;code class=&quot;highlighter-rouge&quot;&gt;“gradient decaying over layers”&lt;/code&gt;. Just for your information, the recent IndRNN model addresses the vanishing and exploding gradient problems. It is much more robust than the traditional RNN.&lt;/li&gt;
  &lt;li&gt;If you just keep on adding convolution layers to a CNN , after a point you will start facing vanishing gradient. You can experiment this using the vgg architecture. To avoid this problem and build deeper networks , most of the modern architectures uses &lt;code class=&quot;highlighter-rouge&quot;&gt;skip connections&lt;/code&gt; like in resent , inception. These modern architectures go deeper to more than 150 layers.&lt;/li&gt;
  &lt;li&gt;RNNs unfolded are deep in common application scenarios, thus prone to severer vanishing gradient problems. For example, when used in language modeling, &lt;code class=&quot;highlighter-rouge&quot;&gt;RNN depth can go as long as the longest sentence in the training corpus&lt;/code&gt;. If the model is character-based, then the depth is even larger. CNNs in typical application scenarios are &lt;code class=&quot;highlighter-rouge&quot;&gt;shallower&lt;/code&gt;, but still suffer from the same problem with gradients.&lt;/li&gt;
  &lt;li&gt;Images typically have &lt;code class=&quot;highlighter-rouge&quot;&gt;hierarchy of scales&lt;/code&gt;, so parts of the image which are far away from each other typically interact only at &lt;code class=&quot;highlighter-rouge&quot;&gt;higher order layers of the network&lt;/code&gt;.
    &lt;ul&gt;
      &lt;li&gt;For RNN parts which are far far away from each other can huge influence. e.g. If you smoke when you are young, you may have cancer 40 years later. RNN should be able to make predictions based on very long term correlations.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Deeper networks are more prone to Vanishing Gradient problem. Now RNNs are (after unfolding) very deep. So they suffer more from Vanishing Gradient problem. Here the Vanishing Gradient problem occurs at the gradient change at the same RNN layer. But CNN by default is shallow network. Here if you stack multiple CNN layer, then vanishing gradient occurs at decaying of gradient at different layer.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/Why-doesnt-CNN-suffer-from-the-vanishing-gradient-problems-of-RNN&quot;&gt;Quora&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-exploding-gradient-problem&quot;&gt;What is Exploding Gradient Problem??&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Exploding gradients are a problem where large error gradients accumulate and result in very large updates to neural network model weights during training. This has the effect of your model being unstable and unable to learn from your training data.&lt;/p&gt;

&lt;h3 id=&quot;what-are-exploding-gradients&quot;&gt;What Are Exploding Gradients?&lt;/h3&gt;

&lt;p&gt;An error gradient is the direction and magnitude calculated during the training of a neural network that is used to update the network weights in the right direction and by the right amount.&lt;/p&gt;

&lt;p&gt;In deep networks or recurrent neural networks, error gradients can accumulate during an update and result in very large gradients. These in turn result in large updates to the network weights, and in turn, an unstable network. At an extreme, the values of weights can become so large as to overflow and result in &lt;code class=&quot;highlighter-rouge&quot;&gt;NaN&lt;/code&gt; values.&lt;/p&gt;

&lt;p&gt;The explosion occurs through exponential growth by repeatedly multiplying gradients through the network layers that have values larger than $1.0$.&lt;/p&gt;

&lt;h3 id=&quot;how-do-you-know-if-you-have-exploding-gradients&quot;&gt;How do You Know if You Have Exploding Gradients?&lt;/h3&gt;

&lt;p&gt;There are some subtle signs that you may be suffering from exploding gradients during the training of your network, such as:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- The model is unable to get traction on your training data (e.g. poor loss).
- The model is unstable, resulting in large changes in loss from update to update.
- The model loss goes to NaN during training.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There are some less subtle signs that you can use to confirm that you have exploding gradients.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- The model weights quickly become very large during training.
- The model weights go to NaN values during training.
- The error gradient values are consistently above 1.0 for each node and layer during training.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;how-to-fix-exploding-or-vanisihng-gradient-in-sequence-model&quot;&gt;how to fix Exploding or Vanisihng Gradient in sequence model?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Tranucated BPTT, i.e, you will not look back more than $k$ timesteps where $k \lt t$&lt;/li&gt;
  &lt;li&gt;Gradient Clipping&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Resource:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cse.iitm.ac.in/~miteshk/CS7015.html&quot;&gt;Lecture 14 by Prof.Mitesh Khapra&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=EB1SoyivHFU&amp;amp;feature=youtu.be&quot;&gt;Youtube video of the above lecture&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-fix-exploding-gradients&quot;&gt;How to Fix Exploding Gradients?&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Re-Design the Network Model&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;In recurrent neural networks, updating across fewer prior time steps during training, called &lt;code class=&quot;highlighter-rouge&quot;&gt;truncated Backpropagation&lt;/code&gt; through time, may reduce the exploding gradient problem.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Using LSTM&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Exploding gradients can be reduced by using the Long Short-Term Memory (LSTM) memory units and perhaps related gated-type neuron structures. Adopting LSTM memory units is a new best practice for recurrent neural networks for sequence prediction.&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;Use Gradient Clipping&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;Exploding gradients can still occur in very deep Multilayer Perceptron networks with a large batch size and LSTMs with very long input sequence lengths.&lt;/li&gt;
  &lt;li&gt;If exploding gradients are still occurring, you can check for and limit the size of gradients during the training of your network. This is called &lt;code class=&quot;highlighter-rouge&quot;&gt;gradient clipping&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;clipping gradients if their norm exceeds a given threshold&lt;/code&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use Weight Regularization&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&quot;https://machinelearningmastery.com/exploding-gradients-in-neural-networks/&quot;&gt;Ref: MachinelearningMastery&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;how-does-lstm-help-prevent-the-vanishing-and-exploding-gradient-problem-in-a-recurrent-neural-network&quot;&gt;How does LSTM help prevent the vanishing (and exploding) gradient problem in a recurrent neural network?&lt;/h3&gt;

&lt;p&gt;There are two factors that affect the magnitude of gradients - the weights and the activation functions (or more precisely, their derivatives) that the gradient passes through.&lt;/p&gt;

&lt;p&gt;If either of these factors is smaller than 1, then the gradients may vanish in time; if larger than 1, then exploding might happen. For example, the $tanh$ derivative is $&amp;lt;1$ for all inputs except 0; sigmoid is even worse and is always $≤ 0.25$.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In the recurrency of the LSTM the activation function is the identity function with a derivative of 1.0. So, the backpropagated gradient neither vanishes or explodes when passing through, but remains constant.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The effective weight of the recurrency is equal to the forget gate activation. So, if the forget gate is on (activation close to 1.0), then the gradient does not vanish. Since the forget gate activation is never &amp;gt;1.0, the gradient can’t explode either.
So that’s why LSTM is so good at learning long range dependencies.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://qph.fs.quoracdn.net/main-qimg-4359968f2dd46aaa1cf862d60724b453&quot; alt=&quot;drawing&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Solution 1)&lt;/strong&gt; Use activation functions which have ‘good’ gradient values. Not ZERO over a reasonable amount of range Not that small, Not that big. e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;ReLu&lt;/code&gt;. &lt;a href=&quot;https://www.quora.com/What-is-the-vanishing-gradient-problem/answer/Ottokar-Tilk&quot;&gt;reference&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution 2)&lt;/strong&gt; Use &lt;code class=&quot;highlighter-rouge&quot;&gt;gating(pass or block, or in other words, 1 or 0) function&lt;/code&gt;, not activation function. And train the ‘combination’ of all those gates. Doing this, no matter how ‘deep’ your network is, or how ‘long’ the input sequence is, the network can remember those values, as long as those gates are all 1 along the path. &amp;lt;- &lt;code class=&quot;highlighter-rouge&quot;&gt;This is how LSTM/GRU did the job&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The vanishing (and exploding) gradient problem is caused by the repeated use of the recurrent weight matrix in RNN. In LSTM, the recurrent weight matrix is replaced by the &lt;code class=&quot;highlighter-rouge&quot;&gt;identity function&lt;/code&gt; in the &lt;strong&gt;carousel&lt;/strong&gt; and controlled by the forget gate. So ignoring the forget gate (which can always be open), the repeated use of the identity function would not introduce the vanishing (and exploding) gradient.&lt;/p&gt;

&lt;p&gt;The recent &lt;strong&gt;IndRNN&lt;/strong&gt;(Building A Longer and Deeper RNN) model also addresses the gradient vanishing and exploding problem. It uses learnable recurrent weights but regulated in a way to avoid the gradient vanishing and exploding problem. Compared with LSTM, &lt;code class=&quot;highlighter-rouge&quot;&gt;it is much faster and can be stacked into deep models. Better performance is shown by IndRNN over the traditional RNN and LSTM&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1803.04831&quot;&gt;IndRNN&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Given that f, the forget gate, is the rate at which you want the neural network to forget its past memory, the error signal is propagated perfectly to the previous time step. In many LSTM papers, this is referred to as the &lt;strong&gt;linear carousel&lt;/strong&gt; that prevents the vanish of gradient through many time steps.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.quora.com/How-does-LSTM-help-prevent-the-vanishing-and-exploding-gradient-problem-in-a-recurrent-neural-network&quot;&gt;Quora Answer&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;why-can-rnns-with-lstm-units-also-suffer-from-exploding-gradients&quot;&gt;Why can RNNs with LSTM units also suffer from “exploding gradients”?&lt;/h3&gt;

&lt;p&gt;In the paper Sequence to &lt;a href=&quot;https://arxiv.org/abs/1409.3215&quot;&gt;Sequence Learning with Neural Networks (by Ilya Sutskever, Oriol Vinyals, Quoc V. Le)&lt;/a&gt;, and, in that paper, section “3.4 Training details”, it is stated &lt;code class=&quot;highlighter-rouge&quot;&gt;Although LSTMs tend to not suffer from the vanishing gradient problem, they can have exploding gradients.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; LSTM decouples cell state (typically denoted by &lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt;) and hidden layer/output (typically denoted by &lt;code class=&quot;highlighter-rouge&quot;&gt;h&lt;/code&gt;), and only do additive updates to &lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt;, which makes memories in &lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt; more stable. Thus the gradient flows through &lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt; is kept and hard to vanish (therefore the overall gradient is hard to vanish). However, other paths may cause gradient explosion.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Detailed Answer:&lt;/strong&gt; &lt;a href=&quot;https://stats.stackexchange.com/questions/320919/why-can-rnns-with-lstm-units-also-suffer-from-exploding-gradients/339129#339129&quot;&gt;StackExchange&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;More Detailed Answer:&lt;/strong&gt; &lt;a href=&quot;https://arxiv.org/abs/1503.04069&quot;&gt;LSTM: A search space odyssey&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;gating-in-deep-learning&quot;&gt;Gating in Deep learning&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Gating meaning:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Although it can be a complex process and involve multiple gates or regions of interest, the process of &lt;strong&gt;gating&lt;/strong&gt; is simply &lt;code class=&quot;highlighter-rouge&quot;&gt;selecting an area on the scatter plot generated during the flow experiment that decides which cells you continue to analyze and which cells you don’t.&lt;/code&gt; &lt;a href=&quot;https://bitesizebio.com/21596/an-introduction-to-gating-in-flow-cytometry/&quot;&gt;reference&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; the process by which a channel in a cell membrane opens or closes. &lt;a href=&quot;https://www.dictionary.com/browse/gating&quot;&gt;Dictionary Meaning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In LSTM, gating decides how much cell state information, from previous step, to flow in the next step or not. Thus gating function is different from the Activation function and helps in case of Vanishing Gradient problem.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/gating-and-depth-in-neural-networks-b2c66ae74c45&quot;&gt;Gating and Depth in the Neural Network&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;why-using-sigmoid-and-tanh-as-the-activation-functions-in-lstm-or-rnn-is-not-problematic-but-this-is-not-the-case-in-other-neural-nets&quot;&gt;Why using sigmoid and tanh as the activation functions in LSTM or RNN is not problematic but this is not the case in other neural nets?&lt;/h3&gt;

&lt;p&gt;Commonly, $sigmoid$ and $tanh$ activation functions are problematic (gradient vanishing) in RNN especially when the training algorithm $BPTT$ is utilized. In LSTM, sigmoid and tanh are used to build the $gates$. Because of these gates, the gradient vanishing problem does not exist.&lt;/p&gt;

&lt;h2 id=&quot;whats-the-difference-between-hidden-and-output-in-pytorch-lstm&quot;&gt;What’s the difference between “hidden” and “output” in PyTorch LSTM?&lt;/h2&gt;

&lt;p&gt;According to Pytorch documentation&lt;/p&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
Outputs: output, (h_n, c_n)

- output (seq_len, batch, hidden_size * num_directions): tensor containing the output features (h_t) from the last layer of the RNN, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.
- h_n (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t=seq_len
- c_n (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t=seq_len
&quot;&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;How to interpret it ?&lt;/p&gt;

&lt;p&gt;output comprises all the hidden states in the last layer (“last” depth-wise, not time-wise). (h_n, c_n) comprises the hidden states after the last timestep, t = n, so you could potentially feed them into another LSTM.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/SjnTl.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The batch dimension is not included.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/48302810/whats-the-difference-between-hidden-and-output-in-pytorch-lstm&quot;&gt;source_stackOverflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;understand-forward-and-backward-with-simple-perceptron&quot;&gt;Understand forward and backward with simple Perceptron&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Check the [&lt;a href=&quot;https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/&quot;&gt;blog&lt;/a&gt;] and it’s implementation and result in the following [&lt;a href=&quot;https://github.com/subhendukhatuya/deeplearning_postmortem/blob/master/Unfolding_Perceptron_Forward_backward.ipynb&quot;&gt;notebook&lt;/a&gt;]&lt;/li&gt;
  &lt;li&gt;Give attention to the intermediate values in the blog and in the code side by side and check they are equal.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tell-about-different-loss-function-when-to-use-them&quot;&gt;Tell about different loss function, when to use them?&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;Importantly, the choice of loss function is directly related to the activation function used in the output layer of your neural network. These two design elements are connected.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;The choice of cost function is tightly coupled with the choice of output unit. Most of the time, we simply use the cross-entropy between the data distribution and the model distribution. The choice of how to represent the output then determines the form of the cross-entropy function.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Regression Problem&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A problem where you predict a real-value quantity.&lt;/p&gt;

&lt;p&gt;Case 1:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Output Layer Configuration:&lt;/code&gt; One node with a linear activation unit.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Loss Function:&lt;/code&gt; Mean Squared Error (MSE).&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\mathcal{L}}=\frac{1}{n}\sum_{i=1}^{n}(y^{(i)}-\hat{y}^{(i)})^{2}&lt;/script&gt;

&lt;p&gt;Case 2:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Output Layer Configuration:&lt;/code&gt; One node with a linear activation unit.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Loss Function:&lt;/code&gt; Mean Squared Logarithmic Error (MSLE)&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\boldsymbol{\mathcal{L}}=\frac{1}{n}\sum_{i=1}^{n}\big(\log(y^{(i)}+1)-\log(\hat{y}^{(i)}+1)\big)^{2}&lt;/script&gt;

&lt;h3 id=&quot;when-to-use-case-2-&quot;&gt;When to use case 2 ?&lt;/h3&gt;

&lt;p&gt;Mean Squared Logarithmic Error (MSLE) loss function is a variant of MSE, which is defined as above.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;It is usually used when the &lt;code class=&quot;highlighter-rouge&quot;&gt;true values&lt;/code&gt; and the &lt;code class=&quot;highlighter-rouge&quot;&gt;predicted values&lt;/code&gt; are very big. And therefore, their difference are also very big. In that situation, you generally don’t want to penalizes huge difference between true and predicted die to their high value range.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;One use case say &lt;code class=&quot;highlighter-rouge&quot;&gt;sales price&lt;/code&gt; prediction. Here the true value range can be very &lt;code class=&quot;highlighter-rouge&quot;&gt;skewed&lt;/code&gt; which affects the prediction as well. Here regular &lt;code class=&quot;highlighter-rouge&quot;&gt;MSE&lt;/code&gt; will heavily penalizes the big differences. But in this scenario it’s wrong because here by default the value ranges are very big i.e. skewed. In this scenario one good approach is take the &lt;code class=&quot;highlighter-rouge&quot;&gt;log(true_value)&lt;/code&gt; and predict that, and by default you have to use the &lt;code class=&quot;highlighter-rouge&quot;&gt;MSLE&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Another usefulness of applying &lt;code class=&quot;highlighter-rouge&quot;&gt;log&lt;/code&gt; is it fixes the skeness and brings it more close to normal distribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Binary Classification Problem&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Output Layer Configuration:&lt;/code&gt; One node with a sigmoid activation unit.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Loss Function:&lt;/code&gt; Cross-Entropy, also referred to as Logarithmic loss.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Multi-Class Classification Problem&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Output Layer Configuration:&lt;/code&gt; One node for each class using the softmax activation function.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Loss Function:&lt;/code&gt; Cross-Entropy, also referred to as Logarithmic loss.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In binary classification, where the number of classes &lt;code class=&quot;highlighter-rouge&quot;&gt;M&lt;/code&gt; equals 2, cross-entropy can be calculated as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;-{(y\log(p) + (1 - y)\log(1 - p))}&lt;/script&gt;

&lt;p&gt;If $M\gt2$ (i.e. multiclass classification), we calculate a separate loss for each class label per observation and sum the result.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;-\sum_{c=1}^My_{o,c}\log(p_{o,c})&lt;/script&gt;

&lt;p&gt;Summary&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/loss_function.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cse.iitm.ac.in/~miteshk/CS7015.html&quot;&gt;Image Source: From Prof. Mitesh Khapra, Lecture 4, [slide page 95]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/&quot;&gt;Source MachineLearningMastery&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/lavanyashukla01/how-i-made-top-0-3-on-a-kaggle-competition&quot;&gt;Source: How_to_be_on_top_0p3_percent_kaggle_competiion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://isaacchanghau.github.io/post/loss_functions/&quot;&gt;Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;(https://www.cse.iitm.ac.in/~miteshk/CS7015.html)&quot;&gt;Deep Learning Lecture 4 by Prof. Mitesh khapra&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-does-attention-works-&quot;&gt;How does Attention Works ?&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;
    &lt;p&gt;The attention-mechanism looks at an input sequence and decides at each step which other parts of the sequence are important.&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;When reading a text, you always focus on the word you read but at the same time your mind still holds the important keywords of the text in memory in order to provide context.&lt;/p&gt;

&lt;p&gt;An attention-mechanism works similarly for a given sequence. For our example with the human Encoder and Decoder, imagine that instead of only writing down the translation of the sentence in the imaginary language, the Encoder also writes down keywords that are important to the semantics of the sentence, and gives them to the Decoder in addition to the regular translation. Those new keywords make the translation much easier for the Decoder because it knows what parts of the sentence are important and which key terms give the sentence context.&lt;/p&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nlp.stanford.edu/pubs/emnlp15_attn.pdf&quot;&gt;Paper: Effective Approaches to Attention-based Neural Machine Translation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&quot;&gt;Visualizing A Neural Machine Translation Model by Jay Alammar&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04&quot;&gt;Important Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@joealato/attention-in-nlp-734c6fa9d983&quot;&gt;Imp: Attention in NLP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/&quot;&gt;Imp: Attention and Memory in NLP&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-transformer-and-its-pros-and-cons&quot;&gt;What is Transformer and it’s pros and cons?&lt;/h2&gt;

&lt;p&gt;From the Author:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;In paper “Attention Is All You Need”, we introduce the Transformer, a novel neural network architecture based on a self-attention mechanism that we believe to be particularly well suited for language understanding.  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Natural Language Understanding (NLU):  language modeling, machine translation and question answering&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Transformer outperforms both recurrent and convolutional models on academic English to German and English to French translation benchmarks.&lt;/li&gt;
  &lt;li&gt;On top of higher translation quality, the Transformer requires less computation to train and is a much better fit for modern machine learning hardware, speeding up training by up to an order of magnitude.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The paper ‘Attention Is All You Need’ describes transformers and what is called a sequence-to-sequence architecture. Sequence-to-Sequence (or Seq2Seq) is a neural net that transforms a given sequence of elements, such as the sequence of words in a sentence, into another sequence.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn-images-1.medium.com/max/800/1*BHzGVskWGS_3jEcYYi6miQ.png&quot; alt=&quot;transformer&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;One interesting point is, even if it’s used for seq2seq generation, but there is no &lt;code class=&quot;highlighter-rouge&quot;&gt;recurrence&lt;/code&gt; part inside the model like the  &lt;code class=&quot;highlighter-rouge&quot;&gt;vanila rnn&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;lstm&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;So one slight but important part of the model is the positional encoding of the different words. Since we have no recurrent networks that can remember how sequences are fed into a model, we need to somehow give every word/part in our sequence a relative position since a sequence depends on the order of its elements. These positions are added to the embedded representation (n-dimensional vector) of each word.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Faster learning. More GPU efficient unlike the &lt;code class=&quot;highlighter-rouge&quot;&gt;vanila rnn&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;References:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Paper: Attention is all you need&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;&gt;Google AI Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04&quot;&gt;Important Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;Important: The Illustrated Transformer by Jay Alammar&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-lstm-give-its-equation-pros-and-cons&quot;&gt;What is LSTM, give it’s equation, pros and cons?&lt;/h2&gt;

&lt;h2 id=&quot;how-does-word2vec-work&quot;&gt;How does &lt;code class=&quot;highlighter-rouge&quot;&gt;word2vec&lt;/code&gt; work?&lt;/h2&gt;

&lt;p&gt;Reference:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://jalammar.github.io/illustrated-word2vec/&quot;&gt;The Illustrated Word2vec by Jay Alammar&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.infoq.com/presentations/nlp-word-embedding/&quot;&gt;Intuition &amp;amp; Use-Cases of Embeddings in NLP &amp;amp; beyond, ,talk by Jay Alammar&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-transfer-learning-bert-elmo-umlfit-&quot;&gt;What is Transfer Learning, BERT, Elmo, UmlFit ?&lt;/h2&gt;

&lt;p&gt;Reference:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://jalammar.github.io/illustrated-bert/&quot;&gt;The Illustrated BERT, ELMo, and co. by Jay Alammar&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-model-perplexity&quot;&gt;What is model Perplexity?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In information theory, perplexity is a measurement of how well a probability distribution or probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample.  [&lt;a href=&quot;https://en.wikipedia.org/wiki/Perplexity&quot;&gt;wiki&lt;/a&gt;]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In natural language processing, perplexity is a way of evaluating language models. A language model is a probability distribution over entire sentences or texts.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reference:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3&quot;&gt;Blog: Towardsdatascience&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-bleu-score&quot;&gt;What is BLEU score?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;BLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation of text to one or more reference translations.&lt;/li&gt;
  &lt;li&gt;Although developed for translation, it can be used to evaluate text generated for a suite of natural language processing tasks.&lt;/li&gt;
  &lt;li&gt;BLEU uses a modified form of precision to compare a candidate translation against multiple reference translations. The metric modifies simple precision since machine translation systems have been known to generate more words than are in a reference text.&lt;/li&gt;
  &lt;li&gt;
    &lt;blockquote&gt;
      &lt;blockquote&gt;
        &lt;p&gt;The primary programming task for a BLEU implementor is to compare n-grams of the candidate with the n-grams of the reference translation and count the number of matches. These matches are position-independent. The more the matches, the better the candidate translation is.&lt;/p&gt;
      &lt;/blockquote&gt;
    &lt;/blockquote&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reference:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/calculate-bleu-score-for-text-python/&quot;&gt;Blog by machinelearningmastery&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/P02-1040.pdf&quot;&gt;Paper: BLEU: a Method for Automatic Evaluation of Machine Translation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tell-about-different-optimizer-adam-rmsprop-and-their-pros-and-cons&quot;&gt;Tell about different optimizer Adam, Rmsprop and their pros and cons?&lt;/h2&gt;

&lt;p&gt;Typically when one sets their learning rate and trains the model, one would only wait for the learning rate to decrease over time and for the model to eventually converge.&lt;/p&gt;

&lt;p&gt;However, as the gradient reaches a &lt;code class=&quot;highlighter-rouge&quot;&gt;plateau&lt;/code&gt;, the training loss becomes harder to improve. Dauphin et al argue that the difficulty in minimizing the loss arises from &lt;code class=&quot;highlighter-rouge&quot;&gt;saddle points&lt;/code&gt; rather than poor local minima.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://miro.medium.com/max/700/0*Q_ZjKKXa9mTShbpV.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A saddle point in the error surface. A saddle point is a point where derivatives of the function become zero but the point is not a local extremum on all axes.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Batch gradient descent&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta = \theta - \eta \cdot \nabla_\theta J( \theta)&lt;/script&gt;

&lt;p&gt;Batch gradient descent also doesn’t allow us to update our model online, i.e. with new examples on-the-fly.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Stochastic gradient descent&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i)}; y^{(i)})&lt;/script&gt;

&lt;p&gt;Batch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online.&lt;/p&gt;

&lt;p&gt;While batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD’s fluctuation, on the one hand, enables it to jump to new and potentially better local minima.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Mini-batch gradient descent&lt;/strong&gt;: Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of n
training examples.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta = \theta - \eta \cdot \nabla_\theta J( \theta; x^{(i:i+n)}; y^{(i:i+n)})&lt;/script&gt;

&lt;p&gt;Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Momentum:&lt;/strong&gt; SGD oscillates across the slopes of the &lt;code class=&quot;highlighter-rouge&quot;&gt;ravine&lt;/code&gt; while only making hesitant progress along the bottom towards the local optimum.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.researchgate.net/profile/Giorgio_Roffo/publication/317277576/figure/fig6/AS:500357433434112@1496305916279/6-LEFT-shows-a-long-shallow-ravine-leading-to-the-optimum-and-steep-walls-on-the.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.researchgate.net/profile/Giorgio_Roffo&quot;&gt;Image Author: Giorgio Roffo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;LEFT: shows a &lt;code class=&quot;highlighter-rouge&quot;&gt;long shallow ravine&lt;/code&gt; leading to the optimum and steep walls on the sides. Standard SGD will tend to oscillate across the narrow ravine. RIGHT: Momentum is one method for pushing the objective more quickly along the shallow ravine.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta) \\ 
\theta = \theta - v_t&lt;/script&gt;

&lt;p&gt;The momentum term $\gamma$ is usually set to &lt;code class=&quot;highlighter-rouge&quot;&gt;0.9&lt;/code&gt; or a similar value.&lt;/p&gt;

&lt;p&gt;Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance, i.e. $\gamma \lt 1$). The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Nesterov accelerated gradient (NAG):&lt;/strong&gt; However, a ball that rolls down a hill, blindly following the slope, is highly unsatisfactory. We’d like to have a smarter ball, a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.
    &lt;ul&gt;
      &lt;li&gt;Nesterov accelerated gradient (NAG) [6] is a way to give our momentum term this kind of prescience.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;v_t = \gamma v_{t-1} + \eta \nabla_\theta J( \theta - \gamma v_{t-1} ) \\ \theta = \theta - v_t&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All previous methods use the same learning rate for each of the parameter. Now we want different learning rate for different parameters. Here we go:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Adagrad:&lt;/strong&gt; It adapts the learning rate to the parameters, performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features. For this reason, it is well-suited for dealing with &lt;code class=&quot;highlighter-rouge&quot;&gt;sparse data&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;g_{t, i} = \nabla_\theta J( \theta_{t, i} )&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\theta_{t+1, i} = \theta_{t, i} - \dfrac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t, i}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;$G_{t} \in \mathbb{R}^{d \times d}$ here is a diagonal matrix where each diagonal element $i,i$ is the sum of the squares of the gradients w.r.t. $\theta_i$ up to time step $t$, while $\epsilon$ is a smoothing term that avoids division by zero.&lt;/p&gt;

&lt;p&gt;As $G_t$ contains the sum of the squares of the past gradients w.r.t. to all parameters $\theta$ along its diagonal, we can now vectorize our implementation by performing a matrix-vector product $\odot$ between $G_t$ and $g_t$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{G_{t} + \epsilon}} \odot g_{t}&lt;/script&gt;

&lt;p&gt;One of Adagrad’s main benefits is that it eliminates the need to manually tune the learning rate. Most implementations use a default value of 0.01 and leave it at that.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Adadelta:&lt;/strong&gt; Adadelta is an extension of Adagrad that seeks to reduce its aggressive, monotonically decreasing learning rate.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;RMSprop&lt;/strong&gt;: It is an unpublished, adaptive learning rate method proposed by Geoff Hinton in Lecture 6e of his Coursera Class.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Adam:&lt;/strong&gt; Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients $v_t$
like Adadelta and RMSprop, Adam also keeps an exponentially decaying average of past gradients $m_t$, similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients $m_t$ and $v_t$ respectively as follows:&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\ 
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2&lt;/script&gt;

&lt;p&gt;As mt and vt are initialized as vectors of 0’s, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small.&lt;/p&gt;

&lt;p&gt;They counteract these biases by computing bias-corrected first and second moment estimates:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{m}_t = \dfrac{m_t}{1 - \beta^t_1} \\ 
\hat{v}_t = \dfrac{v_t}{1 - \beta^t_2}&lt;/script&gt;

&lt;p&gt;They then use these to update the parameters just as we have seen in Adadelta and RMSprop, which yields the Adam update rule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t&lt;/script&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/11681225/49325458-fc785480-f585-11e8-8d2a-9012d6024c6e.gif&quot; width=&quot;500&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.stack.imgur.com/gjDzm.gif&quot; width=&quot;500&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://ruder.io/optimizing-gradient-descent/&quot;&gt;Sebastian Ruder&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tell-about-different-loss-function-and-their-pros-and-cons&quot;&gt;Tell about different loss function and their pros and cons?&lt;/h2&gt;

&lt;h3 id=&quot;regression-loss&quot;&gt;REGRESSION LOSS&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Mean Squared Error Loss&lt;/strong&gt;: The Mean Squared Error, or MSE, loss is the default loss to use for &lt;code class=&quot;highlighter-rouge&quot;&gt;regression problems&lt;/code&gt;.
Mathematically, it is the preferred loss function under the inference framework of maximum likelihood &lt;em&gt;if the distribution of the target variable is Gaussian&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mean Squared Logarithmic Error Loss:&lt;/strong&gt; There may be regression problems in which the target value has a spread of values and when predicting a large value, you may not want to punish a model as heavily as mean squared error. Instead, you can first calculate the natural logarithm of each of the predicted values, then calculate the mean squared error. This is called the Mean Squared Logarithmic Error loss, or MSLE for short. It has the effect of relaxing the punishing effect of large differences in large predicted values.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Mean Absolute Error Loss:&lt;/strong&gt; On some regression problems, the distribution of the target variable may be mostly Gaussian, but may have outliers, e.g. large or small values far from the mean value. The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is more robust to outliers. It is calculated as the average of the absolute difference between the actual and predicted values.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/loss_function.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cse.iitm.ac.in/~miteshk/CS7015.html&quot;&gt;Image Source: From Prof. Mitesh Khapra, Lecture 4, [slide page 95]&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;binary-classification-loss&quot;&gt;BINARY CLASSIFICATION LOSS&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Cross Entropy Loss or Negative Loss Likelihood (NLL):&lt;/strong&gt; It is the default loss function to use for binary classification problems. It is intended for use with binary classification where the target values are in the set &lt;code class=&quot;highlighter-rouge&quot;&gt;{0, 1}&lt;/code&gt;. Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;CrossEntropyLoss = -(y_i \log (\hat y_i) + (1-y_i) \log (1-\hat y_i))&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Hinge Loss or SVM Loss:&lt;/strong&gt; An alternative to cross-entropy for binary classification problems is the hinge loss function, primarily developed for use with &lt;strong&gt;Support Vector Machine&lt;/strong&gt; (SVM) models. It is intended for use with binary classification where the target values are in the set {-1, 1}. The hinge loss function encourages examples to have the correct sign, assigning more error when there is a difference in the sign between the actual and predicted class values.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;HingeLoss = \Sigma_{j \neq y_i} max(0, s_j - s_{y_i}+1)&lt;/script&gt;

&lt;h3 id=&quot;multi-class-classification-loss-functions&quot;&gt;Multi-Class Classification Loss Functions&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Categorical Cross Entropy:&lt;/strong&gt; It is the default loss function to use for multi-class classification problems. In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, …, n}, where each class is assigned a unique integer value. Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;CategoricalCrossEntropyLoss = -y_c \log (\hat y_c)&lt;/script&gt;

&lt;p&gt;, where &lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt; is the class.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sparse Categorical Cross Entropy:&lt;/strong&gt; A possible cause of frustration when using cross-entropy with classification problems with a large number of labels is the one hot encoding process. For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory. Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kullback Leibler Divergence Loss:&lt;/strong&gt; Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution. A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution. As such, the KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Reference:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/&quot;&gt;MMM&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html&quot;&gt;Blog&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;autoencoders&quot;&gt;Autoencoders&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798&quot;&gt;Applied-deep-learning-part-3-autoencoders&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;word-embedding---implementation&quot;&gt;Word embedding - Implementation&lt;/h2&gt;

&lt;p&gt;Natural language processing systems traditionally treat words as discrete atomic symbols, and therefore ‘cat’ may be represented as Id537 and ‘dog’ as Id143. These encodings are arbitrary, and provide no useful information to the system regarding the relationships that may exist between the individual symbols. This means that the model can leverage very little of what it has learned about ‘cats’ when it is processing data about ‘dogs’.&lt;/p&gt;

&lt;p&gt;Word embeddings transform sparse vector representations of words into a dense, continuous vector space, enabling you to identify similarities between words and phrases — on a large scale — based on their context.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Vector space models&lt;/code&gt; (VSMs) represent (&lt;code class=&quot;highlighter-rouge&quot;&gt;embed&lt;/code&gt;) words in a continuous vector space where semantically similar words are mapped to nearby points (&lt;code class=&quot;highlighter-rouge&quot;&gt;'are embedded nearby each other'&lt;/code&gt;). VSMs have a long, rich history in NLP, but all methods depend in some way or another on the &lt;code class=&quot;highlighter-rouge&quot;&gt;Distributional Hypothesis&lt;/code&gt;, which states that words that appear in the same contexts share semantic meaning. The different approaches that leverage this principle can be divided into two categories:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;count-based methods&lt;/code&gt; (e.g. Latent Semantic Analysis)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;predictive methods&lt;/code&gt; (e.g. neural probabilistic language models).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This distinction is elaborated in much more detail by Baroni et al., but in a nutshell:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Count-based methods compute the statistics of how often some word co-occurs with its neighbor words in a large text corpus, and then map these count-statistics down to a small, dense vector for each word.&lt;/li&gt;
  &lt;li&gt;Predictive models directly try to predict a word from its neighbors in terms of learned small, dense embedding vectors (considered parameters of the model).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Neural probabilistic language models are traditionally trained using the maximum likelihood (ML) principle to maximize the probability of the next word $w_t$ (for “target”) given the previous words $h$ (for “history”) in terms of a softmax function&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;P(w_t | h) = \text{softmax} (\text{score} (w_t, h))&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= \frac{\exp \{ \text{score} (w_t, h) \} }
             {\sum_\text{Word w' in Vocab} \exp \{ \text{score} (w', h) \} }&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;where $score(w_t,h)$ computes the compatibility of word $w_t$  with the context $h$ (a dot product is commonly used). We train this model by maximizing its log-likelihood on the training set, i.e. by maximizing&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;J_\text{ML} = \log P(w_t | h)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= \text{score} (w_t, h) -
     \log \left( \sum_\text{Word w' in Vocab} \exp \{ \text{score} (w', h) \} \right).&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This yields a &lt;code class=&quot;highlighter-rouge&quot;&gt;properly normalized probabilistic&lt;/code&gt; model for language modeling. However this is very expensive, because we need to compute and normalize each probability using the score for all other $V$ words $w’$ in the current context $h$ , at every training step.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.tensorflow.org/images/softmax-nplm.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/tutorials/representation/word2vec&quot;&gt;source_1&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;word2vec--implementation&quot;&gt;Word2vec = Implementation&lt;/h2&gt;

&lt;p&gt;Word2vec is a particularly computationally-efficient predictive model for learning word embeddings from raw text. It comes in two flavors:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Continuous Bag-of-Words model (CBOW)&lt;/li&gt;
  &lt;li&gt;Skip-Gram model (Section 3.1 and 3.2 in Mikolov et al.).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Algorithmically, these models are similar, except that CBOW predicts target words (e.g. ‘mat’) from a sliding window of a source context words (‘the cat sits on the’), while the skip-gram does the inverse and predicts source context-words from the target words. This inversion might seem like an arbitrary choice, but statistically it has the effect that CBOW smoothes over a lot of the distributional information (by treating an entire context as one observation). For the most part, this turns out to be a useful thing for smaller datasets. However, skip-gram treats each context-target pair as a new observation, and this tends to do better when we have larger datasets. We will focus on the skip-gram model in the rest of this tutorial.&lt;/p&gt;

&lt;p&gt;For feature learning in word2vec we do not need a full probabilistic model. The CBOW and skip-gram models are instead trained using a binary classification objective (logistic regression) to discriminate the real target words $w_t$ from $k$ imaginary (noise) words $\tilde w$, in the same context. We illustrate this below for a CBOW model. For skip-gram the direction is simply inverted.&lt;/p&gt;

&lt;p&gt;Google’s word2vec is one of the most widely used implementations due to its training speed and performance. Word2vec is a predictive model, which means that instead of utilizing word counts à la  latent Dirichlet allocation (LDA), it is trained to predict a target word from the context of its neighboring words. The model first encodes each word using one-hot-encoding, then feeds it into a hidden layer using a matrix of weights; the output of this process is the target word. The word embedding vectors are are actually the weights of this fitted model. To illustrate, here’s a simple visual:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.datascience.com/hs-fs/hubfs/Resources/Articles/nn_embed.png?t=1532623979827&amp;amp;width=1414&amp;amp;height=694&amp;amp;name=nn_embed.png&quot; alt=&quot;image&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.tensorflow.org/images/nce-nplm.png&quot; alt=&quot;image&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.tensorflow.org/tutorials/representation/word2vec&quot;&gt;source_1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.datascience.com/resources/notebooks/word-embeddings-in-python&quot;&gt;source_2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795&quot;&gt;source_3&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;lda-topic-modelling&quot;&gt;LDA, Topic Modelling&lt;/h2&gt;

&lt;p&gt;Topic Modelling is different from rule-based text mining approaches that use regular expressions or dictionary based keyword searching techniques. It is an unsupervised approach used for finding and observing the bunch of words (called “topics”) in large clusters of texts.&lt;/p&gt;

&lt;p&gt;Topics can be defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model should result in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.&lt;/p&gt;

&lt;p&gt;Topic Models are very useful for the purpose for document clustering, organizing large blocks of textual data, information retrieval from unstructured text and feature selection. For Example – New York Times are using topic models to boost their user – article recommendation engines. Various professionals are using topic models for recruitment industries where they aim to extract latent features of job descriptions and map them to right candidates. They are being used to organize large datasets of emails, customer reviews, and user social media profiles.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.analyticsvidhya.com/wp-content/uploads/2016/08/Modeling1.png&quot; alt=&quot;image&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can try doing topic modelling using the following methods.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Term Frequency and Inverse Document Frequency.&lt;/li&gt;
  &lt;li&gt;Do Non negative Matrix Factorization (NMF)&lt;/li&gt;
  &lt;li&gt;LDA.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NMF is supposed to be a lot faster than LDA, but LDAs supposed to be more accurate. Problem is LDA takes a long time, unless you’re using distributed computing.&lt;/p&gt;

&lt;p&gt;LDA assumes documents are produced from a mixture of topics. Those topics then generate words based on their probability distribution. Given a dataset of documents, LDA backtracks and tries to figure out what topics would create those documents in the first place.&lt;/p&gt;

&lt;p&gt;LDA is a matrix factorization technique. In vector space, any corpus (collection of documents) can be represented as a document-term matrix. LDA converts this Document-Term Matrix into two lower dimensional matrices – M1 and M2.
M1 is a document-topics matrix and M2 is a topic – terms matrix with dimensions ($N$,  $K$) and ($K$, $M$) respectively, where $N$ is the number of documents, $K$ is the number of topics and $M$ is the vocabulary size.&lt;/p&gt;

&lt;p&gt;It Iterates through each word &lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt; for each document &lt;code class=&quot;highlighter-rouge&quot;&gt;d&lt;/code&gt; and tries to adjust the current topic – word assignment with a new assignment. A new topic &lt;code class=&quot;highlighter-rouge&quot;&gt;k&lt;/code&gt; is assigned to word &lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt; with a probability P which is a product of two probabilities p1 and p2.&lt;/p&gt;

&lt;p&gt;For every topic, two probabilities p1 and p2 are calculated.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;P1: $p(topic_t&lt;/td&gt;
          &lt;td&gt;doc_d)$ = the proportion of words in document d that are currently assigned to topic t.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;P2: $p(word_w&lt;/td&gt;
          &lt;td&gt;topic_t)$ = the proportion of assignments to topic t over all documents that come from this word w.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The current topic – word assignment is updated with a new topic with the probability, product of p1 and p2 . In this step, the model assumes that all the existing word – topic assignments except the current word are correct. This is essentially the probability that topic t generated word w, so it makes sense to adjust the current word’s topic with new probability.&lt;/p&gt;

&lt;p&gt;After a number of iterations, a steady state is achieved where the document topic and topic term distributions are fairly good. This is the convergence point of LDA.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/&quot;&gt;AVB&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/&quot;&gt;EdwinChen&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/@connectwithghosh/topic-modelling-with-latent-dirichlet-allocation-lda-in-pyspark-2cb3ebd5678e&quot;&gt;medium-PySparkImplementation&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;lstm-demystified&quot;&gt;LSTM demystified&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Core Idea:&lt;/strong&gt;
The key to LSTMs is the cell state, the horizontal line running through the top of the diagram. The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates. It has &lt;code class=&quot;highlighter-rouge&quot;&gt;forget gate&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;input gate&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;output gate&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Forget Gate:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It controls how much old information (or old memory or state) you want to retain or not. $f_t\in(0,1)$ and thus it maintains a proportion of old memory.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Input Gate:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The next step is to decide what new information we’re going to store in the cell state. This has two parts.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First, a sigmoid layer called the “input gate layer” decides which values we’ll update.&lt;/li&gt;
  &lt;li&gt;Second, a tanh layer creates a vector of new candidate values, $\tilde{C}_t$, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Cell Update:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We multiply the old state by ft, forgetting the things we decided to forget earlier. Then we add $i_t*\tilde{C}_t$. This is the new candidate values, scaled by how much we decided to update each state value.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Output Gate:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First, we run a sigmoid layer which decides what parts of the cell state we’re going to output.&lt;/li&gt;
  &lt;li&gt;Second, we put the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;chris_olah&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714&quot;&gt;medium&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;a_karpathy&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;why-the-inputoutput-gates-are-needed-&quot;&gt;why the input/output gates are needed ?&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html&quot;&gt;source_1&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;adaboost-objective-function&quot;&gt;Adaboost objective function&lt;/h2&gt;

&lt;h2 id=&quot;vanishing-gradient-problem-in-general&quot;&gt;Vanishing Gradient Problem in general&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b&quot;&gt;source&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-is-vanishing-gradient-problem-in-rnn&quot;&gt;What is vanishing gradient problem in RNN?&lt;/h2&gt;
&lt;p&gt;If you don’t already know, the vanishing gradient problem arises when,during backprop, the error signal used to train the network exponentially decreases the further you travel backwards in your network. The effect of this is that the layers closer to your input don’t get trained.&lt;/p&gt;

&lt;p&gt;To understand why LSTMs help, we need to understand the problem with vanilla RNNs. In a vanilla RNN, the hidden vector and the output is computed as such:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;h_t = tanh(W_Ix_t + W_Rh_{t-1})&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;y_t = W_Oh_t&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To do backpropagation through time to train the RNN, we need to compute the gradient of $E$ with respect to $W_R$. The overall error gradient is equal to the sum of the error gradients at each time step. For step $t$, we can use the multivariate chain rule to derive the error gradient as:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial E_t}{\partial W_R} = \sum^{t}_{i=0} \frac{\partial E_t}{\partial y_t}\frac{\partial y_t}{\partial h_t}\frac{\partial h_t}{\partial h_i}\frac{\partial h_i}{\partial W_R}&lt;/script&gt;

&lt;p&gt;Now everything here can be computed pretty easily except the term 
$\frac{\partial h_t}{\partial h_i}$, which needs another chain rule application to compute:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial h_t}{\partial h_i} = \frac{\partial h_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial h_{t-2}}...\frac{\partial h_{i+1}}{\partial h_i} = \prod^{t-1}_{k=i} \frac{\partial h_{k+1}}{\partial h_k}&lt;/script&gt;

&lt;p&gt;Now let us look at a single one of these terms by taking the derivative of $h_{k+1}$ with respect to $h_k$ (where $diag$ turns a vector into a diagonal matrix):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial h_{k+1}}{\partial h_k} =  diag(f'(W_Ix_i + W_Rh_{i-1}))W_R&lt;/script&gt;

&lt;p&gt;Thus, if we want to backpropagate through $k$ timesteps, this gradient will be :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial h_{k}}{\partial h_1} =  \prod_i^k diag(f'(W_Ix_i + W_Rh_{i-1}))W_R&lt;/script&gt;

&lt;p&gt;As shown in this paper, if the dominant eigenvalue of the matrix $W_R$
 is greater than 1, the gradient explodes. If it is less than 1, the gradient vanishes.2 The fact that this equation leads to either vanishing or exploding gradients should make intuitive sense. Note that the values of $f’(x)$ will always be less than 1. So if the magnitude of the values of $W_R$ are too small, then inevitably the derivative will go to 0. The repeated multiplications of values less than one would overpower the repeated multiplications of $W_R$
. On the contrary, make $W_R$ too big and the derivative will go to infinity since the exponentiation of $W_R$ will overpower the repeated multiplication of the values less than 1. In practice, the vanishing gradient is more common, so we will mostly focus on that.&lt;/p&gt;

&lt;p&gt;The derivative $\frac{\partial h_{k}}{\partial h_1}$ is essentially telling us how much our hidden state at time $k$ will change when we change the hidden state at time 1 by a little bit. According to the above math, if the gradient vanishes it means the earlier hidden states have no real effect on the later hidden states, meaning no long term dependencies are learned! This can be formally proved, and has been in many papers, including the original LSTM paper.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html&quot;&gt;source_1&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-does-lstm-solve-vanishing-gradient-problem&quot;&gt;How does LSTM solve vanishing gradient problem?&lt;/h2&gt;

&lt;p&gt;the vanishing gradient problem arises when, during backprop, the error signal used to train the network exponentially decreases the further you travel backwards in your network. The effect of this is that the layers closer to your input don’t get trained. In the case of RNNs (which can be unrolled and thought of as feed forward networks with shared weights) this means that you don’t keep track of any long term dependencies. This is kind of a bummer, since the whole point of an RNN is to keep track of long term dependencies. The situation is analogous to having a video chat application that can’t handle video chats!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;LSTM Equation Reference:&lt;/strong&gt; Quickly, here is a little review of the LSTM equations, with the biases left off&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$f_t=\sigma(W_f[h_{t-1},x_t])$&lt;/li&gt;
  &lt;li&gt;$i_t=\sigma(W_i[h_{t-1},x_t])$&lt;/li&gt;
  &lt;li&gt;$o_t=\sigma(W_o[h_{t-1},x_t])$&lt;/li&gt;
  &lt;li&gt;$\widetilde{C}&lt;em&gt;t=tanh(W_C[h&lt;/em&gt;{t-1},x_t])$&lt;/li&gt;
  &lt;li&gt;$C_t=f_tC_{t-1} + i_t\widetilde{C}_t$&lt;/li&gt;
  &lt;li&gt;$h_t=o_ttanh(C_t)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As we can see above (previous question), the biggest culprit in causing our gradients to vanish is that dastardly recursive derivative we need to compute: 
$\frac{\partial h_t}{\partial h_i}$. If only this derivative was ‘well behaved’ (that is, it doesn’t go to 0 or infinity as we backpropagate through layers) then we could learn long term dependencies!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The original LSTM solution:&lt;/strong&gt; The original motivation behind the LSTM was to make this recursive derivative have a constant value. If this is the case then our gradients would neither explode or vanish. How is this accomplished? As you may know, the LSTM introduces a separate cell state $C_t$. In the original 1997 LSTM, the value for $C_t$ depends on the previous value of the cell state and an update term weighted by the input gate value.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C_t = C_{t-1} + i\widetilde{C}_t&lt;/script&gt;

&lt;p&gt;This formulation doesn’t work well because the cell state tends to grow uncontrollably. In order to prevent this unbounded growth, a forget gate was added to scale the previous cell state, leading to the more modern formulation:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C_t = fC_{t-1} + i\widetilde{C}_t&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;A common misconception:&lt;/strong&gt; Most explanations for why LSTMs solve the vanishing gradient state that under this update rule, the recursive derivative is equal to 1 (in the case of the original LSTM) or f
(in the case of the modern LSTM)3 and is thus well behaved! One thing that is often forgotten is that f, i, and $\widetilde{C}_t$
are all functions of $C_t$, and thus we must take them into consideration when calculating the gradient.&lt;/p&gt;

&lt;p&gt;The reason for this misconception is pretty reasonable. In the original LSTM formulation in 1997, the recursive gradient actually was equal to 1. The reason for this is because, in order to enforce this constant error flow, the gradient calculation was truncated so as not to flow back to the input or candidate gates. So with respect to $C_{t-1}$ they could be treated as constants. In fact truncating the gradients in this way was done up till about 2005, until the publication of this paper by Alex Graves. Since most popular neural network frameworks now do auto differentiation, its likely that you are using the full LSTM gradient formulation too! So, does the above argument about why LSTMs solve the vanishing gradient change when using the full gradient? The answer is no, actually it remains mostly the same. It just gets a bit messier.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Looking at the full LSTM gradient:&lt;/strong&gt; To understand why nothing really changes when using the full gradient, we need to look at what happens to the recursive gradient when we take the full gradient. For the derivation check this &lt;a href=&quot;https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; One important thing to note is that the values 
$f_t$, $o_t$, $i_t$, and $\widetilde{C}_t$ are things that the network learns to set (conditioned on the current input and hidden state). Thus, in this way the network learns to decide when to let the gradient vanish, and when to preserve it, by setting the gate values accordingly!&lt;/p&gt;

&lt;p&gt;This might all seem magical, but it really is just the result of two main things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The additive update function for the cell state gives a derivative thats much more ‘well behaved’&lt;/li&gt;
  &lt;li&gt;The gating functions allow the network to decide how much the gradient vanishes, and can take on different values at each time step. The values that they take on are learned functions of the current input and hidden state.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;there-is-still-a-chance-of-gradient-vanishing-but-the-model-would-regulate-its-forget-gate-value-to-prevent-that-from-happening&quot;&gt;There is still a chance of gradient vanishing, but the model would regulate its forget gate value to prevent that from happening?&lt;/h3&gt;

&lt;p&gt;Yes this is correct. If the model is dumb and always sets its forget gate to a low value, then the gradient will vanish. Since the forget gate value is a learnable function, we hope that it learns to regulate the forget gate in a way that improves task performance&lt;/p&gt;

&lt;h3 id=&quot;how-does-this-prevent-gradient-explosion-from-my-understanding-gradient-clipping-is-applied-to-present-that-from-happening&quot;&gt;How does this prevent gradient explosion? From my understanding gradient clipping is applied to present that from happening.&lt;/h3&gt;

&lt;p&gt;This does not really help with gradient explosions, if your gradients are too high there is little that the LSTM gating functions can do for you. There are two standard methods for preventing exploding gradients: Hope and pray that you don’t get them while training, or use gradient clipping.
The latter has a greater success rate, however I have had some success with the former, YMMV.&lt;/p&gt;

&lt;h2 id=&quot;how-does-cnn-solve-vanishing-gradient-problem&quot;&gt;How does CNN solve vanishing gradient problem?&lt;/h2&gt;

&lt;p&gt;RNNs unfolded are deep in common application scenarios, thus prone to severer vanishing gradient problems. CNNs in typical application scenarios are shallower, but still suffer from the same problem with gradients.&lt;/p&gt;

&lt;p&gt;The most recommended approaches to overcome the vanishing gradient problem are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Layerwise pre-training&lt;/li&gt;
  &lt;li&gt;Choice of the activation function&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You may see that the state-of-the-art deep neural network for computer vision problem (like the ImageNet winners) have used convolutional layers as the first few layers of the their network, but it is not the key for solving the vanishing gradient. The key is usually training the network greedily layer by layer. Using convolutional layers have several other important benefits of course. Especially in vision problems when the input size is large (the pixels of an image), using convolutional layers for the first layers are recommended because they have fewer parameters than fully-connected layers and you don’t end up with billions of parameters for the first layer (which will make your network prone to overfitting).&lt;/p&gt;

&lt;p&gt;However, it has been shown (like this paper) for several tasks that using Rectified linear units alleviates the problem of vanishing gradients (as oppose to conventional sigmoid functions).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://stackoverflow.com/questions/28953622/do-convolutional-neural-networks-suffer-from-the-vanishing-gradient&quot;&gt;source_1&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-autoencoder-works&quot;&gt;How autoencoder works?&lt;/h2&gt;

&lt;h2 id=&quot;what-is-deep-learning--difference-between-machine-learning--deep-learning-&quot;&gt;What is deep learning ? Difference between machine learning &amp;amp; Deep learning ?&lt;/h2&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;how-can-you-cluster-documents-in-unsupervised-way&quot;&gt;&lt;strong&gt;How can you cluster documents in unsupervised way?&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/utkuozbulak/unsupervised-learning-document-clustering&quot;&gt;source&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-do-you-find-the-similar-documents-related-to-some-query-sentencesearch&quot;&gt;&lt;strong&gt;How do you find the similar documents related to some query sentence/search?&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;simplest apporach is to do tf-idf of both documents and query, and then measure cosine distance (i.e., dot product)&lt;/li&gt;
  &lt;li&gt;on top of that, if you use SVD/PCA/LSA on the tfidf matrix, it should further improve results.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.r-bloggers.com/build-a-search-engine-in-20-minutes-or-less/&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;explain-tf-idf-&quot;&gt;&lt;strong&gt;Explain TF-IDF&lt;/strong&gt; ?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/&quot;&gt;link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-word2vec-what-is-the-cost-function-for-skip-gram-modelk-negative-sampling-&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;What is word2vec? What is the cost function for skip-gram model(k-negative sampling) ?&lt;/strong&gt;&lt;/span&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ASn7ExxLZws&amp;amp;list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6&amp;amp;index=3&quot;&gt;cs224-lecture&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://adventuresinmachinelearning.com/word2vec-keras-tutorial/&quot;&gt;keras implementation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/&quot;&gt;AVB-Different word counting technique&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;so-as-per-my-experience-tf-idf-fails-in-document-classificationclustering--how-can-you-improve-further-&quot;&gt;So As per my experience, Tf-Idf fails in document classification/clustering ? How can you improve further ?&lt;/h2&gt;

&lt;h2 id=&quot;what-are-word2vec-vectors&quot;&gt;&lt;strong&gt;What are word2vec vectors?&lt;/strong&gt;&lt;/h2&gt;

&lt;h2 id=&quot;how-did-you-perform-language-identification-from-text-sentence--as-in-my-cv&quot;&gt;How did you perform language identification from text sentence ? (As in my CV)&lt;/h2&gt;
&lt;h2 id=&quot;how-does-you-represent-the-symbolic-chinese-or-japanese-alphabets-here-&quot;&gt;How does you represent the symbolic chinese or japanese alphabets here ?&lt;/h2&gt;
&lt;h2 id=&quot;how-can-i-design-a-chatbot--i-had-little-idea-but-i-tried-answering-it-with-intent-and-response-tf-idf-based-similarity&quot;&gt;&lt;strong&gt;How can I design a chatbot ? (I had little idea but I tried answering it with intent and response tf-idf based similarity)&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://adeshpande3.github.io/adeshpande3.github.io/How-I-Used-Deep-Learning-to-Train-a-Chatbot-to-Talk-Like-Me&quot;&gt;Adit Deshpande&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;can-i-develop-a-chatbot-with-rnn-providing-a-intent-and-response-pair-in-input-&quot;&gt;&lt;strong&gt;Can I develop a chatbot with RNN providing a intent and response pair in input ?&lt;/strong&gt;&lt;/h2&gt;
&lt;h2 id=&quot;suppose-i-developed-a-chatbot-with-rnnlstms-on-reddit-dataset&quot;&gt;Suppose I developed a chatbot with RNN/LSTMs on Reddit dataset.&lt;/h2&gt;
&lt;p&gt;It gives me 10 probable responses ? 
   How can I choose the best reply Or how can I eliminate others replies ?**&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How do you perform text classification ?&lt;/li&gt;
  &lt;li&gt;How can you make sure to learn a context !! Well its not possible with TF-IDF ?
    &lt;ul&gt;
      &lt;li&gt;I told him about taking n-grams say n = 1, 2, 3, 4 and concatenating tf-idf of them to make a long count vector ?
Okay that is the baseline people start with ? What can you do more with machine learning ? 
(I tried suggesting LSTM with word2vec or 1D-CNN with word2vec for classification but 
 he wanted improvements in machine learning based methods :-|)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;how-does-neural-networks-learns-non-linear-shapes-when-it-is-made-of-linear-nodes--what-makes-it-learn-non-linear-boundaries-&quot;&gt;&lt;strong&gt;How does neural networks learns non-linear shapes when it is made of linear nodes ? What makes it learn non-linear boundaries ?&lt;/strong&gt;&lt;/h2&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is the range of sigmoid function&lt;/strong&gt; ?&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
  &lt;li&gt;Text classification method. How will you do it ?&lt;/li&gt;
  &lt;li&gt;Explain Tf-Idf ? &lt;strong&gt;What is the drawback of Tf-Idf&lt;/strong&gt; ? How do you overcome it ?&lt;/li&gt;
  &lt;li&gt;What are bigrams &amp;amp; Tri-grams ? Explain with example of Tf-Idf of bi-grams &amp;amp; trigrams with a text sentence.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;What is an application of word2vec&lt;/strong&gt; ? Example.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How will you design a neural network ?&lt;/strong&gt; How about making it very deep ? Very basic questions on neural network.?&lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;color:red&quot;&gt;&lt;strong&gt;How does LSTM work ? How can it remember the context ?&lt;/strong&gt;&lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Must Watch:&lt;/strong&gt; CS231n by Karpathy in 2016 course and Justin in 2017 course.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;How did you perform language identification ? What were the  feature ?&lt;/li&gt;
  &lt;li&gt;How did you model classifiers like speech vs music and speech vs non-speech ?&lt;/li&gt;
  &lt;li&gt;How can deep neural network be applied in these speech analytics applications ?&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Sankarshan Mridha</name></author><summary type="html">Quick Refresher: Neural Network</summary></entry><entry><title type="html">Blog 201: DL Papers</title><link href="http://localhost:4000/blog/2019/07/28/blog_201_DL_papers" rel="alternate" type="text/html" title="Blog 201: DL Papers" /><published>2019-07-28T00:11:31+05:30</published><updated>2019-07-28T00:11:31+05:30</updated><id>http://localhost:4000/blog/2019/07/28/blog_201_DL_papers</id><content type="html" xml:base="http://localhost:4000/blog/2019/07/28/blog_201_DL_papers">&lt;h1 id=&quot;list-of-papers---you-must-read&quot;&gt;List of papers - you must read&lt;/h1&gt;

&lt;h3 id=&quot;dropout-a-simple-way-to-prevent-neural-networks-from-overfitting&quot;&gt;Dropout: A Simple Way to Prevent Neural Networks from Overfitting&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; Deep neural nets with a large number of parameters are very powerful machine learning
systems. However, overfitting is a serious problem in such networks. Large networks are also
slow to use, making it difficult to deal with overfitting by combining the predictions of many
different large neural nets at test time. Dropout is a technique for addressing this problem.
The key idea is to randomly drop units (along with their connections) from the neural
network during training. This prevents units from co-adapting too much. During training,
dropout samples from an exponential number of different “thinned” networks. At test time,
it is easy to approximate the effect of averaging the predictions of all these thinned networks
by simply using a single unthinned network that has smaller weights. This significantly
reduces overfitting and gives major improvements over other regularization methods. We
show that dropout improves the performance of neural networks on supervised learning
tasks in vision, speech recognition, document classification and computational biology,
obtaining state-of-the-art results on many benchmark data sets.&lt;/p&gt;

&lt;p&gt;JMLR 2014&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift&quot;&gt;Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; Training Deep Neural Networks is complicated
by the fact that the distribution of each layer’s
inputs changes during training, as the parameters
of the previous layers change. This slows
down the training by requiring lower learning
rates and careful parameter initialization, and
makes it notoriously hard to train models with
saturating nonlinearities. We refer to this phenomenon
as internal covariate shift, and address
the problem by normalizing layer inputs.
Our method draws its strength from making normalization
a part of the model architecture and
performing the normalization for each training
mini-batch. Batch Normalization allows us to
use much higher learning rates and be less careful
about initialization, and in some cases eliminates
the need for Dropout. Applied to a stateof-the-art
image classification model, Batch Normalization
achieves the same accuracy with 14
times fewer training steps, and beats the original
model by a significant margin. Using an ensemble
of batch-normalized networks, we improve
upon the best published result on ImageNet classification:
reaching 4.82% top-5 test error, exceeding
the accuracy of human raters.&lt;/p&gt;

&lt;p&gt;ICML 2015&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v37/ioffe15.pdf&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;generative-adversarial-nets&quot;&gt;Generative Adversarial Nets&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; We propose a new framework for estimating generative models via an adversarial
process, in which we simultaneously train two models: a generative model G
that captures the data distribution, and a discriminative model D that estimates
the probability that a sample came from the training data rather than G. The training
procedure for G is to maximize the probability of D making a mistake. This
framework corresponds to a minimax two-player game. In the space of arbitrary
functions G and D, a unique solution exists, with G recovering the training data
distribution and D equal to &lt;code class=&quot;highlighter-rouge&quot;&gt;0.5&lt;/code&gt; everywhere. In the case where G and D are defined
by multilayer perceptrons, the entire system can be trained with backpropagation.
There is no need for any Markov chains or unrolled approximate inference networks
during either training or generation of samples. Experiments demonstrate
the potential of the framework through qualitative and quantitative evaluation of
the generated samples.&lt;/p&gt;

&lt;p&gt;NIPS 2014&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://datascienceassn.org/sites/default/files/Generative%20Adversarial%20Nets.pdf&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;attention-is-all-you-need&quot;&gt;Attention Is All You Need&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt;The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German
translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.0 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature.&lt;/p&gt;

&lt;p&gt;NIPS 2017
&lt;a href=&quot;http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;how-transferable-are-features-in-deep-neural-networks&quot;&gt;How transferable are features in deep neural networks?&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt;Many deep neural networks trained on natural images exhibit a curious phenomenon
in common: on the first layer they learn features similar to Gabor filters
and color blobs. Such first-layer features appear not to be specific to a particular
dataset or task, but general in that they are applicable to many datasets and tasks.
Features must eventually transition from general to specific by the last layer of
the network, but this transition has not been studied extensively. In this paper we
experimentally quantify the generality versus specificity of neurons in each layer
of a deep convolutional neural network and report a few surprising results. Transferability
is negatively affected by two distinct issues: (1) the specialization of
higher layer neurons to their original task at the expense of performance on the
target task, which was expected, and (2) optimization difficulties related to splitting
networks between co-adapted neurons, which was not expected. In an example
network trained on ImageNet, we demonstrate that either of these two issues
may dominate, depending on whether features are transferred from the bottom,
middle, or top of the network. We also document that the transferability of features
decreases as the distance between the base task and target task increases, but
that transferring features even from distant tasks can be better than using random
features. A final surprising result is that initializing a network with transferred
features from almost any number of layers can produce a boost to generalization
that lingers even after fine-tuning to the target dataset.&lt;/p&gt;

&lt;p&gt;NIPS 2014
&lt;a href=&quot;http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;k-means-the-advantages-of-careful-seeding&quot;&gt;k-means++: The Advantages of Careful Seeding&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;: The k-means method is a widely used clustering technique that seeks to minimize the average
squared distance between points in the same cluster. Although it offers no accuracy guarantees,
its simplicity and speed are very appealing in practice. By augmenting k-means with a simple,
randomized seeding technique, we obtain an algorithm that is O(log k)-competitive with the
optimal clustering. Experiments show our augmentation improves both the speed and the
accuracy of k-means, often quite dramatically.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf&quot;&gt;paper&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/terryum/awesome-deep-learning-papers#understanding--generalization--transfer&quot;&gt;Awsome deep learning papers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/solaris33/awesome-machine-learning-papers&quot;&gt;Papers from NIPS 2016&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ChristosChristofidis/awesome-deep-learning&quot;&gt;Detailed list of papers, datasets, etc.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.xn--vjq503akpco3w.top/literature/awesome-free-deep-learning-papers.html&quot;&gt;Concise List&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Sankarshan Mridha</name></author><summary type="html">List of papers - you must read</summary></entry><entry><title type="html">Blog 202: Deep Learning Programming Tips</title><link href="http://localhost:4000/blog/2019/07/28/blog_202_DL_programming_tips" rel="alternate" type="text/html" title="Blog 202: Deep Learning Programming Tips" /><published>2019-07-28T00:11:31+05:30</published><updated>2019-07-28T00:11:31+05:30</updated><id>http://localhost:4000/blog/2019/07/28/blog_202_DL_programming_tips</id><content type="html" xml:base="http://localhost:4000/blog/2019/07/28/blog_202_DL_programming_tips">&lt;h2 id=&quot;deep-learning-best-practices---mistakes-and-tips&quot;&gt;&lt;strong&gt;Deep Learning Best Practices - Mistakes and Tips&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The purpose of this repo is to consolidate all the &lt;strong&gt;Best Practicies&lt;/strong&gt; for building neural network model curated over the internet&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Try to overfit a single batch first
    &lt;ul&gt;
      &lt;li&gt;It’s a very quick sanity test of your wiring; i.e. if you can’t overfit a small amount of data you’ve got a simple bug somewhere&lt;/li&gt;
      &lt;li&gt;it’s by far the most “bang for the buck” trick that noone uses that exists.
5 replies 7 retweets 219 likes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Forgot to toggle train/eval mode for the net&lt;/li&gt;
  &lt;li&gt;Forgot to &lt;code class=&quot;highlighter-rouge&quot;&gt;.zero_grad()&lt;/code&gt; (in pytorch) before &lt;code class=&quot;highlighter-rouge&quot;&gt;.backward()&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Passed &lt;code class=&quot;highlighter-rouge&quot;&gt;softmaxed outputs&lt;/code&gt; to a loss that expects &lt;code class=&quot;highlighter-rouge&quot;&gt;raw logits&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;You didn’t use &lt;code class=&quot;highlighter-rouge&quot;&gt;bias=False&lt;/code&gt; for your &lt;code class=&quot;highlighter-rouge&quot;&gt;Linear/Conv2d&lt;/code&gt; layer when using &lt;code class=&quot;highlighter-rouge&quot;&gt;BatchNorm&lt;/code&gt;, or conversely forget to include it for the output layer .This one won’t make you silently fail, but they are spurious parameters&lt;/li&gt;
  &lt;li&gt;Thinking &lt;code class=&quot;highlighter-rouge&quot;&gt;view()&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;permute()&lt;/code&gt; are the same thing (&amp;amp; incorrectly using view)&lt;/li&gt;
  &lt;li&gt;starting with &lt;code class=&quot;highlighter-rouge&quot;&gt;small model&lt;/code&gt; + &lt;code class=&quot;highlighter-rouge&quot;&gt;small amount of data&lt;/code&gt; &amp;amp; growing both together; I always find it really insightful
    &lt;ul&gt;
      &lt;li&gt;I like to start with the simplest possible sanity checks - e.g. also training on all zero data first to see what loss I get with the base output distribution, then gradually include more inputs and scale up the net, making sure I beat the previous thing each time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Source:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/msank00/datascience_bestpractices/blob/master/dl_mistakes_tips.md&quot;&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Read the below references&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;These are pure gold.&lt;/p&gt;

&lt;h4 id=&quot;reference&quot;&gt;Reference&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/karpathy/status/1013244313327681536&quot;&gt;tweet_andrej_karpathy&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://karpathy.github.io/2019/04/25/recipe/&quot;&gt;Recipe for training neural network&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stats.stackexchange.com/questions/352036/what-should-i-do-when-my-neural-network-doesnt-learn&quot;&gt;What should I do when my neural network doesn’t learn?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/&quot;&gt;Practical Advice for Building Deep Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;techinical-mistakes-while-model-building&quot;&gt;&lt;strong&gt;Techinical Mistakes while Model Building&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Create a &lt;code class=&quot;highlighter-rouge&quot;&gt;non-reproducible&lt;/code&gt; data preparation steps&lt;/li&gt;
  &lt;li&gt;Evaluate a model based on performance of training set&lt;/li&gt;
  &lt;li&gt;Didn’t notice &lt;code class=&quot;highlighter-rouge&quot;&gt;large outlier&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Dropped missing values when it made sense to flag them&lt;/li&gt;
  &lt;li&gt;Flagged missing values when it made sense to drop them&lt;/li&gt;
  &lt;li&gt;Set missing values to Zero&lt;/li&gt;
  &lt;li&gt;Not comparing a complex model to a &lt;strong&gt;simple baseline&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Failed to understand nuances of data collection&lt;/li&gt;
  &lt;li&gt;Build model for &lt;strong&gt;wrong point in time&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Deleted records with missing values&lt;/li&gt;
  &lt;li&gt;Predicted the wrong outcome&lt;/li&gt;
  &lt;li&gt;Made &lt;strong&gt;faulty assumptions&lt;/strong&gt; about &lt;code class=&quot;highlighter-rouge&quot;&gt;time zones&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Made &lt;strong&gt;faulty assumptions&lt;/strong&gt; about &lt;code class=&quot;highlighter-rouge&quot;&gt;data format&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Made &lt;strong&gt;faulty assumptions&lt;/strong&gt; about &lt;code class=&quot;highlighter-rouge&quot;&gt;data source&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Included &lt;code class=&quot;highlighter-rouge&quot;&gt;anachronistic&lt;/code&gt; (belonging to a period other than that being portrayed) variables&lt;/li&gt;
  &lt;li&gt;Treated categorical variables as continuous&lt;/li&gt;
  &lt;li&gt;Treated continuous variables as categorical&lt;/li&gt;
  &lt;li&gt;Filtered training set to &lt;strong&gt;incorrect population&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Forgot to include &lt;code class=&quot;highlighter-rouge&quot;&gt;y-variable&lt;/code&gt; in the training set&lt;/li&gt;
  &lt;li&gt;Didn’t look at &lt;strong&gt;number of missing&lt;/strong&gt; values in column&lt;/li&gt;
  &lt;li&gt;Not filtering for &lt;strong&gt;duplicates&lt;/strong&gt; in the dataset&lt;/li&gt;
  &lt;li&gt;Accidently included &lt;code class=&quot;highlighter-rouge&quot;&gt;ID&lt;/code&gt; field as predictors&lt;/li&gt;
  &lt;li&gt;Failing to bin or account for &lt;strong&gt;rare categories&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Using proxies of outcomes as predictors&lt;/li&gt;
  &lt;li&gt;Incorrect handling of &lt;code class=&quot;highlighter-rouge&quot;&gt;missing values&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Capped outliers in a way that didn’t make sense with data&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Misunderstanding of variable&lt;/strong&gt; relationships due to incomplete EDA&lt;/li&gt;
  &lt;li&gt;Failed to create calculated variables from raw data&lt;/li&gt;
  &lt;li&gt;Building model on the wrong population&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;reference-1&quot;&gt;Reference&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/beeonaposy/status/1122964504910938121&quot;&gt;tweet_Caitlin_Hudon&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/debug-ml-iclr2019/debug-ml-iclr2019.github.io&quot;&gt;ICLR2019_Workshop_on_Debug_ML&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;9-tips-for-training-lightning-fast-neural-networks-in-pytorch&quot;&gt;&lt;strong&gt;9 Tips For Training Lightning-Fast Neural Networks In Pytorch&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;blog&quot;&gt;&lt;a href=&quot;https://towardsdatascience.com/9-tips-for-training-lightning-fast-neural-networks-in-pytorch-8e63a502f565&quot;&gt;Blog&lt;/a&gt;&lt;/h2&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-to-write-training-loop-in-pytorch&quot;&gt;&lt;strong&gt;How to write training loop in PyTorch?&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/msank00/deeplearning_4m_scratch/blob/master/03_minibatch_training.ipynb&quot;&gt;notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;dive-into-deep-learning-with-pytroch&quot;&gt;&lt;strong&gt;Dive into Deep Learning with PyTroch&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.d2l.ai/&quot;&gt;Original Book using mxnet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/dsgiitr/d2l-pytorch&quot;&gt;PyTorch Version&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;how-to-design-and-debug-deep-learning-models&quot;&gt;&lt;strong&gt;How to design and debug deep learning models?&lt;/strong&gt;&lt;/h2&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Learning&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ML&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;engineering&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slog&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;even&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;legendary&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hackers&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;like&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gdb&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;IMO&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;two&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hardest&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parts&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ML&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eng&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Feedback&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loops&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;measured&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minutes&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;days&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ML&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compared&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seconds&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Errors&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;are&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;often&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;silent&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ML&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Most&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ML&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;people&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;deal&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;silent&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;slow&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feedback&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loops&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;via&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ratchet&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;approach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Start&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;known&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;working&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Record&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;curves&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;small&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;task&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Make&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tiny&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;code&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;change&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Inspect&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;curves&lt;/span&gt;
&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Run&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;full&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;after&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tiny&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;changes&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Downside&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ratchet&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;approach&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;some&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;designs&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cant&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;be&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reached&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;via&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;small&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;incremental&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;changes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Also&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hard&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;know&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tiny&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;code&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;changes&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-py highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Within&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ratchet&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;approach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;want&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;more&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tools&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;practices&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;making&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feedback&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loops&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shorter&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;making&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;errors&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;louder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Below&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;short&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;development&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hacks&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;I&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;have&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;found&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;useful&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;ml-dev-speed-hack-0---overfit-a-single-batch&quot;&gt;&lt;strong&gt;ML dev speed hack #0 - Overfit a single batch&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Before doing anything else, verify that your model can memorize the labels for a single batch and quickly bring the loss to zero&lt;/li&gt;
  &lt;li&gt;This is fast to run, and if the model can’t do this, then you know it is broken&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ml-dev-speed-hack-1---pytorch-over-tf&quot;&gt;&lt;strong&gt;ML dev speed hack #1 - PyTorch over TF&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Time to first step is faster b/c no static graph compilation&lt;/li&gt;
  &lt;li&gt;Easier to get loud errors via assertions within the code&lt;/li&gt;
  &lt;li&gt;Easier to drop into debugger and inspect tensors&lt;/li&gt;
  &lt;li&gt;(TF2.0 may solve some of these problems but is still raw)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ml-dev-speed-hack-2---assert-tensor-shapes&quot;&gt;&lt;strong&gt;ML dev speed hack #2 - Assert tensor shapes&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Wrong shapes due to silent broadcasting or reduction is an extreme hot spot for silent errors, asserting on shapes (in torch or TF) makes them loud&lt;/li&gt;
  &lt;li&gt;If you’re ever tempted to write shapes in a comment, make an assert instead&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ml-dev-speed-hack-3---add-ml-test-to-ci&quot;&gt;&lt;strong&gt;ML dev speed hack #3 - Add ML test to CI&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;If more than one entrypoint or more than one person working on the codebase, then add a test that runs for N steps and then checks loss&lt;/li&gt;
  &lt;li&gt;If you only have one person and entrypoint then an ML test in CI is probably overkill&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ml-dev-speed-hack-4---use-ipdbset_trace&quot;&gt;&lt;strong&gt;ML dev speed hack #4 - Use &lt;code class=&quot;highlighter-rouge&quot;&gt;ipdb.set_trace()&lt;/code&gt;&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;It’s hard to make an ML job take less than 10 seconds to start, which is too slow to maintain flow&lt;/li&gt;
  &lt;li&gt;Using the ipdb workflow lets you zero in on a bug and play with tensors with a fast feedback loop&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ml-dev-speed-hack-5---use-nvvp-to-debug-throughput&quot;&gt;&lt;strong&gt;ML dev speed hack #5 - Use &lt;code class=&quot;highlighter-rouge&quot;&gt;nvvp&lt;/code&gt; to debug throughput&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;ML throughput (step time) is one place where we have the tools to make errors loud and feedback fast&lt;/li&gt;
  &lt;li&gt;You can use &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.cuda.nvtx.range_push&lt;/code&gt; to annotate the nvvp timeline to be more readable&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Source:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://twitter.com/nottombrown/status/1156350020351713281&quot;&gt;Twitter Thread&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.gregbrockman.com/how-i-became-a-machine-learning-practitioner&quot;&gt;how-i-became-a-machine-learning-practitioner&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?time_continue=8&amp;amp;v=GwGTwPcG0YM&quot;&gt;Youtube: Troubleshooting Deep Neural Networks - Full Stack Deep Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/channel/UCVchfoB65aVtQiDITbGq2LQ/videos&quot;&gt;Youtube: Full Stack Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;</content><author><name>Sankarshan Mridha</name></author><summary type="html">Deep Learning Best Practices - Mistakes and Tips</summary></entry></feed>