---
layout: post
title:  "Deep Learning: Computer Vision"
date:   2019-08-10 00:00:10 -0030
categories: jekyll update
mathjax: true
---

# Content

1. TOC
{:toc}
---

# Short Summary of CNN

- The interest in CNN started with AlexNet in 2012 and it has grown exponentially ever since.
- The main advantage of CNN compared to its predecessors is that it automatically detects the important features without any human supervision.
- CNN is also computationally efficient. It uses special convolution and pooling operations and performs parameter sharing. This enables CNN models to run on any device, making them universally attractive.

## Architecture

- All CNN models follow a similar architecture, as shown in the figure below.

![image](https://miro.medium.com/max/700/1*uulvWMFJMidBfbH9tMVNTw@2x.png)

## Convolution

- We perform a series `convolution` + `pooling operations`, followed by a number of fully connected layers. 

<center>
<img src="https://miro.medium.com/max/700/1*cTEp-IvCCUYPTT0QpE3Gjg@2x.png" height="250">
</center>


<center>
<img src="https://miro.medium.com/max/700/1*ghaknijNGolaA3DpjvDxfQ@2x.png" height="250">
</center>

- The green area where the convolution operation takes place is called the `receptive field`.


<center>
<img src="https://miro.medium.com/max/700/1*VVvdh-BUKFh2pwDD0kPeRA@2x.gif" height="250">
</center>

- **NOTE:** We perform multiple convolutions on an input, each using a different filter and resulting in a distinct feature map. We then stack all these feature maps together and that becomes the final output of the convolution layer.


<center>
<img src="https://miro.medium.com/max/700/1*45GSvnTvpHV0oiRr78dBiw@2x.png" height="150">
</center>


<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

## Non-Linearity

- We again pass the result of the convolution operation through `relu` or `Leaky-Relu` activation function. So the values in the final feature maps are not actually the sums, but the relu function applied to them.

## Stride and Padding

- Stride specifies how much we move the convolution filter at each step. By default the value is 1
- We see that the size of the feature map is smaller than the input, because the convolution filter needs to be contained in the input. If we want to maintain the same dimensionality, we can use padding to surround the input with zeros.
  
<center>
<img src="https://miro.medium.com/max/700/1*W2D564Gkad9lj3_6t9I2PA@2x.gif" height="250">
</center>


- The gray area around the input is the padding. We either pad with zeros or the values on the edge.

## Pooling

- After a convolution operation we usually perform pooling to `reduce the dimensionality`. This enables us to reduce the number of parameters, which both shortens the training time and combats overfitting. Pooling layers downsample each feature map independently, reducing the height and width, keeping the depth intact.

<center>
<img src="https://miro.medium.com/max/700/1*ReZNSf_Yr7Q1nqegGirsMQ@2x.png" height="150">
</center>

- In CNN architectures, pooling is typically performed with 2x2 windows, stride 2 and no padding. While convolution is done with 3x3 windows, stride 1 and with padding.

## Hyperparameters

- Filter size: we typically use `3x3` filters, but `5x5` or `7x7` are also used depending on the application.
- Filter count: this is the most variable parameter, it’s a power of two anywhere between 32 and 1024. Using more filters results in a more powerful model, but we risk overfitting due to increased parameter count. 
- `Stride`: we keep it at the default value 1.
- `Padding`: we usually use padding.

## Fully Connected

- After the convolution + pooling layers we add a couple of fully connected layers to wrap up the CNN architecture.
- Remember that the output of both convolution and pooling layers are 3D volumes, but a fully connected layer expects a 1D vector of numbers. So we flatten the output of the final pooling layer to a vector and that becomes the input to the fully connected layer. 


## Training:

- You do not fix the filter coefficients, rather you learn them over several iterations of training. The initialization can be random, or can be based on pre-trained model weights (such as those from the 'modelzoo' in github repos for popular models such as Alexnet, VGG, etc)
- Once you decide the filter size, we randomly initialize the weight of the filter and allow back propagation algorithm to learn weights automatically.


<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

## CNN Dimension Analysis:

The input dimensions are as follows

- $W_I = 227$
- $H_I = 227$
- $D_I = 3$
- The filter is of scale $F = 11$, i.e `11x11x3`, where 3 is the same depth as $D_I$ .
- We apply $96$ Filter operations, so therefore $K = 96$
- We do not take any padding ($P=0$)
- We choose a stride length of $S = 4$
- Thus, going by the above information, the output volume can be calculated as follows:
- $W_O = \frac{W_I - F + 2P}{S}+1=55$
- $H_O = \frac{H_I - F + 2P}{S}+1=55$
- $D_O=K=96$
- Thus, the output of the convolutional layer has the dimensions `55x55x96`.

<object data="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+11_+Understanding+the+input_output+dimensions.pdf" type="application/pdf" width="700px" height="700px">
    <embed src="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+11_+Understanding+the+input_output+dimensions.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+11_+Understanding+the+input_output+dimensions.pdf">Download PDF</a>.</p>
    </embed>
</object>



## Differences between Fully-connected DNNs and CNNs

- Sparse Connectivity
- Weight Sharing


<object data="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+12_+Sparse+Connectivity+and+Weight+Sharing.pdf" type="application/pdf" width="700px" height="700px">
    <embed src="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+12_+Sparse+Connectivity+and+Weight+Sharing.pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+12_+Sparse+Connectivity+and+Weight+Sharing.pdf">Download PDF</a>.
        </p>
    </embed>
</object>

## How to understand the LeNet architecture?

![Image-LeNet](https://missinglink.ai/wp-content/uploads/2019/08/LeNet-5-1998.png)


<object data="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+14_+Our+First+Convolutional+Neural+Network+(CNN).pdf" type="application/pdf" width="700px" height="700px">
    <embed src="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+14_+Our+First+Convolutional+Neural+Network+(CNN).pdf">
        <p>This browser does not support PDFs. Please download the PDF to view it: <a href="https://s3.ap-south-1.amazonaws.com/videos.guvi.in/DeepLearningCourse/pdf/ConvolutionalNeuralNetworks/Lesson+14_+Our+First+Convolutional+Neural+Network+(CNN).pdf">Download PDF</a>.
        </p>
    </embed>
</object>



**Remember:**

- Say I have an image size `64x64x3` and filter size is `4x4`. When I will `convolve` this filter over the image, the filter will be applied on all the `3` channels of the image simultaneously, i.e across the depth of the image (in general volume). So image `64x64x3` and a filter `4x4x3` (i.e the initial filter `4x4` is expanded to the depth and becomes `4x4x3` ) is applied and a scalar value is obtained after all the matrix element wise multiplication and sum. Next we will stride i.e convolve across the height and width of the image (but not across the depth as the filter depth and image depth are same) and get the next scalar value. So after the convolution I will get a set of scalar values i.e neurons arranged over a 2D plane.
  - So, a 3D filter when applied on 3D volume (when both depths are same) gives a 2D plane of neurons i.e scalar.
  - So if K such filters are used there will be K such 2D planes

- Next we apply non-linearity over those values by applying some activation function
- But when applying **max-pooling**, the max-pooing matrix will not be expanded across the depth and will be applied independently on the all the 2D planes (stacked together) obtained after the convolution followed by activation. So after the max-pooling the depth of the volume will be same as the depth of the last convolution step. [check Lecture 14, of Convolution Neural Network, PadhAI, in guvi.in]

**References:**

- [TDS: Applied-deep-learning-part-4](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)
- [Adesh Pande](https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html?source=post_page)

<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

# CNN Convolution On RGB Images 

For example, if we want to detect features, not just in a grayscale image, but in an RGB image. 

Let’s name them: this first $6$ here is the height of the image, the second 6 is the width, and the 3 is the number of channels. Similarly, our filter also have a height, width and the number of channels. Number of channels in our image must match the number of channels in our filter, so these two numbers have to be equal. **The output of this will be a 4×4 image, and notice this is 4×4×1, there’s no longer 3 at the end**. Look at the image below.

<center>

<img src="/assets/images/CV/image_3_cnn_1.png" width="500">

</center>

Here we can see the 6×6×3 image and the 3×3×3 filter. The last number is the number of channels and it matches between the image and the filter. To simplify the drawing the 3×3×3 filter, we can draw it as a stack of three matrices. Sometimes, the filter is drawn as a **three-dimensional cube** as we can see in the image below.

<center>

<img src="/assets/images/CV/image_3_cnn_2.png" width="500">

</center>


:dart: **How does the computation work?**

To compute the output of this convolution operation, we take the 3×3×3 filter and first place it in that most upper left position. Notice that 3×3×3 filter has 27 numbers. We take each of these 27 numbers and multiply them with the corresponding numbers from the red, green and blue channel. So, take the first nine numbers from red channel, then the three beneath it for the green channel, then three beneath it from the blue channel and multiply them with the corresponding 27 numbers covered by this yellow cube. Then, <span style="background-color: #FFFF00">we add up all those numbers and this gives us the first number in the output</span>. To compute the next output we take this cube and slide it over by one. Again **we do the twenty-seven element wise multiplications and sum up 27 numbers and that gives us the next output**.

![image](https://i.stack.imgur.com/FjvuN.gif)

By convention, in computer vision when you have an input with a certain `height` and `width`, and a number of `channels`, then your filter can have a different height and width,  <span style="background-color: #FFFF00">but number of channels will be the same</span>. Again, notice that convolving a 6×6×3 volume with a 3×3×3 gives a 4×4 , a 2D output.


:atom_symbol: Knowing how to convolve on volumes is crucial for building convolutional neural networks. New question is, what if we want to detect vertical edges and horizontal edges and maybe even 45° or 70° as well. In other words, what if we want to use multiple filters at the same time?

<span style="background-color: #FFFF00"> Different filters are used for different feature extraction </span>. e.g: one filter for vertical edge, one filter for horizontanl edge. Let's see an example where the filter's job is to identify "eye" in the image.

![image](https://i.stack.imgur.com/9bi5k.gif)

Look at the convolution output, where it finds "eye", output non-zero value, remaining zeros only.

**Effect of filters on original image**

<center>

<img src="https://i.stack.imgur.com/NuldH.png" width="400">

</center>

We can add a new second filter denoted by orange color, which could be a horizontal edge detector. Convolving an image with the filters gives us different 4×4 outputs. These two 4×4 outputs, can be stacked together obtaining a 4×4×2 output volume. The volume can be drawn this as a box of a 4×4×2 volume, where 2 denotes the fact that we used two different filters.

<center>

<img src="/assets/images/CV/image_3_cnn_3.png" width="500">

</center>


## How one `volume` is converted into different `volume` after convolution?

Convolving with the first filter gives us one 4×4 image output, and convolving with the second filter gives a different 4×4 output. To turn this into a convolutional neural network layer we need to **add bias which is a scalar**. 

- Python broadcasting provides that bias is added to every element in that 4×4 output, or to all these sixteen elements. Then we will **apply activation function**, for example `𝑅𝑒𝐿𝑈()` activation function. The same we will do with the output we got by applying the second 3×3×3 filter (kernel). So, once again we will add a different bias and then we will apply a 𝑅𝑒𝐿𝑈 activation function. After adding a bias and after applying a 𝑅𝑒𝐿𝑈 activation function dimensions of outputs remain the same, so we have two 4×4 matrices.

Next, we will repeat previous steps. Then we **stack up** with a 4×4×2 output. This computation have gone from 6×6×3 to a 4×4×2 and it represents one layer of a convolutional neural network.

<center>

<img src="/assets/images/CV/image_3_cnn_4.png" width="600">

</center>

In neural networks one step of a forward propagation step was: $𝑍^{[1]}=𝑊^{[1]}×𝑎^{[0]}+𝑏^{[1]}$,  where $𝑎^{[0]}=𝑥$. Then we applied the non-linearity (e.g. $ReLu()$) to get $𝑎^{[1]}=𝑔^{𝑍[𝑙]}$. The same idea we will apply in a layer of the Convolutional Neural Network.


<center>

<img src="/assets/images/CV/image_3_cnn_5.png" width="450">

</center>

<center>
<figure class="video_container">
  <iframe src="https://www.youtube.com/embed/qyvlt7kiQoI" frameborder="0" allowfullscreen="true" width="100%" height="300"> </iframe>
</figure>
</center>

_*In case the above link is broken, click [here](https://www.youtube.com/watch?v=qyvlt7kiQoI&t=42m29s) and listen from $42:30$ minutes_



**Reference:**

- [Part 1 - How do we make convolutions on RGB images?](http://datahacker.rs/convolution-rgb-image/) :fire:
- [Part 2 - How do we make convolutions on RGB images?](http://datahacker.rs/one-layer-covolutional-neural-network/)
- [Chapter - 4: Learn TensorFlow and deep learning, without a Ph.D.](https://cloud.google.com/blog/products/gcp/learn-tensorflow-and-deep-learning-without-a-phd) :fire:
- [Stackoverflow](https://stackoverflow.com/questions/42883547/intuitive-understanding-of-1d-2d-and-3d-convolutions-in-convolutional-neural-n) :fire:

<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

-----

# Backpropagation In Convolutional Neural Networks

- [Imp_Blog](https://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/?source=post_page)

<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

# VGG Model

Let’s now take a look at an example state-of-the art CNN model from 2014. VGG is a convolutional neural network from researchers at Oxford’s Visual Geometry Group, hence the name VGG. It was the runner up of the ImageNet classification challenge with `7.3%` error rate.

Among the best performing CNN models, VGG is remarkable for its simplicity. Let’s take a look at its architecture.


<center>
<img src="https://miro.medium.com/max/700/1*U8uoGoZDs8nwzQE3tOhfkw@2x.png" height="350">
</center>

- It only uses 3x3 convolutions throughout the network. 
- **NOTE:** The two back to back `3x3` convolutions have the effective receptive field of a `single 5x5` convolution. And three stacked `3x3` convolutions have the receptive field of a single 7x7 one. Here’s the visualization of two stacked 3x3 convolutions resulting in 5x5.


<center>
<img src="https://miro.medium.com/max/700/1*YpXrr8bN5XyqOlztKPHvDw@2x.png" height="200">
</center>

- Does that mean, if `n` `3x3` filters are used back to back that is equivalent     to applying 1 filter of size `2n+1`

- Another advantage of stacking two convolutions instead of one is that we use **two relu operations**, and **more non-linearity** gives **more power** to the model.

- The number of filters increase as we go deeper into the network. The spatial size of the feature maps decrease since we do pooling, but the depth of the volumes increase as we use more filters.


<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

## How to extract the features and feed to other model?

![image](https://www.researchgate.net/profile/Max_Ferguson/publication/322512435/figure/fig3/AS:697390994567179@1543282378794/Fig-A1-The-standard-VGG-16-network-architecture-as-proposed-in-32-Note-that-only.png)

In the above VGG diagram the last 3 gree boxes are the Fully Connected (FC) layers and they are doing the classification. And before that FC layers, everything else are doing feature selection. 

Now if you pay attention to the last (from left to right) red box (after the max pooling, size: `7x7x512`), that encodes all the features for that image. 

Now you can take that out, flatten it as a `1D` vector and can pass it to other architecture. 

### Where is this useful?

Say you are working on Image to Caption generation task. There task is to generate caption `given image` and previous captions. So in this Encoder-Decoder model, instead of feeding the ras image (pixels value as vector), we can first pass the Image through VGG16 and then extract that red box (refer above image), flatten it and pass that (encoded features of image) to the encoder model.


<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

## Visualizing Feature Maps

- VGG convolutional layers are named as follows: blockX_convY. For example the second filter in the third convolution block is called block3_conv2.

![image](https://miro.medium.com/max/700/1*OuxhgVj1WDDfo5UO5GIhgA@2x.png)

>>As we go deeper into the network, the feature maps look less like the original image and more like an abstract representation of it. As you can see in block3_conv1 the cat is somewhat visible, but after that it becomes unrecognizable. The reason is that deeper feature maps encode high level concepts like “cat nose” or “dog ear” while lower level feature maps detect simple edges and shapes. That’s why deeper feature maps contain less information about the image and more about the class of the image. They still encode useful features, but they are less visually interpretable by us.

![image](https://miro.medium.com/max/700/1*A86wUjL-Z0SWDDI3slKqtg@2x.png)

>> The feature maps become sparser as we go deeper, meaning the filters detect less features. It makes sense because the filters in the first layers detect simple shapes, and every image contains those. But as we go deeper we start looking for more complex stuff like “dog tail” and they don’t appear in every image. That’s why in the first figure with 8 filters per layer, we see more of the feature maps as blank as we go deeper (block4_conv1 and block5_conv1)

- Remember that each filter acts as a detector for a particular feature. The input image we generate will contain a lot of these features.

<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

## Filter and Featuremap Visualization

![image](https://miro.medium.com/max/700/1*w41F9cu7vnvts1e06VoK0A@2x.png)

>> As we go deeper into the network, the filters build on top of each other, and learn to encode more complex patterns. For example filter 41 in block5_conv3 seems to be a bird detector. You can see multiple heads in different orientations, because the particular location of the bird in the image is not important, as long as it appears somewhere the filter will activate. That’s why the filter tries to detect the bird head in several positions by encoding it in multiple locations in the filter.

**Reference:**

- [Very_Imp_Blog](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)


<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

# Best deep CNN architectures and their principles: from AlexNet to EfficientNet

- [AiSummer](https://theaisummer.com/cnn-architectures/) :fire:

----

# How does CNN work ? Explain the implementation details?

+ [Adit Deshpande Blog](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)
+ [cs231n](http://cs231n.github.io/)


<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

# Explain the back propagation steps of Convolution Neural Network?


+ [Andrej Karpathy YouTube](https://www.youtube.com/watch?v=i94OvYb6noo)
+ [Andrej Karpathy Medium](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)
+ [summary short](https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199)
+ [MUST READ: how to compute backprop in program](http://cs231n.github.io/optimization-2/), 


<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

# Why layer normalization is required in Deep Learning?

**Covariate Shift:** Covariate shift refers to the change in the distribution of the input values to a learning algorithm. For instance, if the `train and test sets come from entirely different sources` (e.g. training images come from the web while test images are pictures taken on the iPhone), the distributions would differ. The reason covariance shift can be a problem is that the behavior of machine learning algorithms can change when the input distribution changes.

**A neural network changes the weights of each layer over the course of training**. This means that the activations of each layer change as well. Since the activations of a previous layer are the inputs of the next layer, each layer in the neural network is faced with a situation where the **input distribution changes with each step**. This is problematic because it **forces each intermediate layer to continuously adapt** to its changing inputs.


The basic idea behind batch normalization is **to limit covariate shift by normalizing the activations** of each layer (transforming the inputs to be mean 0 and unit variance). This, supposedly, allows each layer to learn on a `more stable distribution of inputs`, and would thus accelerate the training of the network.

Batch normalization is one of the reasons why deep learning has made such outstanding progress in recent years. Batch normalization **enables the use of higher learning rates**, greatly accelerating the learning process. It also **enabled the training of deep neural networks with sigmoid activations** that were previously deemed too difficult to train due to the vanishing gradient problem. Based on its success, other normalization methods such as layer normalization and weight normalization have appeared and are also finding use within the field.

Though batch normalization is now a standard feature of any deep learning framework and can be used off the shelf, using it naively can lead to difficulties in practice. Despite the simplicity of batch normalization, the actual reason why it is effective is more complicated than meets the eye (Hint: It’s not as simple as just saying “covariate shift”). 

This series of posts attempts to provide a holistic understanding of normalization methods in deep learning. This post attempts to provide a better intuition into batch normalization – the first normalization method to gain widespread usage – and why it improves performance.

1. **Intuition 1: Covariate Shift**

The first intuition concerns “covariate shift” (which is in the title of the original batch normalization paper).

Covariate shift refers to the **change in the distribution of the input values to a learning algorithm**. This is a problem that is not unique to deep learning. For instance, if the train and test sets come from entirely different sources (e.g. training images come from the web while test images are pictures taken on the iPhone), the distributions would differ. The reason covariance shift can be a problem is that the behavior of machine learning algorithms can change when the input distribution changes.

In the context of deep learning, we are particularly concerned with the change in the distribution of the inputs to the inner nodes within a network. A neural network changes the weights of each layer over the course of training. This means that the activations of each layer change as well. Since the activations of a previous layer are the inputs of the next layer, each layer in the neural network is faced with a situation where the input distribution changes with each step. This is problematic because it forces each intermediate layer to continuously adapt to its changing inputs.

The basic idea behind batch normalization is to limit covariate shift by normalizing the activations of each layer (transforming the inputs to be mean 0 and unit variance). This, supposedly, allows each layer to learn on a more stable distribution of inputs, and would thus accelerate the training of the network.

In practice, restricting the activations of each layer to be strictly $0$ `mean` and `unit variance` can **limit the expressive power** of the network. Therefore, in practice, batch normalization allows the network to learn parameters $\gamma$ and $\beta$ that can convert the mean and variance to any value that the network desires.

<center>
<img src="/assets/images/image_32_batch_norm_3.png" 
alt="image"  height="350"/>
</center>


**Batch normalization** is a normalization method that normalizes activations in a network across the mini-batch. For each feature, batch normalization computes the mean and variance of that feature in the mini-batch. It then subtracts the mean and divides the feature by its mini-batch standard deviation.


According to the book Deep Learning by Ian Goodfellow, batch normalization can be understood from the perspective of high-order interactions.


- In a neural network, changing one weight affects subsequent layers, which then affect subsequent layers, and so on.
- This means changing one weight can make the activations fly all over the place in complex ways.
- This forces us to use lower learning rates to prevent the gradients from exploding, or – if we use sigmoid activations – to prevent the gradients from disappearing.
- Batch normalization to the rescue! **Batch normalization re-parameterizes the network to make optimization easier**.
- With batch normalization, we can `control the magnitude and mean of the activations independent of all other layers`.
- This means weights don’t fly all over the place (as long as we have sensible initial means and magnitudes), and we can optimize much easier.

**Alternatives:**

<center>
<img src="/assets/images/image_32_batch_norm_1.png" alt="image" height="120">
</center>

- Layer Normalization

<center>
<img src="/assets/images/image_32_batch_norm_2.png" alt="image" height="200">
</center>


In batch normalization, the statistics are computed across the batch and are the same for each example in the batch. In contrast, in layer normalization, the statistics are computed across each feature and are independent of other examples.


**More Explanation:**

![image](https://gradientscience.org/images/batchnorm/landscapes.jpg)

The `smoothing effect` of BatchNorm makes the optimization landscape much easier to navigate, which could explain the faster convergence and robustness to hyper-parameters observed in practice.


**Reference:**

- [An-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning](https://mlexplained.com/2018/01/10/an-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning-part-1/)
- [An-overview-of-normalization-methods-in-deep-learning](https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/)
- [batchnorm](http://gradientscience.org/batchnorm/)

<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

## When should we apply batch-normalization?

The original paper instructs to put the batchnorm before activation layer but there is ongoing debate (look at the below github and reddit link). Many people showed emperically that by applying batchnorm after the activation function they got better result.

BN is better understood as a technique which **reduces second-order relationships between parameters of different layers** than a method to reduce covariate shift. Thus, the before/after distinction doesn't matter, and differences in performance could simply be because of other particular artifacts of the model.

Source: the deep learning book, 

**Reference:**

- [github:Batchnorm Before/After](https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md)
- [Reddit:Batch_normalization_before_or_after_relu](https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/)
- [Youtube-Ian Goodfellow](https://www.youtube.com/embed/Xogn6veSyxA?start=325&end=664&version=3)
- [Batch normalization](https://e2eml.school/batch_normalization.html) :fire:

<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

# Linear Classification

![image](/assets/images/CV/image_1_KNN_1.png)
![image](/assets/images/CV/image_1_KNN_2.png)

- **Q:** With N examples, how fast are training and prediction?
  - **Ans:** 
    - Train O(1),     
    - Predict O(N)
    - This is bad: we want classifiers that are fastat prediction; slow for training is ok


**Reference:**

- [cs231n_2019_lecture02](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture02.pdf)

<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----


# Linear Classification - Loss Function, Optimization

![image](/assets/images/CV/image_2_LOSS_1.png)
![image](/assets/images/CV/image_2_LOSS_2.png)
![image](/assets/images/CV/image_2_LOSS_3.png)
![image](/assets/images/CV/image_2_LOSS_4.png)

- Suppose that we found a $W$ such that $L = 0$. Is this $W$ unique?
  - NO. $2W$ also has $L = 0$


![image](/assets/images/CV/image_2_LOSS_5.png)

**Why regularize?**

- Express preferences over weights
- Make the model simple so it works on test data
- Improve optimization by adding curvature

![image](/assets/images/CV/image_2_LOSS_6.png)
 

- Q1: What is the `min/max` possible loss $L_i$?
  - A: min $0$, max $\infin$

- Q2: At initialization all s (score) will be approximately equal; what is the loss?
  - A: $\log(\text{number Of Class})$, e.g $\log(10)$ ≈ $2.3$


**Reference:**

- [cs231n_2019_lecture03](http://cs231n.stanford.edu/slides/2019/cs231n_2019_lecture03.pdf)


<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

# Siamese Network

> Siamese networks are a special type of neural network architecture. Instead of a model learning to classify its inputs, the neural networks learns to differentiate between two inputs. It learns the similarity between them.


A **twin neural network** (sometimes called a Siamese Network, though this term is frowned upon) is an artificial neural network that **uses the same weights** while working in tandem (having two things arranged one in front of the other) on two different input vectors to compute comparable output vectors. Often one of the output vectors is precomputed, thus forming a baseline against which the other output vector is compared. This is similar to comparing fingerprints but can be described more technically as a distance function for `locality-sensitive hashing`.

It is possible to make a kind of structure that is functional similar to a siamese network, but implements a slightly different function. This is typically used for comparing similar instances in different type sets.

Uses of similarity measures where a twin network might be used are such things as 

- Recognizing handwritten checks
- Automatic detection of faces in camera images
- Matching queries with indexed documents.

## Learning

Learning in twin networks can be done with `triplet loss` or `contrastive loss`.

### Triplet Loss

Triplet loss is a loss function for artificial neural networks where a baseline (`anchor`) input is compared to a positive (`truthy`) input and a negative (`falsy`) input. The distance from the baseline (anchor) input to the positive (truthy) input is minimized, and the distance from the baseline (anchor) input to the negative (falsy) input is maximized. [wiki](https://en.wikipedia.org/wiki/Triplet_loss)

- minimize distance(baseline,truth)
- maximize distance(baseline,false) 

The negative (false) vector will force learning in the network, while the positive vector (truth) will act like a regularizer.

### Predefined metrics, Euclidean distance metric

The common learning goal is to minimize a distance metric for similar objects and maximize for distinct ones. This gives a loss function like 

<center>

$
\delta(x^i, x^j)=\left\{
                \begin{array}{ll}
                  min \vert\vert f(x^i) - f(x^j) \vert\vert, i \ne j\\
                  max \vert\vert f(x^i) - f(x^j) \vert\vert, i = j
                \end{array}
              \right.
$

</center>


### Twin Networks for Object Tracking

Twin networks have been used in object tracking because of its unique two tandem inputs and **similarity measurement**. In object tracking, one input of the twin network is user pre-selected exemplar image, the other input is a larger search image, which twin network's job is to locate exemplar inside of search image. By measuring the similarity between exemplar and each part of the search image, a map of similarity score can be given by the twin network. 

Furthermore, using a Fully Convolutional Network, the process of computing each sector's similarity score can be replaced with only one cross correlation layer.


:paperclip: **Reference:**

- [wiki: Siamese network](https://en.wikipedia.org/wiki/Siamese_neural_network)
- [wiki: triplet loss](https://en.wikipedia.org/wiki/Triplet_loss)
- [blog1](https://hackernoon.com/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e)
- [blog:Siamese Networks: Algorithm, Applications And PyTorch Implementation](https://becominghuman.ai/siamese-networks-algorithm-applications-and-pytorch-implementation-4ffa3304c18)

<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>


----

# Object Detection

To build a model that can detect and localize specific objects in images.

Learning both regression and classification together.

Regression: 4 bounding box co-ordinates
Classification: Object Label

<center>
<img src="https://raw.githubusercontent.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection/master/img/baseball.gif" height="300">
</center>


## Bounding box

- `Ground Truth Bounding Box`: 4-dimensional array representing the rectangle which surrounds the ground truth object in the image (related to the dataset)
- `Anchor Box`: 4-dimensional array representing the rectangular patch of the input image the model looks at, to figure out which objects it contains (related to the model)
- `Anchor Box Activations or Predicted Bounding Box`: 4-dimensional array which should be as close as possible to the Ground Truth Bounding Box. I found the difference between Anchor Box and its Activations extremely confusing. In fact, they are the same thing. The Activations are nothing else that an updated version of the Anchor Box as the training phase proceeds. We keep them separated just because (in this specific implementation of SSD) we apply some position/size constraints to the Activations, based on the position/size of the original Anchor. The job of the Anchor Box is to fit the Ground Truth Bounding Box (and its class) as well as possible. Nothing else.

<center>
<img src="https://www.francescopochetti.com/wp-content/uploads/2019/01/CVBB-768x852.png" height="400">
</center>

<center>
<img src="https://www.francescopochetti.com/wp-content/uploads/2019/01/architecture-2-1024x576.png" height="300">
</center>


**Reference:**

- [PyTorch-Tutorial-to-Object-Detection](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Object-Detection)
- [Important Blog](http://francescopochetti.com/fast-ai-dl2-lesson-9-single-shot-detection-detailed-walkthrough/)
- [Deep Object Detection](http://www.cs.toronto.edu/~kkundu/CSC2523DeepObjDet.pdf)

<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

# What is RCNN?

RCNN: Region Based CNN for Object Detection



<center>
<img src="/assets/images/image_09_rcnn_1.png" height="300">
</center>


<center>
<img src="/assets/images/image_09_rcnn_2.png" height="300">
</center>


<center>
<img src="/assets/images/image_09_rcnn_3.png" height="300">
</center>


# What is SPP-net?


<center>
<img src="/assets/images/image_09_rcnn_4.png" height="300">
</center>

# What is Fast-RCNN?


<center>
<img src="/assets/images/image_09_rcnn_5.png" height="300">
</center>

- Fast test-time line SPP-net
- One network, trained in one stage
- Higher mean average precision than slow RCNN and SPP-net



<center>
<img src="/assets/images/image_09_rcnn_6.png" height="300">
</center>


**Reference:**

- [FAIR: Ross Girshick](http://www.robots.ox.ac.uk/~tvg/publications/talks/fast-rcnn-slides.pdf)
- [Barkley: Ross Girshick](https://courses.cs.washington.edu/courses/cse590v/14au/cse590v_wk1_rcnn.pdf)
- [cs231: Fei Fei Li](http://cs231n.stanford.edu/slides/2018/cs231n_2018_lecture11.pdf)

<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

# Object detection with RCNN

**Steps:**

![image](/assets/images/image_09_rcnn_0.png)

## Region Proposal

![image](/assets/images/image_09_rcnn_7.png)
![image](/assets/images/image_09_rcnn_8.png)

## Feature Extraction

![image](/assets/images/image_09_rcnn_9.png)

## Classification

![image](/assets/images/image_09_rcnn_10.png)


## Regression

![image](/assets/images/image_09_rcnn_11.png)

Similarly e calculate for $y$ coordinate.

**Reference:**

- Padhai, Prof. Mikesh, Lecture: Object Detection

<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

# List of Object Detection Algorithms and Repositories

<center>
<img src="https://camo.githubusercontent.com/e69d4118b20a42de4e23b9549f9a6ec6dbbb0814/687474703a2f2f706a7265646469652e636f6d2f6d656469612f66696c65732f6461726b6e65742d626c61636b2d736d616c6c2e706e67" height="200">
</center>

- [awesome-object-detection](https://github.com/amusi/awesome-object-detection)


<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

# What is YOLO?

YOLO: You Only Look Once


<center>
<img src="https://blog.paperspace.com/content/images/2018/04/yolo-5.png" height="500">
</center>


**Reference**

- [how-to-implement-a-yolo-object-detector-in-pytorch](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/)

<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>

----

# Exercise:

1. Face Recognition
2. Single Shot Detection (SSD)

----

<a href="#Top"><img align="right" width="28" height="28" src="/assets/images/icon/arrow_circle_up-24px.svg" alt="Top"></a>